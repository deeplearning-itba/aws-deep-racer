@timestamp,@message
2019-10-02 14:20:31.827,Uploaded 3 files for checkpoint 70
2019-10-02 14:20:31.827,INFO:tensorflow:Froze 11 variables.
2019-10-02 14:20:31.827,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 14:20:31.827,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_70.pb
2019-10-02 14:20:28.826,"Policy training> Surrogate loss=-0.09455987066030502, KL divergence=0.06008188799023628, Entropy=0.9619999527931213, training epoch=9, learning_rate=3e-05"
2019-10-02 14:20:28.826,Checkpoint> Saving in path=['./checkpoint/70_Step-112838.ckpt']
2019-10-02 14:20:13.014,"Policy training> Surrogate loss=-0.09113054722547531, KL divergence=0.05674266815185547, Entropy=0.9708907604217529, training epoch=8, learning_rate=3e-05"
2019-10-02 14:19:57.895,"Policy training> Surrogate loss=-0.08882192522287369, KL divergence=0.052716974169015884, Entropy=0.9773287773132324, training epoch=7, learning_rate=3e-05"
2019-10-02 14:19:42.355,"Policy training> Surrogate loss=-0.08291217684745789, KL divergence=0.053578052669763565, Entropy=0.9803560376167297, training epoch=6, learning_rate=3e-05"
2019-10-02 14:19:26.820,"Policy training> Surrogate loss=-0.07817550003528595, KL divergence=0.05134481191635132, Entropy=0.9886428117752075, training epoch=5, learning_rate=3e-05"
2019-10-02 14:19:11.564,"Policy training> Surrogate loss=-0.07277252525091171, KL divergence=0.05066557601094246, Entropy=1.0191377401351929, training epoch=4, learning_rate=3e-05"
2019-10-02 14:18:55.118,"Policy training> Surrogate loss=-0.06626638770103455, KL divergence=0.04921131581068039, Entropy=1.01876699924469, training epoch=3, learning_rate=3e-05"
2019-10-02 14:18:39.794,"Policy training> Surrogate loss=-0.05272362008690834, KL divergence=0.05391751602292061, Entropy=1.0265969038009644, training epoch=2, learning_rate=3e-05"
2019-10-02 14:18:24.144,"Policy training> Surrogate loss=-0.02820674143731594, KL divergence=0.06556794792413712, Entropy=1.031825065612793, training epoch=1, learning_rate=3e-05"
2019-10-02 14:18:08.885,"Policy training> Surrogate loss=0.015617536380887032, KL divergence=0.030932903289794922, Entropy=1.0339890718460083, training epoch=0, learning_rate=3e-05"
2019-10-02 14:17:50.857,"Training> Name=main_level/agent, Worker=0, Episode=1400, Total reward=40.43, Steps=112838, Training iteration=69"
2019-10-02 14:17:44.848,"Training> Name=main_level/agent, Worker=0, Episode=1399, Total reward=54.85, Steps=112760, Training iteration=69"
2019-10-02 14:17:36.846,"Training> Name=main_level/agent, Worker=0, Episode=1398, Total reward=162.03, Steps=112642, Training iteration=69"
2019-10-02 14:17:22.842,"Training> Name=main_level/agent, Worker=0, Episode=1397, Total reward=66.26, Steps=112447, Training iteration=69"
2019-10-02 14:17:11.838,"Training> Name=main_level/agent, Worker=0, Episode=1396, Total reward=141.08, Steps=112294, Training iteration=69"
2019-10-02 14:16:54.834,"Training> Name=main_level/agent, Worker=0, Episode=1395, Total reward=37.95, Steps=112043, Training iteration=69"
2019-10-02 14:16:48.832,"Training> Name=main_level/agent, Worker=0, Episode=1394, Total reward=85.73, Steps=111959, Training iteration=69"
2019-10-02 14:16:36.829,"Training> Name=main_level/agent, Worker=0, Episode=1393, Total reward=112.57, Steps=111792, Training iteration=69"
2019-10-02 14:16:20.825,"Training> Name=main_level/agent, Worker=0, Episode=1392, Total reward=165.21, Steps=111562, Training iteration=69"
2019-10-02 14:16:04.820,"Training> Name=main_level/agent, Worker=0, Episode=1391, Total reward=44.75, Steps=111329, Training iteration=69"
2019-10-02 14:15:57.818,"Training> Name=main_level/agent, Worker=0, Episode=1390, Total reward=8.48, Steps=111228, Training iteration=69"
2019-10-02 14:15:55.818,"Training> Name=main_level/agent, Worker=0, Episode=1389, Total reward=100.13, Steps=111211, Training iteration=69"
2019-10-02 14:15:38.812,"Training> Name=main_level/agent, Worker=0, Episode=1388, Total reward=123.87, Steps=110958, Training iteration=69"
2019-10-02 14:15:20.807,"Training> Name=main_level/agent, Worker=0, Episode=1387, Total reward=21.23, Steps=110701, Training iteration=69"
2019-10-02 14:15:15.806,"Training> Name=main_level/agent, Worker=0, Episode=1386, Total reward=25.14, Steps=110639, Training iteration=69"
2019-10-02 14:15:10.804,"Training> Name=main_level/agent, Worker=0, Episode=1385, Total reward=17.29, Steps=110572, Training iteration=69"
2019-10-02 14:15:06.803,"Training> Name=main_level/agent, Worker=0, Episode=1384, Total reward=204.92, Steps=110517, Training iteration=69"
2019-10-02 14:14:51.798,"Training> Name=main_level/agent, Worker=0, Episode=1383, Total reward=33.25, Steps=110299, Training iteration=69"
2019-10-02 14:14:45.796,"Training> Name=main_level/agent, Worker=0, Episode=1382, Total reward=122.67, Steps=110237, Training iteration=69"
2019-10-02 14:14:30.792,"Training> Name=main_level/agent, Worker=0, Episode=1381, Total reward=136.39, Steps=110020, Training iteration=69"
2019-10-02 14:14:13.788,Uploaded 3 files for checkpoint 69
2019-10-02 14:14:13.788,INFO:tensorflow:Froze 11 variables.
2019-10-02 14:14:13.788,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 14:14:13.788,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_69.pb
2019-10-02 14:14:11.787,Checkpoint> Saving in path=['./checkpoint/69_Step-109795.ckpt']
2019-10-02 14:14:10.786,"Policy training> Surrogate loss=-0.08346044272184372, KL divergence=0.055505700409412384, Entropy=0.999538779258728, training epoch=9, learning_rate=3e-05"
2019-10-02 14:13:54.390,"Policy training> Surrogate loss=-0.08346804976463318, KL divergence=0.05402292311191559, Entropy=0.9975408911705017, training epoch=8, learning_rate=3e-05"
2019-10-02 14:13:37.603,"Policy training> Surrogate loss=-0.07658279687166214, KL divergence=0.0517374612390995, Entropy=1.0078319311141968, training epoch=7, learning_rate=3e-05"
2019-10-02 14:13:20.134,"Policy training> Surrogate loss=-0.07540176063776016, KL divergence=0.04999398812651634, Entropy=1.0159683227539062, training epoch=6, learning_rate=3e-05"
2019-10-02 14:13:03.242,"Policy training> Surrogate loss=-0.07004670053720474, KL divergence=0.04972141608595848, Entropy=1.0230331420898438, training epoch=5, learning_rate=3e-05"
2019-10-02 14:12:46.943,"Policy training> Surrogate loss=-0.06332503259181976, KL divergence=0.047785405069589615, Entropy=1.028262734413147, training epoch=4, learning_rate=3e-05"
2019-10-02 14:12:30.166,"Policy training> Surrogate loss=-0.0565071776509285, KL divergence=0.04640056937932968, Entropy=1.0506740808486938, training epoch=3, learning_rate=3e-05"
2019-10-02 14:12:13.839,"Policy training> Surrogate loss=-0.0465187169611454, KL divergence=0.05278816074132919, Entropy=1.0460625886917114, training epoch=2, learning_rate=3e-05"
2019-10-02 14:11:56.362,"Policy training> Surrogate loss=-0.02814769744873047, KL divergence=0.06188086420297623, Entropy=1.042048454284668, training epoch=1, learning_rate=3e-05"
2019-10-02 14:11:39.821,"Policy training> Surrogate loss=0.029402241110801697, KL divergence=0.041940636932849884, Entropy=1.028433084487915, training epoch=0, learning_rate=3e-05"
2019-10-02 14:11:20.519,"Training> Name=main_level/agent, Worker=0, Episode=1380, Total reward=50.54, Steps=109795, Training iteration=68"
2019-10-02 14:11:12.517,"Training> Name=main_level/agent, Worker=0, Episode=1379, Total reward=38.83, Steps=109686, Training iteration=68"
2019-10-02 14:11:05.515,"Training> Name=main_level/agent, Worker=0, Episode=1378, Total reward=177.42, Steps=109586, Training iteration=68"
2019-10-02 14:10:49.510,"Training> Name=main_level/agent, Worker=0, Episode=1377, Total reward=134.42, Steps=109352, Training iteration=68"
2019-10-02 14:10:33.506,"Training> Name=main_level/agent, Worker=0, Episode=1376, Total reward=146.08, Steps=109112, Training iteration=68"
2019-10-02 14:10:15.501,"Training> Name=main_level/agent, Worker=0, Episode=1375, Total reward=72.46, Steps=108864, Training iteration=68"
2019-10-02 14:10:05.498,"Training> Name=main_level/agent, Worker=0, Episode=1374, Total reward=201.19, Steps=108709, Training iteration=68"
2019-10-02 14:09:48.493,"Training> Name=main_level/agent, Worker=0, Episode=1373, Total reward=116.11, Steps=108472, Training iteration=68"
2019-10-02 14:09:32.488,"Training> Name=main_level/agent, Worker=0, Episode=1372, Total reward=181.33, Steps=108243, Training iteration=68"
2019-10-02 14:09:18.484,"Training> Name=main_level/agent, Worker=0, Episode=1371, Total reward=28.44, Steps=108033, Training iteration=68"
2019-10-02 14:09:12.482,"Training> Name=main_level/agent, Worker=0, Episode=1370, Total reward=89.65, Steps=107973, Training iteration=68"
2019-10-02 14:08:58.478,"Training> Name=main_level/agent, Worker=0, Episode=1369, Total reward=83.79, Steps=107773, Training iteration=68"
2019-10-02 14:08:42.474,"Training> Name=main_level/agent, Worker=0, Episode=1368, Total reward=13.62, Steps=107534, Training iteration=68"
2019-10-02 14:08:39.473,"Training> Name=main_level/agent, Worker=0, Episode=1367, Total reward=115.26, Steps=107498, Training iteration=68"
2019-10-02 14:08:23.468,"Training> Name=main_level/agent, Worker=0, Episode=1366, Total reward=34.25, Steps=107262, Training iteration=68"
2019-10-02 14:08:15.466,"Training> Name=main_level/agent, Worker=0, Episode=1365, Total reward=31.01, Steps=107155, Training iteration=68"
2019-10-02 14:08:09.464,"Training> Name=main_level/agent, Worker=0, Episode=1364, Total reward=33.51, Steps=107067, Training iteration=68"
2019-10-02 14:08:03.462,"Training> Name=main_level/agent, Worker=0, Episode=1363, Total reward=41.76, Steps=106984, Training iteration=68"
2019-10-02 14:07:55.460,"Training> Name=main_level/agent, Worker=0, Episode=1362, Total reward=166.12, Steps=106885, Training iteration=68"
2019-10-02 14:07:38.455,"Training> Name=main_level/agent, Worker=0, Episode=1361, Total reward=47.77, Steps=106639, Training iteration=68"
2019-10-02 14:07:30.453,Uploaded 3 files for checkpoint 68
2019-10-02 14:07:30.453,INFO:tensorflow:Froze 11 variables.
2019-10-02 14:07:30.453,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 14:07:30.453,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_68.pb
2019-10-02 14:07:26.451,"Policy training> Surrogate loss=-0.0857904925942421, KL divergence=0.061910420656204224, Entropy=1.013540267944336, training epoch=9, learning_rate=3e-05"
2019-10-02 14:07:26.451,Checkpoint> Saving in path=['./checkpoint/68_Step-106541.ckpt']
2019-10-02 14:07:11.166,"Policy training> Surrogate loss=-0.08907726407051086, KL divergence=0.05816736817359924, Entropy=1.0096399784088135, training epoch=8, learning_rate=3e-05"
2019-10-02 14:06:54.960,"Policy training> Surrogate loss=-0.08479359745979309, KL divergence=0.05645590275526047, Entropy=1.0261491537094116, training epoch=7, learning_rate=3e-05"
2019-10-02 14:06:40.385,"Policy training> Surrogate loss=-0.08178490400314331, KL divergence=0.05436505377292633, Entropy=1.0298354625701904, training epoch=6, learning_rate=3e-05"
2019-10-02 14:06:24.820,"Policy training> Surrogate loss=-0.07563579827547073, KL divergence=0.05398086458444595, Entropy=1.033573865890503, training epoch=5, learning_rate=3e-05"
2019-10-02 14:06:09.218,"Policy training> Surrogate loss=-0.0739806592464447, KL divergence=0.05244889110326767, Entropy=1.0368797779083252, training epoch=4, learning_rate=3e-05"
2019-10-02 14:05:52.815,"Policy training> Surrogate loss=-0.06328894197940826, KL divergence=0.05318420007824898, Entropy=1.0615148544311523, training epoch=3, learning_rate=3e-05"
2019-10-02 14:05:38.655,"Policy training> Surrogate loss=-0.05414619296789169, KL divergence=0.05329860374331474, Entropy=1.0597673654556274, training epoch=2, learning_rate=3e-05"
2019-10-02 14:05:23.416,"Policy training> Surrogate loss=-0.03006831184029579, KL divergence=0.056593380868434906, Entropy=1.0470707416534424, training epoch=1, learning_rate=3e-05"
2019-10-02 14:05:06.874,"Policy training> Surrogate loss=0.02507658302783966, KL divergence=0.038603633642196655, Entropy=1.0347998142242432, training epoch=0, learning_rate=3e-05"
2019-10-02 14:04:49.492,"Training> Name=main_level/agent, Worker=0, Episode=1360, Total reward=130.02, Steps=106541, Training iteration=67"
2019-10-02 14:04:33.488,"Training> Name=main_level/agent, Worker=0, Episode=1359, Total reward=12.94, Steps=106312, Training iteration=67"
2019-10-02 14:04:30.487,"Training> Name=main_level/agent, Worker=0, Episode=1358, Total reward=83.83, Steps=106276, Training iteration=67"
2019-10-02 14:04:18.483,"Training> Name=main_level/agent, Worker=0, Episode=1357, Total reward=136.14, Steps=106094, Training iteration=67"
2019-10-02 14:04:01.478,"Training> Name=main_level/agent, Worker=0, Episode=1356, Total reward=13.13, Steps=105857, Training iteration=67"
2019-10-02 14:03:58.477,"Training> Name=main_level/agent, Worker=0, Episode=1355, Total reward=65.65, Steps=105816, Training iteration=67"
2019-10-02 14:03:48.475,"Training> Name=main_level/agent, Worker=0, Episode=1354, Total reward=44.15, Steps=105673, Training iteration=67"
2019-10-02 14:03:41.473,"Training> Name=main_level/agent, Worker=0, Episode=1353, Total reward=114.1, Steps=105575, Training iteration=67"
2019-10-02 14:03:25.468,"Training> Name=main_level/agent, Worker=0, Episode=1352, Total reward=98.33, Steps=105353, Training iteration=67"
2019-10-02 14:03:12.464,"Training> Name=main_level/agent, Worker=0, Episode=1351, Total reward=154.47, Steps=105159, Training iteration=67"
2019-10-02 14:02:56.460,"Training> Name=main_level/agent, Worker=0, Episode=1350, Total reward=159.75, Steps=104926, Training iteration=67"
2019-10-02 14:02:40.455,"Training> Name=main_level/agent, Worker=0, Episode=1349, Total reward=26.05, Steps=104696, Training iteration=67"
2019-10-02 14:02:34.453,"Training> Name=main_level/agent, Worker=0, Episode=1348, Total reward=131.78, Steps=104626, Training iteration=67"
2019-10-02 14:02:19.449,"Training> Name=main_level/agent, Worker=0, Episode=1347, Total reward=163.32, Steps=104412, Training iteration=67"
2019-10-02 14:02:04.445,"Training> Name=main_level/agent, Worker=0, Episode=1346, Total reward=25.01, Steps=104199, Training iteration=67"
2019-10-02 14:02:00.444,"Training> Name=main_level/agent, Worker=0, Episode=1345, Total reward=41.27, Steps=104140, Training iteration=67"
2019-10-02 14:01:53.442,"Training> Name=main_level/agent, Worker=0, Episode=1344, Total reward=136.22, Steps=104041, Training iteration=67"
2019-10-02 14:01:38.437,"Training> Name=main_level/agent, Worker=0, Episode=1343, Total reward=27.14, Steps=103827, Training iteration=67"
2019-10-02 14:01:34.436,"Training> Name=main_level/agent, Worker=0, Episode=1342, Total reward=64.3, Steps=103783, Training iteration=67"
2019-10-02 14:01:23.433,"Training> Name=main_level/agent, Worker=0, Episode=1341, Total reward=32.09, Steps=103623, Training iteration=67"
2019-10-02 14:01:18.431,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_67.pb
2019-10-02 14:01:17.431,Uploaded 3 files for checkpoint 67
2019-10-02 14:01:17.431,INFO:tensorflow:Froze 11 variables.
2019-10-02 14:01:17.431,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 14:01:15.430,"Policy training> Surrogate loss=-0.09435936063528061, KL divergence=0.058542296290397644, Entropy=1.0643872022628784, training epoch=9, learning_rate=3e-05"
2019-10-02 14:01:15.430,Checkpoint> Saving in path=['./checkpoint/67_Step-103564.ckpt']
2019-10-02 14:01:01.667,"Policy training> Surrogate loss=-0.09217552095651627, KL divergence=0.05511672794818878, Entropy=1.072316288948059, training epoch=8, learning_rate=3e-05"
2019-10-02 14:00:48.721,"Policy training> Surrogate loss=-0.08726873993873596, KL divergence=0.05730874463915825, Entropy=1.0597989559173584, training epoch=7, learning_rate=3e-05"
2019-10-02 14:00:34.162,"Policy training> Surrogate loss=-0.08628102391958237, KL divergence=0.05163048580288887, Entropy=1.0721839666366577, training epoch=6, learning_rate=3e-05"
2019-10-02 14:00:21.309,"Policy training> Surrogate loss=-0.0848817452788353, KL divergence=0.05126092582941055, Entropy=1.0700948238372803, training epoch=5, learning_rate=3e-05"
2019-10-02 14:00:07.174,"Policy training> Surrogate loss=-0.07729316502809525, KL divergence=0.04662149399518967, Entropy=1.1017118692398071, training epoch=4, learning_rate=3e-05"
2019-10-02 13:59:54.477,"Policy training> Surrogate loss=-0.0661824643611908, KL divergence=0.05196089297533035, Entropy=1.1170607805252075, training epoch=3, learning_rate=3e-05"
2019-10-02 13:59:40.891,"Policy training> Surrogate loss=-0.06377284228801727, KL divergence=0.052982959896326065, Entropy=1.1062276363372803, training epoch=2, learning_rate=3e-05"
2019-10-02 13:59:26.256,"Policy training> Surrogate loss=-0.041045524179935455, KL divergence=0.0634651854634285, Entropy=1.1227301359176636, training epoch=1, learning_rate=3e-05"
2019-10-02 13:59:12.217,"Policy training> Surrogate loss=0.01496864389628172, KL divergence=0.029416974633932114, Entropy=1.124870777130127, training epoch=0, learning_rate=3e-05"
2019-10-02 13:58:55.617,"Training> Name=main_level/agent, Worker=0, Episode=1340, Total reward=58.99, Steps=103564, Training iteration=66"
2019-10-02 13:58:45.614,"Training> Name=main_level/agent, Worker=0, Episode=1339, Total reward=20.99, Steps=103425, Training iteration=66"
2019-10-02 13:58:42.613,"Training> Name=main_level/agent, Worker=0, Episode=1338, Total reward=98.61, Steps=103377, Training iteration=66"
2019-10-02 13:58:26.608,"Training> Name=main_level/agent, Worker=0, Episode=1337, Total reward=39.33, Steps=103148, Training iteration=66"
2019-10-02 13:58:20.607,"Training> Name=main_level/agent, Worker=0, Episode=1336, Total reward=47.51, Steps=103076, Training iteration=66"
2019-10-02 13:58:14.605,"Training> Name=main_level/agent, Worker=0, Episode=1335, Total reward=15.21, Steps=102990, Training iteration=66"
2019-10-02 13:58:11.604,"Training> Name=main_level/agent, Worker=0, Episode=1334, Total reward=120.67, Steps=102955, Training iteration=66"
2019-10-02 13:57:56.599,"Training> Name=main_level/agent, Worker=0, Episode=1333, Total reward=15.98, Steps=102741, Training iteration=66"
2019-10-02 13:57:52.598,"Training> Name=main_level/agent, Worker=0, Episode=1332, Total reward=140.34, Steps=102684, Training iteration=66"
2019-10-02 13:57:38.594,"Training> Name=main_level/agent, Worker=0, Episode=1331, Total reward=127.55, Steps=102483, Training iteration=66"
2019-10-02 13:57:23.590,"Training> Name=main_level/agent, Worker=0, Episode=1330, Total reward=201.58, Steps=102258, Training iteration=66"
2019-10-02 13:57:08.586,"Training> Name=main_level/agent, Worker=0, Episode=1329, Total reward=31.82, Steps=102040, Training iteration=66"
2019-10-02 13:57:01.584,"Training> Name=main_level/agent, Worker=0, Episode=1328, Total reward=117.28, Steps=101944, Training iteration=66"
2019-10-02 13:56:46.580,"Training> Name=main_level/agent, Worker=0, Episode=1327, Total reward=69.73, Steps=101737, Training iteration=66"
2019-10-02 13:56:35.576,"Training> Name=main_level/agent, Worker=0, Episode=1326, Total reward=48.92, Steps=101566, Training iteration=66"
2019-10-02 13:56:25.574,"Training> Name=main_level/agent, Worker=0, Episode=1325, Total reward=42.93, Steps=101434, Training iteration=66"
2019-10-02 13:56:17.571,"Training> Name=main_level/agent, Worker=0, Episode=1324, Total reward=25.41, Steps=101322, Training iteration=66"
2019-10-02 13:56:12.570,"Training> Name=main_level/agent, Worker=0, Episode=1323, Total reward=133.5, Steps=101257, Training iteration=66"
2019-10-02 13:55:58.566,"Training> Name=main_level/agent, Worker=0, Episode=1322, Total reward=48.92, Steps=101049, Training iteration=66"
2019-10-02 13:55:50.563,"Training> Name=main_level/agent, Worker=0, Episode=1321, Total reward=32.36, Steps=100947, Training iteration=66"
2019-10-02 13:55:44.562,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_66.pb
2019-10-02 13:55:43.561,Uploaded 3 files for checkpoint 66
2019-10-02 13:55:43.561,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:55:43.561,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:55:37.559,"Policy training> Surrogate loss=-0.09920912235975266, KL divergence=0.055960360914468765, Entropy=1.0502349138259888, training epoch=9, learning_rate=3e-05"
2019-10-02 13:55:37.559,Checkpoint> Saving in path=['./checkpoint/66_Step-100882.ckpt']
2019-10-02 13:55:24.867,"Policy training> Surrogate loss=-0.0892292857170105, KL divergence=0.05284850299358368, Entropy=1.0598726272583008, training epoch=8, learning_rate=3e-05"
2019-10-02 13:55:12.375,"Policy training> Surrogate loss=-0.08889622986316681, KL divergence=0.052836790680885315, Entropy=1.0587654113769531, training epoch=7, learning_rate=3e-05"
2019-10-02 13:54:59.157,"Policy training> Surrogate loss=-0.08866822719573975, KL divergence=0.047464534640312195, Entropy=1.0741571187973022, training epoch=6, learning_rate=3e-05"
2019-10-02 13:54:45.889,"Policy training> Surrogate loss=-0.08250068128108978, KL divergence=0.048920344561338425, Entropy=1.0782783031463623, training epoch=5, learning_rate=3e-05"
2019-10-02 13:54:32.479,"Policy training> Surrogate loss=-0.07897602766752243, KL divergence=0.04771013930439949, Entropy=1.0714141130447388, training epoch=4, learning_rate=3e-05"
2019-10-02 13:54:20.108,"Policy training> Surrogate loss=-0.07162794470787048, KL divergence=0.05070053040981293, Entropy=1.0964915752410889, training epoch=3, learning_rate=3e-05"
2019-10-02 13:54:06.818,"Policy training> Surrogate loss=-0.05810478329658508, KL divergence=0.05384599789977074, Entropy=1.0998891592025757, training epoch=2, learning_rate=3e-05"
2019-10-02 13:53:53.274,"Policy training> Surrogate loss=-0.03213779255747795, KL divergence=0.058468256145715714, Entropy=1.077826738357544, training epoch=1, learning_rate=3e-05"
2019-10-02 13:53:40.615,"Policy training> Surrogate loss=0.013887712731957436, KL divergence=0.0331055112183094, Entropy=1.1029391288757324, training epoch=0, learning_rate=3e-05"
2019-10-02 13:53:26.051,"Training> Name=main_level/agent, Worker=0, Episode=1320, Total reward=65.38, Steps=100882, Training iteration=65"
2019-10-02 13:53:15.048,"Training> Name=main_level/agent, Worker=0, Episode=1319, Total reward=23.29, Steps=100737, Training iteration=65"
2019-10-02 13:53:11.047,"Training> Name=main_level/agent, Worker=0, Episode=1318, Total reward=92.92, Steps=100680, Training iteration=65"
2019-10-02 13:52:57.043,"Training> Name=main_level/agent, Worker=0, Episode=1317, Total reward=42.58, Steps=100477, Training iteration=65"
2019-10-02 13:52:51.041,"Training> Name=main_level/agent, Worker=0, Episode=1316, Total reward=38.85, Steps=100387, Training iteration=65"
2019-10-02 13:52:44.039,"Training> Name=main_level/agent, Worker=0, Episode=1315, Total reward=174.49, Steps=100300, Training iteration=65"
2019-10-02 13:52:30.035,"Training> Name=main_level/agent, Worker=0, Episode=1314, Total reward=19.25, Steps=100101, Training iteration=65"
2019-10-02 13:52:27.034,"Training> Name=main_level/agent, Worker=0, Episode=1313, Total reward=86.01, Steps=100051, Training iteration=65"
2019-10-02 13:52:11.029,"Training> Name=main_level/agent, Worker=0, Episode=1312, Total reward=11.69, Steps=99831, Training iteration=65"
2019-10-02 13:52:07.028,"Training> Name=main_level/agent, Worker=0, Episode=1311, Total reward=19.71, Steps=99793, Training iteration=65"
2019-10-02 13:52:04.027,"Training> Name=main_level/agent, Worker=0, Episode=1310, Total reward=10.33, Steps=99744, Training iteration=65"
2019-10-02 13:52:02.027,"Training> Name=main_level/agent, Worker=0, Episode=1309, Total reward=198.49, Steps=99723, Training iteration=65"
2019-10-02 13:51:46.022,"Training> Name=main_level/agent, Worker=0, Episode=1308, Total reward=120.32, Steps=99496, Training iteration=65"
2019-10-02 13:51:31.018,"Training> Name=main_level/agent, Worker=0, Episode=1307, Total reward=23.13, Steps=99279, Training iteration=65"
2019-10-02 13:51:28.017,"Training> Name=main_level/agent, Worker=0, Episode=1306, Total reward=17.15, Steps=99236, Training iteration=65"
2019-10-02 13:51:23.015,"Training> Name=main_level/agent, Worker=0, Episode=1305, Total reward=35.79, Steps=99179, Training iteration=65"
2019-10-02 13:51:15.013,"Training> Name=main_level/agent, Worker=0, Episode=1304, Total reward=170.62, Steps=99068, Training iteration=65"
2019-10-02 13:50:59.008,"Training> Name=main_level/agent, Worker=0, Episode=1303, Total reward=64.02, Steps=98829, Training iteration=65"
2019-10-02 13:50:49.005,"Training> Name=main_level/agent, Worker=0, Episode=1302, Total reward=56.48, Steps=98695, Training iteration=65"
2019-10-02 13:50:40.003,"Training> Name=main_level/agent, Worker=0, Episode=1301, Total reward=129.56, Steps=98561, Training iteration=65"
2019-10-02 13:50:22.998,Uploaded 3 files for checkpoint 65
2019-10-02 13:50:22.998,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:50:22.998,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:50:22.998,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_65.pb
2019-10-02 13:50:17.996,"Policy training> Surrogate loss=-0.09881554543972015, KL divergence=0.053912047296762466, Entropy=1.1261765956878662, training epoch=9, learning_rate=3e-05"
2019-10-02 13:50:17.996,Checkpoint> Saving in path=['./checkpoint/65_Step-98343.ckpt']
2019-10-02 13:50:03.273,"Policy training> Surrogate loss=-0.09563469141721725, KL divergence=0.05135050788521767, Entropy=1.132168173789978, training epoch=8, learning_rate=3e-05"
2019-10-02 13:49:48.518,"Policy training> Surrogate loss=-0.0888543576002121, KL divergence=0.05105806514620781, Entropy=1.1390808820724487, training epoch=7, learning_rate=3e-05"
2019-10-02 13:49:33.885,"Policy training> Surrogate loss=-0.08622212707996368, KL divergence=0.047202035784721375, Entropy=1.142437219619751, training epoch=6, learning_rate=3e-05"
2019-10-02 13:49:18.390,"Policy training> Surrogate loss=-0.08357702195644379, KL divergence=0.04669063910841942, Entropy=1.15517258644104, training epoch=5, learning_rate=3e-05"
2019-10-02 13:49:03.302,"Policy training> Surrogate loss=-0.07865290343761444, KL divergence=0.046338796615600586, Entropy=1.151198148727417, training epoch=4, learning_rate=3e-05"
2019-10-02 13:48:48.256,"Policy training> Surrogate loss=-0.06897587329149246, KL divergence=0.04607663303613663, Entropy=1.173517107963562, training epoch=3, learning_rate=3e-05"
2019-10-02 13:48:32.983,"Policy training> Surrogate loss=-0.058656781911849976, KL divergence=0.05399058014154434, Entropy=1.1461371183395386, training epoch=2, learning_rate=3e-05"
2019-10-02 13:48:18.787,"Policy training> Surrogate loss=-0.037395115941762924, KL divergence=0.060670435428619385, Entropy=1.1778199672698975, training epoch=1, learning_rate=3e-05"
2019-10-02 13:48:04.112,"Policy training> Surrogate loss=0.015605194494128227, KL divergence=0.033460382372140884, Entropy=1.1348309516906738, training epoch=0, learning_rate=3e-05"
2019-10-02 13:47:46.486,"Training> Name=main_level/agent, Worker=0, Episode=1300, Total reward=136.38, Steps=98343, Training iteration=64"
2019-10-02 13:47:30.481,"Training> Name=main_level/agent, Worker=0, Episode=1299, Total reward=61.63, Steps=98102, Training iteration=64"
2019-10-02 13:47:18.478,"Training> Name=main_level/agent, Worker=0, Episode=1298, Total reward=81.94, Steps=97942, Training iteration=64"
2019-10-02 13:47:05.474,"Training> Name=main_level/agent, Worker=0, Episode=1297, Total reward=13.07, Steps=97745, Training iteration=64"
2019-10-02 13:47:02.473,"Training> Name=main_level/agent, Worker=0, Episode=1296, Total reward=154.07, Steps=97722, Training iteration=64"
2019-10-02 13:46:44.468,"Training> Name=main_level/agent, Worker=0, Episode=1295, Total reward=77.42, Steps=97472, Training iteration=64"
2019-10-02 13:46:33.465,"Training> Name=main_level/agent, Worker=0, Episode=1294, Total reward=89.31, Steps=97311, Training iteration=64"
2019-10-02 13:46:18.460,"Training> Name=main_level/agent, Worker=0, Episode=1293, Total reward=16.49, Steps=97100, Training iteration=64"
2019-10-02 13:46:14.459,"Training> Name=main_level/agent, Worker=0, Episode=1292, Total reward=12.85, Steps=97044, Training iteration=64"
2019-10-02 13:46:11.458,"Training> Name=main_level/agent, Worker=0, Episode=1291, Total reward=151.0, Steps=97004, Training iteration=64"
2019-10-02 13:45:56.454,"Training> Name=main_level/agent, Worker=0, Episode=1290, Total reward=118.6, Steps=96794, Training iteration=64"
2019-10-02 13:45:42.450,"Training> Name=main_level/agent, Worker=0, Episode=1289, Total reward=22.07, Steps=96587, Training iteration=64"
2019-10-02 13:45:39.449,"Training> Name=main_level/agent, Worker=0, Episode=1288, Total reward=85.51, Steps=96546, Training iteration=64"
2019-10-02 13:45:26.445,"Training> Name=main_level/agent, Worker=0, Episode=1287, Total reward=83.94, Steps=96360, Training iteration=64"
2019-10-02 13:45:12.441,"Training> Name=main_level/agent, Worker=0, Episode=1286, Total reward=24.96, Steps=96158, Training iteration=64"
2019-10-02 13:45:07.439,"Training> Name=main_level/agent, Worker=0, Episode=1285, Total reward=54.8, Steps=96098, Training iteration=64"
2019-10-02 13:44:57.436,"Training> Name=main_level/agent, Worker=0, Episode=1284, Total reward=126.25, Steps=95954, Training iteration=64"
2019-10-02 13:44:42.432,"Training> Name=main_level/agent, Worker=0, Episode=1283, Total reward=45.05, Steps=95730, Training iteration=64"
2019-10-02 13:44:34.430,"Training> Name=main_level/agent, Worker=0, Episode=1282, Total reward=62.4, Steps=95630, Training iteration=64"
2019-10-02 13:44:25.427,"Training> Name=main_level/agent, Worker=0, Episode=1281, Total reward=35.29, Steps=95494, Training iteration=64"
2019-10-02 13:44:20.426,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_64.pb
2019-10-02 13:44:18.425,Uploaded 3 files for checkpoint 64
2019-10-02 13:44:18.425,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:44:18.425,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:44:12.423,"Policy training> Surrogate loss=-0.0924656093120575, KL divergence=0.0557006411254406, Entropy=1.1114352941513062, training epoch=9, learning_rate=3e-05"
2019-10-02 13:44:12.423,Checkpoint> Saving in path=['./checkpoint/64_Step-95437.ckpt']
2019-10-02 13:43:59.834,"Policy training> Surrogate loss=-0.09426599740982056, KL divergence=0.05214998871088028, Entropy=1.1168913841247559, training epoch=8, learning_rate=3e-05"
2019-10-02 13:43:46.538,"Policy training> Surrogate loss=-0.0859309583902359, KL divergence=0.0516589991748333, Entropy=1.1220946311950684, training epoch=7, learning_rate=3e-05"
2019-10-02 13:43:33.785,"Policy training> Surrogate loss=-0.088660329580307, KL divergence=0.04791118577122688, Entropy=1.134252667427063, training epoch=6, learning_rate=3e-05"
2019-10-02 13:43:20.600,"Policy training> Surrogate loss=-0.08031310141086578, KL divergence=0.04580359160900116, Entropy=1.137784481048584, training epoch=5, learning_rate=3e-05"
2019-10-02 13:43:08.600,"Policy training> Surrogate loss=-0.07677928358316422, KL divergence=0.0449836403131485, Entropy=1.157332181930542, training epoch=4, learning_rate=3e-05"
2019-10-02 13:42:55.312,"Policy training> Surrogate loss=-0.07298854738473892, KL divergence=0.042258370667696, Entropy=1.1489810943603516, training epoch=3, learning_rate=3e-05"
2019-10-02 13:42:43.211,"Policy training> Surrogate loss=-0.05082736164331436, KL divergence=0.045897625386714935, Entropy=1.1539226770401, training epoch=2, learning_rate=3e-05"
2019-10-02 13:42:30.033,"Policy training> Surrogate loss=-0.030762240290641785, KL divergence=0.04789898172020912, Entropy=1.1398340463638306, training epoch=1, learning_rate=3e-05"
2019-10-02 13:42:17.434,"Policy training> Surrogate loss=0.021830087527632713, KL divergence=0.02866896241903305, Entropy=1.1493576765060425, training epoch=0, learning_rate=3e-05"
2019-10-02 13:42:03.110,"Training> Name=main_level/agent, Worker=0, Episode=1280, Total reward=141.42, Steps=95437, Training iteration=63"
2019-10-02 13:41:46.102,"Training> Name=main_level/agent, Worker=0, Episode=1279, Total reward=31.7, Steps=95192, Training iteration=63"
2019-10-02 13:41:41.100,"Training> Name=main_level/agent, Worker=0, Episode=1278, Total reward=36.15, Steps=95123, Training iteration=63"
2019-10-02 13:41:34.098,"Training> Name=main_level/agent, Worker=0, Episode=1277, Total reward=24.78, Steps=95039, Training iteration=63"
2019-10-02 13:41:31.097,"Training> Name=main_level/agent, Worker=0, Episode=1276, Total reward=72.57, Steps=95000, Training iteration=63"
2019-10-02 13:41:21.094,"Training> Name=main_level/agent, Worker=0, Episode=1275, Total reward=157.55, Steps=94851, Training iteration=63"
2019-10-02 13:41:05.089,"Training> Name=main_level/agent, Worker=0, Episode=1274, Total reward=146.58, Steps=94621, Training iteration=63"
2019-10-02 13:40:50.085,"Training> Name=main_level/agent, Worker=0, Episode=1273, Total reward=94.62, Steps=94413, Training iteration=63"
2019-10-02 13:40:37.081,"Training> Name=main_level/agent, Worker=0, Episode=1272, Total reward=14.86, Steps=94213, Training iteration=63"
2019-10-02 13:40:34.080,"Training> Name=main_level/agent, Worker=0, Episode=1271, Total reward=54.42, Steps=94179, Training iteration=63"
2019-10-02 13:40:24.077,"Training> Name=main_level/agent, Worker=0, Episode=1270, Total reward=91.55, Steps=94042, Training iteration=63"
2019-10-02 13:40:09.073,"Training> Name=main_level/agent, Worker=0, Episode=1269, Total reward=11.8, Steps=93832, Training iteration=63"
2019-10-02 13:40:07.072,"Training> Name=main_level/agent, Worker=0, Episode=1268, Total reward=127.99, Steps=93802, Training iteration=63"
2019-10-02 13:39:51.068,"Training> Name=main_level/agent, Worker=0, Episode=1267, Total reward=133.16, Steps=93567, Training iteration=63"
2019-10-02 13:39:34.063,"Training> Name=main_level/agent, Worker=0, Episode=1266, Total reward=17.1, Steps=93337, Training iteration=63"
2019-10-02 13:39:30.062,"Training> Name=main_level/agent, Worker=0, Episode=1265, Total reward=29.08, Steps=93287, Training iteration=63"
2019-10-02 13:39:26.060,"Training> Name=main_level/agent, Worker=0, Episode=1264, Total reward=35.69, Steps=93235, Training iteration=63"
2019-10-02 13:39:21.059,"Training> Name=main_level/agent, Worker=0, Episode=1263, Total reward=25.74, Steps=93159, Training iteration=63"
2019-10-02 13:39:17.058,"Training> Name=main_level/agent, Worker=0, Episode=1262, Total reward=48.59, Steps=93106, Training iteration=63"
2019-10-02 13:39:09.055,"Training> Name=main_level/agent, Worker=0, Episode=1261, Total reward=23.58, Steps=93004, Training iteration=63"
2019-10-02 13:39:05.054,Uploaded 3 files for checkpoint 63
2019-10-02 13:39:05.054,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:39:05.054,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:39:05.054,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_63.pb
2019-10-02 13:39:03.053,Checkpoint> Saving in path=['./checkpoint/63_Step-92967.ckpt']
2019-10-02 13:39:02.053,"Policy training> Surrogate loss=-0.09922356903553009, KL divergence=0.0572616383433342, Entropy=1.1562108993530273, training epoch=9, learning_rate=3e-05"
2019-10-02 13:38:49.695,"Policy training> Surrogate loss=-0.0971841961145401, KL divergence=0.0524960532784462, Entropy=1.1683260202407837, training epoch=8, learning_rate=3e-05"
2019-10-02 13:38:36.370,"Policy training> Surrogate loss=-0.09591488540172577, KL divergence=0.05124839395284653, Entropy=1.1667283773422241, training epoch=7, learning_rate=3e-05"
2019-10-02 13:38:23.005,"Policy training> Surrogate loss=-0.0898430347442627, KL divergence=0.04823382571339607, Entropy=1.1687008142471313, training epoch=6, learning_rate=3e-05"
2019-10-02 13:38:09.564,"Policy training> Surrogate loss=-0.08590350300073624, KL divergence=0.04832526296377182, Entropy=1.1758501529693604, training epoch=5, learning_rate=3e-05"
2019-10-02 13:37:56.556,"Policy training> Surrogate loss=-0.07795418798923492, KL divergence=0.04675125330686569, Entropy=1.2106750011444092, training epoch=4, learning_rate=3e-05"
2019-10-02 13:37:43.087,"Policy training> Surrogate loss=-0.06886732578277588, KL divergence=0.04515416547656059, Entropy=1.1938762664794922, training epoch=3, learning_rate=3e-05"
2019-10-02 13:37:29.715,"Policy training> Surrogate loss=-0.05550893023610115, KL divergence=0.048798732459545135, Entropy=1.218209147453308, training epoch=2, learning_rate=3e-05"
2019-10-02 13:37:16.138,"Policy training> Surrogate loss=-0.027895763516426086, KL divergence=0.06228066235780716, Entropy=1.1729389429092407, training epoch=1, learning_rate=3e-05"
2019-10-02 13:37:02.876,"Policy training> Surrogate loss=0.020314941182732582, KL divergence=0.026648307219147682, Entropy=1.1592366695404053, training epoch=0, learning_rate=3e-05"
2019-10-02 13:36:48.279,"Training> Name=main_level/agent, Worker=0, Episode=1260, Total reward=54.5, Steps=92967, Training iteration=62"
2019-10-02 13:36:39.263,"Training> Name=main_level/agent, Worker=0, Episode=1259, Total reward=43.45, Steps=92847, Training iteration=62"
2019-10-02 13:36:31.261,"Training> Name=main_level/agent, Worker=0, Episode=1258, Total reward=34.43, Steps=92742, Training iteration=62"
2019-10-02 13:36:27.259,"Training> Name=main_level/agent, Worker=0, Episode=1257, Total reward=33.38, Steps=92676, Training iteration=62"
2019-10-02 13:36:22.258,"Training> Name=main_level/agent, Worker=0, Episode=1256, Total reward=199.31, Steps=92612, Training iteration=62"
2019-10-02 13:36:05.253,"Training> Name=main_level/agent, Worker=0, Episode=1255, Total reward=138.99, Steps=92375, Training iteration=62"
2019-10-02 13:35:50.249,"Training> Name=main_level/agent, Worker=0, Episode=1254, Total reward=21.6, Steps=92156, Training iteration=62"
2019-10-02 13:35:45.247,"Training> Name=main_level/agent, Worker=0, Episode=1253, Total reward=17.48, Steps=92095, Training iteration=62"
2019-10-02 13:35:41.246,"Training> Name=main_level/agent, Worker=0, Episode=1252, Total reward=88.68, Steps=92040, Training iteration=62"
2019-10-02 13:35:28.242,"Training> Name=main_level/agent, Worker=0, Episode=1251, Total reward=168.34, Steps=91847, Training iteration=62"
2019-10-02 13:35:13.238,"Training> Name=main_level/agent, Worker=0, Episode=1250, Total reward=120.65, Steps=91638, Training iteration=62"
2019-10-02 13:34:59.234,"Training> Name=main_level/agent, Worker=0, Episode=1249, Total reward=93.38, Steps=91429, Training iteration=62"
2019-10-02 13:34:43.229,"Training> Name=main_level/agent, Worker=0, Episode=1248, Total reward=73.76, Steps=91198, Training iteration=62"
2019-10-02 13:34:32.226,"Training> Name=main_level/agent, Worker=0, Episode=1247, Total reward=19.56, Steps=91036, Training iteration=62"
2019-10-02 13:34:28.225,"Training> Name=main_level/agent, Worker=0, Episode=1246, Total reward=27.73, Steps=90987, Training iteration=62"
2019-10-02 13:34:23.223,"Training> Name=main_level/agent, Worker=0, Episode=1245, Total reward=10.55, Steps=90926, Training iteration=62"
2019-10-02 13:34:21.222,"Training> Name=main_level/agent, Worker=0, Episode=1244, Total reward=71.43, Steps=90902, Training iteration=62"
2019-10-02 13:34:10.219,"Training> Name=main_level/agent, Worker=0, Episode=1243, Total reward=75.09, Steps=90744, Training iteration=62"
2019-10-02 13:33:56.216,"Training> Name=main_level/agent, Worker=0, Episode=1242, Total reward=21.16, Steps=90551, Training iteration=62"
2019-10-02 13:33:54.215,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_62.pb
2019-10-02 13:33:54.215,"Training> Name=main_level/agent, Worker=0, Episode=1241, Total reward=57.26, Steps=90521, Training iteration=62"
2019-10-02 13:33:43.212,Uploaded 3 files for checkpoint 62
2019-10-02 13:33:43.212,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:33:43.212,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:33:41.211,Checkpoint> Saving in path=['./checkpoint/62_Step-90403.ckpt']
2019-10-02 13:33:40.211,"Policy training> Surrogate loss=-0.09300044178962708, KL divergence=0.054436229169368744, Entropy=1.18343186378479, training epoch=9, learning_rate=3e-05"
2019-10-02 13:33:24.841,"Policy training> Surrogate loss=-0.08849017322063446, KL divergence=0.05235568434000015, Entropy=1.205062747001648, training epoch=8, learning_rate=3e-05"
2019-10-02 13:33:10.410,"Policy training> Surrogate loss=-0.08700190484523773, KL divergence=0.04946598410606384, Entropy=1.195643424987793, training epoch=7, learning_rate=3e-05"
2019-10-02 13:32:54.599,"Policy training> Surrogate loss=-0.08125176280736923, KL divergence=0.04985132813453674, Entropy=1.2032709121704102, training epoch=6, learning_rate=3e-05"
2019-10-02 13:32:39.652,"Policy training> Surrogate loss=-0.07672888040542603, KL divergence=0.048334650695323944, Entropy=1.213924765586853, training epoch=5, learning_rate=3e-05"
2019-10-02 13:32:24.269,"Policy training> Surrogate loss=-0.0713237002491951, KL divergence=0.04544062539935112, Entropy=1.22189199924469, training epoch=4, learning_rate=3e-05"
2019-10-02 13:32:08.621,"Policy training> Surrogate loss=-0.06384876370429993, KL divergence=0.04605927690863609, Entropy=1.2257786989212036, training epoch=3, learning_rate=3e-05"
2019-10-02 13:31:52.840,"Policy training> Surrogate loss=-0.04749324917793274, KL divergence=0.05592087656259537, Entropy=1.229162573814392, training epoch=2, learning_rate=3e-05"
2019-10-02 13:31:37.466,"Policy training> Surrogate loss=-0.027386412024497986, KL divergence=0.06756491959095001, Entropy=1.2436444759368896, training epoch=1, learning_rate=3e-05"
2019-10-02 13:31:22.048,"Policy training> Surrogate loss=0.026809724047780037, KL divergence=0.039895787835121155, Entropy=1.164465069770813, training epoch=0, learning_rate=3e-05"
2019-10-02 13:31:05.783,"Training> Name=main_level/agent, Worker=0, Episode=1240, Total reward=46.88, Steps=90403, Training iteration=61"
2019-10-02 13:30:57.781,"Training> Name=main_level/agent, Worker=0, Episode=1239, Total reward=12.81, Steps=90300, Training iteration=61"
2019-10-02 13:30:55.780,"Training> Name=main_level/agent, Worker=0, Episode=1238, Total reward=152.53, Steps=90271, Training iteration=61"
2019-10-02 13:30:39.776,"Training> Name=main_level/agent, Worker=0, Episode=1237, Total reward=59.61, Steps=90038, Training iteration=61"
2019-10-02 13:30:30.773,"Training> Name=main_level/agent, Worker=0, Episode=1236, Total reward=68.84, Steps=89915, Training iteration=61"
2019-10-02 13:30:18.770,"Training> Name=main_level/agent, Worker=0, Episode=1235, Total reward=85.52, Steps=89750, Training iteration=61"
2019-10-02 13:30:06.766,"Training> Name=main_level/agent, Worker=0, Episode=1234, Total reward=196.89, Steps=89574, Training iteration=61"
2019-10-02 13:29:52.762,"Training> Name=main_level/agent, Worker=0, Episode=1233, Total reward=13.93, Steps=89379, Training iteration=61"
2019-10-02 13:29:48.761,"Training> Name=main_level/agent, Worker=0, Episode=1232, Total reward=41.61, Steps=89330, Training iteration=61"
2019-10-02 13:29:40.758,"Training> Name=main_level/agent, Worker=0, Episode=1231, Total reward=30.22, Steps=89215, Training iteration=61"
2019-10-02 13:29:35.757,"Training> Name=main_level/agent, Worker=0, Episode=1230, Total reward=133.82, Steps=89157, Training iteration=61"
2019-10-02 13:29:20.753,"Training> Name=main_level/agent, Worker=0, Episode=1229, Total reward=101.15, Steps=88942, Training iteration=61"
2019-10-02 13:29:05.748,"Training> Name=main_level/agent, Worker=0, Episode=1228, Total reward=192.54, Steps=88721, Training iteration=61"
2019-10-02 13:28:51.744,"Training> Name=main_level/agent, Worker=0, Episode=1227, Total reward=54.53, Steps=88512, Training iteration=61"
2019-10-02 13:28:41.741,"Training> Name=main_level/agent, Worker=0, Episode=1226, Total reward=66.98, Steps=88377, Training iteration=61"
2019-10-02 13:28:27.737,"Training> Name=main_level/agent, Worker=0, Episode=1225, Total reward=34.78, Steps=88180, Training iteration=61"
2019-10-02 13:28:21.736,"Training> Name=main_level/agent, Worker=0, Episode=1224, Total reward=175.18, Steps=88094, Training iteration=61"
2019-10-02 13:28:08.732,"Training> Name=main_level/agent, Worker=0, Episode=1223, Total reward=129.52, Steps=87910, Training iteration=61"
2019-10-02 13:27:55.728,"Training> Name=main_level/agent, Worker=0, Episode=1222, Total reward=85.54, Steps=87716, Training iteration=61"
2019-10-02 13:27:42.724,"Training> Name=main_level/agent, Worker=0, Episode=1221, Total reward=45.71, Steps=87537, Training iteration=61"
2019-10-02 13:27:34.722,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_61.pb
2019-10-02 13:27:33.722,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:27:33.722,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:27:33.721,Uploaded 3 files for checkpoint 61
2019-10-02 13:27:31.721,"Policy training> Surrogate loss=-0.09314022213220596, KL divergence=0.04950224235653877, Entropy=1.184959888458252, training epoch=9, learning_rate=3e-05"
2019-10-02 13:27:31.721,Checkpoint> Saving in path=['./checkpoint/61_Step-87442.ckpt']
2019-10-02 13:27:16.342,"Policy training> Surrogate loss=-0.08841227740049362, KL divergence=0.048187050968408585, Entropy=1.185939908027649, training epoch=8, learning_rate=3e-05"
2019-10-02 13:27:01.727,"Policy training> Surrogate loss=-0.08652862161397934, KL divergence=0.04754523187875748, Entropy=1.1918953657150269, training epoch=7, learning_rate=3e-05"
2019-10-02 13:26:47.044,"Policy training> Surrogate loss=-0.08016576617956161, KL divergence=0.04599401727318764, Entropy=1.204645037651062, training epoch=6, learning_rate=3e-05"
2019-10-02 13:26:32.502,"Policy training> Surrogate loss=-0.08021067082881927, KL divergence=0.044677332043647766, Entropy=1.203999638557434, training epoch=5, learning_rate=3e-05"
2019-10-02 13:26:18.241,"Policy training> Surrogate loss=-0.07127227634191513, KL divergence=0.043234847486019135, Entropy=1.2222191095352173, training epoch=4, learning_rate=3e-05"
2019-10-02 13:26:03.771,"Policy training> Surrogate loss=-0.06439346820116043, KL divergence=0.04568946734070778, Entropy=1.226502537727356, training epoch=3, learning_rate=3e-05"
2019-10-02 13:25:49.895,"Policy training> Surrogate loss=-0.05507592856884003, KL divergence=0.04910533130168915, Entropy=1.253697395324707, training epoch=2, learning_rate=3e-05"
2019-10-02 13:25:35.071,"Policy training> Surrogate loss=-0.02858586423099041, KL divergence=0.058727383613586426, Entropy=1.2199410200119019, training epoch=1, learning_rate=3e-05"
2019-10-02 13:25:20.483,"Policy training> Surrogate loss=0.018973493948578835, KL divergence=0.02720789797604084, Entropy=1.2233392000198364, training epoch=0, learning_rate=3e-05"
2019-10-02 13:25:04.259,"Training> Name=main_level/agent, Worker=0, Episode=1220, Total reward=56.21, Steps=87442, Training iteration=60"
2019-10-02 13:24:55.256,"Training> Name=main_level/agent, Worker=0, Episode=1219, Total reward=14.77, Steps=87325, Training iteration=60"
2019-10-02 13:24:52.255,"Training> Name=main_level/agent, Worker=0, Episode=1218, Total reward=89.71, Steps=87287, Training iteration=60"
2019-10-02 13:24:39.251,"Training> Name=main_level/agent, Worker=0, Episode=1217, Total reward=27.69, Steps=87093, Training iteration=60"
2019-10-02 13:24:35.250,"Training> Name=main_level/agent, Worker=0, Episode=1216, Total reward=150.02, Steps=87046, Training iteration=60"
2019-10-02 13:24:19.245,"Training> Name=main_level/agent, Worker=0, Episode=1215, Total reward=105.82, Steps=86819, Training iteration=60"
2019-10-02 13:24:02.240,"Training> Name=main_level/agent, Worker=0, Episode=1214, Total reward=198.49, Steps=86570, Training iteration=60"
2019-10-02 13:23:48.236,"Training> Name=main_level/agent, Worker=0, Episode=1213, Total reward=90.12, Steps=86364, Training iteration=60"
2019-10-02 13:23:34.232,"Training> Name=main_level/agent, Worker=0, Episode=1212, Total reward=107.47, Steps=86163, Training iteration=60"
2019-10-02 13:23:19.228,"Training> Name=main_level/agent, Worker=0, Episode=1211, Total reward=131.32, Steps=85947, Training iteration=60"
2019-10-02 13:23:05.224,"Training> Name=main_level/agent, Worker=0, Episode=1210, Total reward=80.27, Steps=85751, Training iteration=60"
2019-10-02 13:22:54.221,"Training> Name=main_level/agent, Worker=0, Episode=1209, Total reward=24.36, Steps=85592, Training iteration=60"
2019-10-02 13:22:49.220,"Training> Name=main_level/agent, Worker=0, Episode=1208, Total reward=13.76, Steps=85521, Training iteration=60"
2019-10-02 13:22:46.219,"Training> Name=main_level/agent, Worker=0, Episode=1207, Total reward=196.73, Steps=85485, Training iteration=60"
2019-10-02 13:22:29.214,"Training> Name=main_level/agent, Worker=0, Episode=1206, Total reward=14.29, Steps=85246, Training iteration=60"
2019-10-02 13:22:25.213,"Training> Name=main_level/agent, Worker=0, Episode=1205, Total reward=61.42, Steps=85194, Training iteration=60"
2019-10-02 13:22:16.210,"Training> Name=main_level/agent, Worker=0, Episode=1204, Total reward=16.78, Steps=85067, Training iteration=60"
2019-10-02 13:22:13.209,"Training> Name=main_level/agent, Worker=0, Episode=1203, Total reward=66.03, Steps=85027, Training iteration=60"
2019-10-02 13:22:01.206,"Training> Name=main_level/agent, Worker=0, Episode=1202, Total reward=46.58, Steps=84870, Training iteration=60"
2019-10-02 13:21:55.204,"Training> Name=main_level/agent, Worker=0, Episode=1201, Total reward=113.25, Steps=84787, Training iteration=60"
2019-10-02 13:21:42.200,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_60.pb
2019-10-02 13:21:41.200,Uploaded 3 files for checkpoint 60
2019-10-02 13:21:41.200,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:21:41.200,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:21:39.199,"Policy training> Surrogate loss=-0.0998811200261116, KL divergence=0.05829695612192154, Entropy=1.2623018026351929, training epoch=9, learning_rate=3e-05"
2019-10-02 13:21:39.199,Checkpoint> Saving in path=['./checkpoint/60_Step-84605.ckpt']
2019-10-02 13:21:25.331,"Policy training> Surrogate loss=-0.10013435781002045, KL divergence=0.05607057735323906, Entropy=1.267343282699585, training epoch=8, learning_rate=3e-05"
2019-10-02 13:21:12.280,"Policy training> Surrogate loss=-0.09737370163202286, KL divergence=0.05808303505182266, Entropy=1.2641303539276123, training epoch=7, learning_rate=3e-05"
2019-10-02 13:20:58.473,"Policy training> Surrogate loss=-0.09037231653928757, KL divergence=0.05490487813949585, Entropy=1.2764543294906616, training epoch=6, learning_rate=3e-05"
2019-10-02 13:20:44.153,"Policy training> Surrogate loss=-0.08874796330928802, KL divergence=0.051989782601594925, Entropy=1.2907147407531738, training epoch=5, learning_rate=3e-05"
2019-10-02 13:20:31.058,"Policy training> Surrogate loss=-0.0783047080039978, KL divergence=0.050274498760700226, Entropy=1.299841284751892, training epoch=4, learning_rate=3e-05"
2019-10-02 13:20:18.581,"Policy training> Surrogate loss=-0.06838640570640564, KL divergence=0.05212387442588806, Entropy=1.3012433052062988, training epoch=3, learning_rate=3e-05"
2019-10-02 13:20:05.009,"Policy training> Surrogate loss=-0.05964189022779465, KL divergence=0.06071116775274277, Entropy=1.3008991479873657, training epoch=2, learning_rate=3e-05"
2019-10-02 13:19:52.429,"Policy training> Surrogate loss=-0.023296182975172997, KL divergence=0.08077763020992279, Entropy=1.3657290935516357, training epoch=1, learning_rate=3e-05"
2019-10-02 13:19:39.270,"Policy training> Surrogate loss=0.015347396023571491, KL divergence=0.03321482986211777, Entropy=1.2921090126037598, training epoch=0, learning_rate=3e-05"
2019-10-02 13:19:23.765,"Training> Name=main_level/agent, Worker=0, Episode=1200, Total reward=72.22, Steps=84605, Training iteration=59"
2019-10-02 13:19:13.762,"Training> Name=main_level/agent, Worker=0, Episode=1199, Total reward=14.39, Steps=84466, Training iteration=59"
2019-10-02 13:19:11.761,"Training> Name=main_level/agent, Worker=0, Episode=1198, Total reward=35.19, Steps=84435, Training iteration=59"
2019-10-02 13:19:05.759,"Training> Name=main_level/agent, Worker=0, Episode=1197, Total reward=53.52, Steps=84352, Training iteration=59"
2019-10-02 13:18:57.757,"Training> Name=main_level/agent, Worker=0, Episode=1196, Total reward=80.18, Steps=84251, Training iteration=59"
2019-10-02 13:18:45.754,"Training> Name=main_level/agent, Worker=0, Episode=1195, Total reward=61.23, Steps=84066, Training iteration=59"
2019-10-02 13:18:35.751,"Training> Name=main_level/agent, Worker=0, Episode=1194, Total reward=19.14, Steps=83936, Training iteration=59"
2019-10-02 13:18:32.750,"Training> Name=main_level/agent, Worker=0, Episode=1193, Total reward=41.84, Steps=83898, Training iteration=59"
2019-10-02 13:18:23.747,"Training> Name=main_level/agent, Worker=0, Episode=1192, Total reward=23.34, Steps=83775, Training iteration=59"
2019-10-02 13:18:19.746,"Training> Name=main_level/agent, Worker=0, Episode=1191, Total reward=150.21, Steps=83713, Training iteration=59"
2019-10-02 13:18:05.742,"Training> Name=main_level/agent, Worker=0, Episode=1190, Total reward=160.64, Steps=83507, Training iteration=59"
2019-10-02 13:17:51.738,"Training> Name=main_level/agent, Worker=0, Episode=1189, Total reward=14.06, Steps=83316, Training iteration=59"
2019-10-02 13:17:48.737,"Training> Name=main_level/agent, Worker=0, Episode=1188, Total reward=45.56, Steps=83279, Training iteration=59"
2019-10-02 13:17:40.735,"Training> Name=main_level/agent, Worker=0, Episode=1187, Total reward=148.07, Steps=83162, Training iteration=59"
2019-10-02 13:17:25.730,"Training> Name=main_level/agent, Worker=0, Episode=1186, Total reward=51.92, Steps=82942, Training iteration=59"
2019-10-02 13:17:16.728,"Training> Name=main_level/agent, Worker=0, Episode=1185, Total reward=44.95, Steps=82818, Training iteration=59"
2019-10-02 13:17:07.725,"Training> Name=main_level/agent, Worker=0, Episode=1184, Total reward=66.95, Steps=82699, Training iteration=59"
2019-10-02 13:16:58.722,"Training> Name=main_level/agent, Worker=0, Episode=1183, Total reward=144.12, Steps=82568, Training iteration=59"
2019-10-02 13:16:45.719,"Training> Name=main_level/agent, Worker=0, Episode=1182, Total reward=99.44, Steps=82374, Training iteration=59"
2019-10-02 13:16:32.715,"Training> Name=main_level/agent, Worker=0, Episode=1181, Total reward=114.06, Steps=82188, Training iteration=59"
2019-10-02 13:16:24.713,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_59.pb
2019-10-02 13:16:16.710,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:16:16.710,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:16:15.710,Uploaded 3 files for checkpoint 59
2019-10-02 13:16:14.709,Checkpoint> Saving in path=['./checkpoint/59_Step-81997.ckpt']
2019-10-02 13:16:13.709,"Policy training> Surrogate loss=-0.10165121406316757, KL divergence=0.05348065868020058, Entropy=1.2555041313171387, training epoch=9, learning_rate=3e-05"
2019-10-02 13:15:58.956,"Policy training> Surrogate loss=-0.09990409016609192, KL divergence=0.05203801020979881, Entropy=1.252856731414795, training epoch=8, learning_rate=3e-05"
2019-10-02 13:15:44.905,"Policy training> Surrogate loss=-0.09604337811470032, KL divergence=0.048902273178100586, Entropy=1.263694405555725, training epoch=7, learning_rate=3e-05"
2019-10-02 13:15:31.186,"Policy training> Surrogate loss=-0.09237262606620789, KL divergence=0.048719972372055054, Entropy=1.2588801383972168, training epoch=6, learning_rate=3e-05"
2019-10-02 13:15:16.682,"Policy training> Surrogate loss=-0.08791083097457886, KL divergence=0.044806987047195435, Entropy=1.281698226928711, training epoch=5, learning_rate=3e-05"
2019-10-02 13:15:01.086,"Policy training> Surrogate loss=-0.08166815340518951, KL divergence=0.04368509724736214, Entropy=1.2896703481674194, training epoch=4, learning_rate=3e-05"
2019-10-02 13:14:47.157,"Policy training> Surrogate loss=-0.07079894840717316, KL divergence=0.044702887535095215, Entropy=1.3113898038864136, training epoch=3, learning_rate=3e-05"
2019-10-02 13:14:33.343,"Policy training> Surrogate loss=-0.05958835408091545, KL divergence=0.04488780349493027, Entropy=1.3249719142913818, training epoch=2, learning_rate=3e-05"
2019-10-02 13:14:19.138,"Policy training> Surrogate loss=-0.03465346246957779, KL divergence=0.04805159568786621, Entropy=1.2927073240280151, training epoch=1, learning_rate=3e-05"
2019-10-02 13:14:04.167,"Policy training> Surrogate loss=0.01979014463722706, KL divergence=0.027473976835608482, Entropy=1.3078750371932983, training epoch=0, learning_rate=3e-05"
2019-10-02 13:13:47.093,"Training> Name=main_level/agent, Worker=0, Episode=1180, Total reward=68.7, Steps=81997, Training iteration=58"
2019-10-02 13:13:35.084,"Training> Name=main_level/agent, Worker=0, Episode=1179, Total reward=126.11, Steps=81825, Training iteration=58"
2019-10-02 13:13:20.080,"Training> Name=main_level/agent, Worker=0, Episode=1178, Total reward=37.94, Steps=81612, Training iteration=58"
2019-10-02 13:13:13.078,"Training> Name=main_level/agent, Worker=0, Episode=1177, Total reward=43.22, Steps=81528, Training iteration=58"
2019-10-02 13:13:07.076,"Training> Name=main_level/agent, Worker=0, Episode=1176, Total reward=39.04, Steps=81441, Training iteration=58"
2019-10-02 13:13:01.074,"Training> Name=main_level/agent, Worker=0, Episode=1175, Total reward=91.82, Steps=81358, Training iteration=58"
2019-10-02 13:12:47.070,"Training> Name=main_level/agent, Worker=0, Episode=1174, Total reward=64.99, Steps=81157, Training iteration=58"
2019-10-02 13:12:37.067,"Training> Name=main_level/agent, Worker=0, Episode=1173, Total reward=60.61, Steps=81012, Training iteration=58"
2019-10-02 13:12:27.064,"Training> Name=main_level/agent, Worker=0, Episode=1172, Total reward=153.48, Steps=80876, Training iteration=58"
2019-10-02 13:12:13.060,"Training> Name=main_level/agent, Worker=0, Episode=1171, Total reward=69.63, Steps=80676, Training iteration=58"
2019-10-02 13:12:03.057,"Training> Name=main_level/agent, Worker=0, Episode=1170, Total reward=88.74, Steps=80534, Training iteration=58"
2019-10-02 13:11:50.054,"Training> Name=main_level/agent, Worker=0, Episode=1169, Total reward=136.73, Steps=80350, Training iteration=58"
2019-10-02 13:11:35.050,"Training> Name=main_level/agent, Worker=0, Episode=1168, Total reward=20.23, Steps=80139, Training iteration=58"
2019-10-02 13:11:32.049,"Training> Name=main_level/agent, Worker=0, Episode=1167, Total reward=15.66, Steps=80095, Training iteration=58"
2019-10-02 13:11:29.048,"Training> Name=main_level/agent, Worker=0, Episode=1166, Total reward=121.85, Steps=80055, Training iteration=58"
2019-10-02 13:11:14.044,"Training> Name=main_level/agent, Worker=0, Episode=1165, Total reward=26.02, Steps=79847, Training iteration=58"
2019-10-02 13:11:07.041,"Training> Name=main_level/agent, Worker=0, Episode=1164, Total reward=129.31, Steps=79787, Training iteration=58"
2019-10-02 13:10:53.037,"Training> Name=main_level/agent, Worker=0, Episode=1163, Total reward=46.79, Steps=79583, Training iteration=58"
2019-10-02 13:10:47.035,"Training> Name=main_level/agent, Worker=0, Episode=1162, Total reward=130.96, Steps=79491, Training iteration=58"
2019-10-02 13:10:33.031,"Training> Name=main_level/agent, Worker=0, Episode=1161, Total reward=23.0, Steps=79291, Training iteration=58"
2019-10-02 13:10:28.030,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_58.pb
2019-10-02 13:10:27.029,Uploaded 3 files for checkpoint 58
2019-10-02 13:10:27.029,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:10:27.029,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:10:23.028,"Policy training> Surrogate loss=-0.1084187924861908, KL divergence=0.05860895290970802, Entropy=1.2319403886795044, training epoch=9, learning_rate=3e-05"
2019-10-02 13:10:23.028,Checkpoint> Saving in path=['./checkpoint/58_Step-79242.ckpt']
2019-10-02 13:10:10.768,"Policy training> Surrogate loss=-0.10595468431711197, KL divergence=0.05744914337992668, Entropy=1.2407294511795044, training epoch=8, learning_rate=3e-05"
2019-10-02 13:09:57.088,"Policy training> Surrogate loss=-0.10253850370645523, KL divergence=0.055993806570768356, Entropy=1.2399801015853882, training epoch=7, learning_rate=3e-05"
2019-10-02 13:09:44.738,"Policy training> Surrogate loss=-0.10066572576761246, KL divergence=0.0514884777367115, Entropy=1.2566144466400146, training epoch=6, learning_rate=3e-05"
2019-10-02 13:09:31.078,"Policy training> Surrogate loss=-0.08894822746515274, KL divergence=0.04980697110295296, Entropy=1.2564687728881836, training epoch=5, learning_rate=3e-05"
2019-10-02 13:09:19.412,"Policy training> Surrogate loss=-0.08552920818328857, KL divergence=0.04788726568222046, Entropy=1.2616387605667114, training epoch=4, learning_rate=3e-05"
2019-10-02 13:09:06.515,"Policy training> Surrogate loss=-0.07994989305734634, KL divergence=0.04522999748587608, Entropy=1.2604725360870361, training epoch=3, learning_rate=3e-05"
2019-10-02 13:08:53.647,"Policy training> Surrogate loss=-0.06193213537335396, KL divergence=0.04249119013547897, Entropy=1.2898905277252197, training epoch=2, learning_rate=3e-05"
2019-10-02 13:08:40.326,"Policy training> Surrogate loss=-0.04212159663438797, KL divergence=0.05168147012591362, Entropy=1.268489956855774, training epoch=1, learning_rate=3e-05"
2019-10-02 13:08:27.777,"Policy training> Surrogate loss=0.020009124651551247, KL divergence=0.030790453776717186, Entropy=1.2678310871124268, training epoch=0, learning_rate=3e-05"
2019-10-02 13:08:12.018,"Training> Name=main_level/agent, Worker=0, Episode=1160, Total reward=132.5, Steps=79242, Training iteration=57"
2019-10-02 13:07:50.011,"Training> Name=main_level/agent, Worker=0, Episode=1159, Total reward=67.64, Steps=79019, Training iteration=57"
2019-10-02 13:07:38.008,"Training> Name=main_level/agent, Worker=0, Episode=1158, Total reward=29.42, Steps=78856, Training iteration=57"
2019-10-02 13:07:33.006,"Training> Name=main_level/agent, Worker=0, Episode=1157, Total reward=71.32, Steps=78793, Training iteration=57"
2019-10-02 13:07:24.004,"Training> Name=main_level/agent, Worker=0, Episode=1156, Total reward=31.09, Steps=78653, Training iteration=57"
2019-10-02 13:07:18.002,"Training> Name=main_level/agent, Worker=0, Episode=1155, Total reward=148.91, Steps=78583, Training iteration=57"
2019-10-02 13:07:02.997,"Training> Name=main_level/agent, Worker=0, Episode=1154, Total reward=59.51, Steps=78360, Training iteration=57"
2019-10-02 13:06:53.995,"Training> Name=main_level/agent, Worker=0, Episode=1153, Total reward=22.79, Steps=78237, Training iteration=57"
2019-10-02 13:06:49.993,"Training> Name=main_level/agent, Worker=0, Episode=1152, Total reward=94.12, Steps=78174, Training iteration=57"
2019-10-02 13:06:34.989,"Training> Name=main_level/agent, Worker=0, Episode=1151, Total reward=50.82, Steps=77968, Training iteration=57"
2019-10-02 13:06:26.987,"Training> Name=main_level/agent, Worker=0, Episode=1150, Total reward=36.38, Steps=77854, Training iteration=57"
2019-10-02 13:06:19.985,"Training> Name=main_level/agent, Worker=0, Episode=1149, Total reward=12.71, Steps=77758, Training iteration=57"
2019-10-02 13:06:16.984,"Training> Name=main_level/agent, Worker=0, Episode=1148, Total reward=29.01, Steps=77725, Training iteration=57"
2019-10-02 13:06:10.982,"Training> Name=main_level/agent, Worker=0, Episode=1147, Total reward=86.69, Steps=77643, Training iteration=57"
2019-10-02 13:05:57.978,"Training> Name=main_level/agent, Worker=0, Episode=1146, Total reward=16.36, Steps=77455, Training iteration=57"
2019-10-02 13:05:54.977,"Training> Name=main_level/agent, Worker=0, Episode=1145, Total reward=36.99, Steps=77416, Training iteration=57"
2019-10-02 13:05:46.975,"Training> Name=main_level/agent, Worker=0, Episode=1144, Total reward=192.04, Steps=77309, Training iteration=57"
2019-10-02 13:05:32.971,"Training> Name=main_level/agent, Worker=0, Episode=1143, Total reward=135.82, Steps=77100, Training iteration=57"
2019-10-02 13:05:17.966,"Training> Name=main_level/agent, Worker=0, Episode=1142, Total reward=49.04, Steps=76883, Training iteration=57"
2019-10-02 13:05:10.964,"Training> Name=main_level/agent, Worker=0, Episode=1141, Total reward=35.15, Steps=76787, Training iteration=57"
2019-10-02 13:05:04.963,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:05:04.963,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:05:04.963,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_57.pb
2019-10-02 13:05:04.962,Uploaded 3 files for checkpoint 57
2019-10-02 13:05:02.962,Checkpoint> Saving in path=['./checkpoint/57_Step-76731.ckpt']
2019-10-02 13:05:01.961,"Policy training> Surrogate loss=-0.11020594835281372, KL divergence=0.05031750351190567, Entropy=1.3181343078613281, training epoch=9, learning_rate=3e-05"
2019-10-02 13:04:52.760,"Policy training> Surrogate loss=-0.11223342269659042, KL divergence=0.05095775052905083, Entropy=1.3243190050125122, training epoch=8, learning_rate=3e-05"
2019-10-02 13:04:43.515,"Policy training> Surrogate loss=-0.10467025637626648, KL divergence=0.04924701154232025, Entropy=1.318176031112671, training epoch=7, learning_rate=3e-05"
2019-10-02 13:04:33.868,"Policy training> Surrogate loss=-0.10516120493412018, KL divergence=0.045663703233003616, Entropy=1.3325974941253662, training epoch=6, learning_rate=3e-05"
2019-10-02 13:04:25.123,"Policy training> Surrogate loss=-0.09431952983140945, KL divergence=0.044850658625364304, Entropy=1.3267152309417725, training epoch=5, learning_rate=3e-05"
2019-10-02 13:04:15.557,"Policy training> Surrogate loss=-0.09514614939689636, KL divergence=0.04338664188981056, Entropy=1.3257616758346558, training epoch=4, learning_rate=3e-05"
2019-10-02 13:04:05.921,"Policy training> Surrogate loss=-0.08645366877317429, KL divergence=0.04253539443016052, Entropy=1.3510266542434692, training epoch=3, learning_rate=3e-05"
2019-10-02 13:03:55.593,"Policy training> Surrogate loss=-0.07051333039999008, KL divergence=0.04355794936418533, Entropy=1.3659354448318481, training epoch=2, learning_rate=3e-05"
2019-10-02 13:03:45.971,"Policy training> Surrogate loss=-0.046874143183231354, KL divergence=0.046522144228219986, Entropy=1.3416721820831299, training epoch=1, learning_rate=3e-05"
2019-10-02 13:03:37.176,"Policy training> Surrogate loss=0.010704675689339638, KL divergence=0.02226073667407036, Entropy=1.326539397239685, training epoch=0, learning_rate=3e-05"
2019-10-02 13:03:25.998,"Training> Name=main_level/agent, Worker=0, Episode=1140, Total reward=37.33, Steps=76731, Training iteration=56"
2019-10-02 13:03:19.995,"Training> Name=main_level/agent, Worker=0, Episode=1139, Total reward=76.94, Steps=76648, Training iteration=56"
2019-10-02 13:03:07.991,"Training> Name=main_level/agent, Worker=0, Episode=1138, Total reward=27.25, Steps=76482, Training iteration=56"
2019-10-02 13:03:03.990,"Training> Name=main_level/agent, Worker=0, Episode=1137, Total reward=26.61, Steps=76431, Training iteration=56"
2019-10-02 13:03:00.989,"Training> Name=main_level/agent, Worker=0, Episode=1136, Total reward=50.41, Steps=76384, Training iteration=56"
2019-10-02 13:02:52.987,"Training> Name=main_level/agent, Worker=0, Episode=1135, Total reward=85.19, Steps=76278, Training iteration=56"
2019-10-02 13:02:39.983,"Training> Name=main_level/agent, Worker=0, Episode=1134, Total reward=165.82, Steps=76089, Training iteration=56"
2019-10-02 13:02:25.979,"Training> Name=main_level/agent, Worker=0, Episode=1133, Total reward=35.22, Steps=75886, Training iteration=56"
2019-10-02 13:02:18.977,"Training> Name=main_level/agent, Worker=0, Episode=1132, Total reward=46.78, Steps=75793, Training iteration=56"
2019-10-02 13:02:10.974,"Training> Name=main_level/agent, Worker=0, Episode=1131, Total reward=20.4, Steps=75690, Training iteration=56"
2019-10-02 13:02:07.973,"Training> Name=main_level/agent, Worker=0, Episode=1130, Total reward=7.8, Steps=75640, Training iteration=56"
2019-10-02 13:02:05.973,"Training> Name=main_level/agent, Worker=0, Episode=1129, Total reward=217.55, Steps=75622, Training iteration=56"
2019-10-02 13:01:50.968,"Training> Name=main_level/agent, Worker=0, Episode=1128, Total reward=18.05, Steps=75405, Training iteration=56"
2019-10-02 13:01:47.967,"Training> Name=main_level/agent, Worker=0, Episode=1127, Total reward=17.59, Steps=75369, Training iteration=56"
2019-10-02 13:01:43.966,"Training> Name=main_level/agent, Worker=0, Episode=1126, Total reward=44.66, Steps=75314, Training iteration=56"
2019-10-02 13:01:35.964,"Training> Name=main_level/agent, Worker=0, Episode=1125, Total reward=21.6, Steps=75206, Training iteration=56"
2019-10-02 13:01:31.963,"Training> Name=main_level/agent, Worker=0, Episode=1124, Total reward=33.43, Steps=75154, Training iteration=56"
2019-10-02 13:01:26.961,"Training> Name=main_level/agent, Worker=0, Episode=1123, Total reward=54.63, Steps=75082, Training iteration=56"
2019-10-02 13:01:18.958,"Training> Name=main_level/agent, Worker=0, Episode=1122, Total reward=12.22, Steps=74970, Training iteration=56"
2019-10-02 13:01:15.958,"Training> Name=main_level/agent, Worker=0, Episode=1121, Total reward=39.58, Steps=74947, Training iteration=56"
2019-10-02 13:01:06.955,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_56.pb
2019-10-02 13:01:05.955,Uploaded 3 files for checkpoint 56
2019-10-02 13:01:05.955,INFO:tensorflow:Froze 11 variables.
2019-10-02 13:01:05.955,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 13:01:03.954,"Policy training> Surrogate loss=-0.11085551232099533, KL divergence=0.05169668048620224, Entropy=1.2675318717956543, training epoch=9, learning_rate=3e-05"
2019-10-02 13:01:03.954,Checkpoint> Saving in path=['./checkpoint/56_Step-74839.ckpt']
2019-10-02 13:00:53.716,"Policy training> Surrogate loss=-0.10818219929933548, KL divergence=0.04967382922768593, Entropy=1.280530571937561, training epoch=8, learning_rate=3e-05"
2019-10-02 13:00:43.321,"Policy training> Surrogate loss=-0.10598301887512207, KL divergence=0.047602578997612, Entropy=1.2775095701217651, training epoch=7, learning_rate=3e-05"
2019-10-02 13:00:32.365,"Policy training> Surrogate loss=-0.10412593930959702, KL divergence=0.044597040861845016, Entropy=1.299848198890686, training epoch=6, learning_rate=3e-05"
2019-10-02 13:00:21.871,"Policy training> Surrogate loss=-0.09981867671012878, KL divergence=0.040044043213129044, Entropy=1.2967195510864258, training epoch=5, learning_rate=3e-05"
2019-10-02 13:00:11.142,"Policy training> Surrogate loss=-0.09109319001436234, KL divergence=0.039248913526535034, Entropy=1.3100364208221436, training epoch=4, learning_rate=3e-05"
2019-10-02 13:00:01.508,"Policy training> Surrogate loss=-0.08520343899726868, KL divergence=0.03705739974975586, Entropy=1.318077564239502, training epoch=3, learning_rate=3e-05"
2019-10-02 12:59:50.210,"Policy training> Surrogate loss=-0.06965351104736328, KL divergence=0.03623246029019356, Entropy=1.3369596004486084, training epoch=2, learning_rate=3e-05"
2019-10-02 12:59:39.382,"Policy training> Surrogate loss=-0.04097475856542587, KL divergence=0.03825636953115463, Entropy=1.367392659187317, training epoch=1, learning_rate=3e-05"
2019-10-02 12:59:30.032,"Policy training> Surrogate loss=0.014269913546741009, KL divergence=0.026245929300785065, Entropy=1.2741018533706665, training epoch=0, learning_rate=3e-05"
2019-10-02 12:59:17.545,"Training> Name=main_level/agent, Worker=0, Episode=1120, Total reward=50.64, Steps=74839, Training iteration=55"
2019-10-02 12:59:07.542,"Training> Name=main_level/agent, Worker=0, Episode=1119, Total reward=9.71, Steps=74698, Training iteration=55"
2019-10-02 12:59:05.541,"Training> Name=main_level/agent, Worker=0, Episode=1118, Total reward=30.52, Steps=74676, Training iteration=55"
2019-10-02 12:59:01.540,"Training> Name=main_level/agent, Worker=0, Episode=1117, Total reward=45.19, Steps=74624, Training iteration=55"
2019-10-02 12:58:56.538,"Training> Name=main_level/agent, Worker=0, Episode=1116, Total reward=87.05, Steps=74544, Training iteration=55"
2019-10-02 12:58:41.534,"Training> Name=main_level/agent, Worker=0, Episode=1115, Total reward=34.81, Steps=74330, Training iteration=55"
2019-10-02 12:58:35.532,"Training> Name=main_level/agent, Worker=0, Episode=1114, Total reward=49.47, Steps=74253, Training iteration=55"
2019-10-02 12:58:27.530,"Training> Name=main_level/agent, Worker=0, Episode=1113, Total reward=11.85, Steps=74154, Training iteration=55"
2019-10-02 12:58:24.529,"Training> Name=main_level/agent, Worker=0, Episode=1112, Total reward=11.24, Steps=74121, Training iteration=55"
2019-10-02 12:58:22.528,"Training> Name=main_level/agent, Worker=0, Episode=1111, Total reward=91.43, Steps=74088, Training iteration=55"
2019-10-02 12:58:08.524,"Training> Name=main_level/agent, Worker=0, Episode=1110, Total reward=69.71, Steps=73889, Training iteration=55"
2019-10-02 12:57:57.521,"Training> Name=main_level/agent, Worker=0, Episode=1109, Total reward=9.93, Steps=73737, Training iteration=55"
2019-10-02 12:57:55.520,"Training> Name=main_level/agent, Worker=0, Episode=1108, Total reward=82.06, Steps=73715, Training iteration=55"
2019-10-02 12:57:43.517,"Training> Name=main_level/agent, Worker=0, Episode=1107, Total reward=125.04, Steps=73538, Training iteration=55"
2019-10-02 12:57:29.513,"Training> Name=main_level/agent, Worker=0, Episode=1106, Total reward=21.8, Steps=73339, Training iteration=55"
2019-10-02 12:57:25.512,"Training> Name=main_level/agent, Worker=0, Episode=1105, Total reward=29.29, Steps=73290, Training iteration=55"
2019-10-02 12:57:21.510,"Training> Name=main_level/agent, Worker=0, Episode=1104, Total reward=26.97, Steps=73233, Training iteration=55"
2019-10-02 12:57:16.509,"Training> Name=main_level/agent, Worker=0, Episode=1103, Total reward=46.11, Steps=73163, Training iteration=55"
2019-10-02 12:57:08.506,"Training> Name=main_level/agent, Worker=0, Episode=1102, Total reward=50.64, Steps=73057, Training iteration=55"
2019-10-02 12:57:00.504,"Training> Name=main_level/agent, Worker=0, Episode=1101, Total reward=37.93, Steps=72950, Training iteration=55"
2019-10-02 12:56:52.501,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_55.pb
2019-10-02 12:56:51.501,Uploaded 3 files for checkpoint 55
2019-10-02 12:56:51.501,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:56:51.501,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:56:49.500,Checkpoint> Saving in path=['./checkpoint/55_Step-72848.ckpt']
2019-10-02 12:56:48.500,"Policy training> Surrogate loss=-0.0908164530992508, KL divergence=0.05242706462740898, Entropy=1.3325153589248657, training epoch=9, learning_rate=3e-05"
2019-10-02 12:56:33.926,"Policy training> Surrogate loss=-0.09163413941860199, KL divergence=0.04882751777768135, Entropy=1.331272840499878, training epoch=8, learning_rate=3e-05"
2019-10-02 12:56:18.372,"Policy training> Surrogate loss=-0.08687680959701538, KL divergence=0.048517029732465744, Entropy=1.339917778968811, training epoch=7, learning_rate=3e-05"
2019-10-02 12:56:02.210,"Policy training> Surrogate loss=-0.08410888910293579, KL divergence=0.04557767137885094, Entropy=1.3386356830596924, training epoch=6, learning_rate=3e-05"
2019-10-02 12:55:46.785,"Policy training> Surrogate loss=-0.07882878929376602, KL divergence=0.04350430518388748, Entropy=1.3448565006256104, training epoch=5, learning_rate=3e-05"
2019-10-02 12:55:30.841,"Policy training> Surrogate loss=-0.0728856548666954, KL divergence=0.04391133785247803, Entropy=1.3445870876312256, training epoch=4, learning_rate=3e-05"
2019-10-02 12:55:15.648,"Policy training> Surrogate loss=-0.06313510239124298, KL divergence=0.03819584473967552, Entropy=1.3513318300247192, training epoch=3, learning_rate=3e-05"
2019-10-02 12:55:00.679,"Policy training> Surrogate loss=-0.054486896842718124, KL divergence=0.043832987546920776, Entropy=1.3661952018737793, training epoch=2, learning_rate=3e-05"
2019-10-02 12:54:45.264,"Policy training> Surrogate loss=-0.02965247444808483, KL divergence=0.057499226182699203, Entropy=1.3423701524734497, training epoch=1, learning_rate=3e-05"
2019-10-02 12:54:29.903,"Policy training> Surrogate loss=0.01896279864013195, KL divergence=0.026969818398356438, Entropy=1.3237282037734985, training epoch=0, learning_rate=3e-05"
2019-10-02 12:54:11.449,"Training> Name=main_level/agent, Worker=0, Episode=1100, Total reward=55.35, Steps=72848, Training iteration=54"
2019-10-02 12:54:03.446,"Training> Name=main_level/agent, Worker=0, Episode=1099, Total reward=134.27, Steps=72727, Training iteration=54"
2019-10-02 12:53:47.442,"Training> Name=main_level/agent, Worker=0, Episode=1098, Total reward=67.32, Steps=72504, Training iteration=54"
2019-10-02 12:53:36.439,"Training> Name=main_level/agent, Worker=0, Episode=1097, Total reward=31.52, Steps=72341, Training iteration=54"
2019-10-02 12:53:31.437,"Training> Name=main_level/agent, Worker=0, Episode=1096, Total reward=14.81, Steps=72278, Training iteration=54"
2019-10-02 12:53:28.436,"Training> Name=main_level/agent, Worker=0, Episode=1095, Total reward=43.03, Steps=72246, Training iteration=54"
2019-10-02 12:53:21.434,"Training> Name=main_level/agent, Worker=0, Episode=1094, Total reward=119.64, Steps=72147, Training iteration=54"
2019-10-02 12:53:07.430,"Training> Name=main_level/agent, Worker=0, Episode=1093, Total reward=56.55, Steps=71942, Training iteration=54"
2019-10-02 12:52:58.427,"Training> Name=main_level/agent, Worker=0, Episode=1092, Total reward=64.79, Steps=71813, Training iteration=54"
2019-10-02 12:52:48.424,"Training> Name=main_level/agent, Worker=0, Episode=1091, Total reward=64.71, Steps=71676, Training iteration=54"
2019-10-02 12:52:37.421,"Training> Name=main_level/agent, Worker=0, Episode=1090, Total reward=122.14, Steps=71516, Training iteration=54"
2019-10-02 12:52:22.416,"Training> Name=main_level/agent, Worker=0, Episode=1089, Total reward=95.53, Steps=71298, Training iteration=54"
2019-10-02 12:52:07.412,"Training> Name=main_level/agent, Worker=0, Episode=1088, Total reward=138.29, Steps=71080, Training iteration=54"
2019-10-02 12:51:51.407,"Training> Name=main_level/agent, Worker=0, Episode=1087, Total reward=19.46, Steps=70857, Training iteration=54"
2019-10-02 12:51:47.406,"Training> Name=main_level/agent, Worker=0, Episode=1086, Total reward=152.91, Steps=70800, Training iteration=54"
2019-10-02 12:51:32.402,"Training> Name=main_level/agent, Worker=0, Episode=1085, Total reward=28.92, Steps=70587, Training iteration=54"
2019-10-02 12:51:26.400,"Training> Name=main_level/agent, Worker=0, Episode=1084, Total reward=33.44, Steps=70501, Training iteration=54"
2019-10-02 12:51:20.398,"Training> Name=main_level/agent, Worker=0, Episode=1083, Total reward=152.95, Steps=70420, Training iteration=54"
2019-10-02 12:51:05.394,"Training> Name=main_level/agent, Worker=0, Episode=1082, Total reward=46.67, Steps=70206, Training iteration=54"
2019-10-02 12:50:57.392,"Training> Name=main_level/agent, Worker=0, Episode=1081, Total reward=205.0, Steps=70100, Training iteration=54"
2019-10-02 12:50:40.387,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_54.pb
2019-10-02 12:50:39.386,Uploaded 3 files for checkpoint 54
2019-10-02 12:50:39.386,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:50:39.386,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:50:26.382,"Policy training> Surrogate loss=-0.10697583854198456, KL divergence=0.04482977092266083, Entropy=1.3324410915374756, training epoch=9, learning_rate=3e-05"
2019-10-02 12:50:26.382,Checkpoint> Saving in path=['./checkpoint/54_Step-69872.ckpt']
2019-10-02 12:50:15.238,"Policy training> Surrogate loss=-0.10595562309026718, KL divergence=0.043508294969797134, Entropy=1.3178510665893555, training epoch=8, learning_rate=3e-05"
2019-10-02 12:50:04.367,"Policy training> Surrogate loss=-0.09849298000335693, KL divergence=0.04084395617246628, Entropy=1.3400108814239502, training epoch=7, learning_rate=3e-05"
2019-10-02 12:49:52.853,"Policy training> Surrogate loss=-0.09682687371969223, KL divergence=0.0388474240899086, Entropy=1.3422925472259521, training epoch=6, learning_rate=3e-05"
2019-10-02 12:49:42.557,"Policy training> Surrogate loss=-0.09298503398895264, KL divergence=0.03787007927894592, Entropy=1.3493150472640991, training epoch=5, learning_rate=3e-05"
2019-10-02 12:49:31.488,"Policy training> Surrogate loss=-0.08400581032037735, KL divergence=0.03566975146532059, Entropy=1.3594028949737549, training epoch=4, learning_rate=3e-05"
2019-10-02 12:49:19.610,"Policy training> Surrogate loss=-0.07592727243900299, KL divergence=0.0361054390668869, Entropy=1.3823236227035522, training epoch=3, learning_rate=3e-05"
2019-10-02 12:49:09.191,"Policy training> Surrogate loss=-0.06274423748254776, KL divergence=0.03850052133202553, Entropy=1.3653168678283691, training epoch=2, learning_rate=3e-05"
2019-10-02 12:48:57.651,"Policy training> Surrogate loss=-0.03458048030734062, KL divergence=0.036241527646780014, Entropy=1.4087538719177246, training epoch=1, learning_rate=3e-05"
2019-10-02 12:48:46.918,"Policy training> Surrogate loss=0.013646069914102554, KL divergence=0.026165207847952843, Entropy=1.3251603841781616, training epoch=0, learning_rate=3e-05"
2019-10-02 12:48:34.942,"Training> Name=main_level/agent, Worker=0, Episode=1080, Total reward=54.38, Steps=69872, Training iteration=53"
2019-10-02 12:48:26.928,"Training> Name=main_level/agent, Worker=0, Episode=1079, Total reward=68.23, Steps=69767, Training iteration=53"
2019-10-02 12:48:16.925,"Training> Name=main_level/agent, Worker=0, Episode=1078, Total reward=79.07, Steps=69630, Training iteration=53"
2019-10-02 12:48:03.921,"Training> Name=main_level/agent, Worker=0, Episode=1077, Total reward=28.07, Steps=69439, Training iteration=53"
2019-10-02 12:48:00.920,"Training> Name=main_level/agent, Worker=0, Episode=1076, Total reward=128.49, Steps=69391, Training iteration=53"
2019-10-02 12:47:46.916,"Training> Name=main_level/agent, Worker=0, Episode=1075, Total reward=89.05, Steps=69196, Training iteration=53"
2019-10-02 12:47:32.912,"Training> Name=main_level/agent, Worker=0, Episode=1074, Total reward=55.05, Steps=68999, Training iteration=53"
2019-10-02 12:47:24.910,"Training> Name=main_level/agent, Worker=0, Episode=1073, Total reward=15.95, Steps=68888, Training iteration=53"
2019-10-02 12:47:21.909,"Training> Name=main_level/agent, Worker=0, Episode=1072, Total reward=12.29, Steps=68841, Training iteration=53"
2019-10-02 12:47:18.908,"Training> Name=main_level/agent, Worker=0, Episode=1071, Total reward=69.61, Steps=68807, Training iteration=53"
2019-10-02 12:47:08.905,"Training> Name=main_level/agent, Worker=0, Episode=1070, Total reward=30.34, Steps=68661, Training iteration=53"
2019-10-02 12:47:01.902,"Training> Name=main_level/agent, Worker=0, Episode=1069, Total reward=40.93, Steps=68572, Training iteration=53"
2019-10-02 12:46:54.900,"Training> Name=main_level/agent, Worker=0, Episode=1068, Total reward=21.91, Steps=68469, Training iteration=53"
2019-10-02 12:46:50.899,"Training> Name=main_level/agent, Worker=0, Episode=1067, Total reward=159.87, Steps=68421, Training iteration=53"
2019-10-02 12:46:36.895,"Training> Name=main_level/agent, Worker=0, Episode=1066, Total reward=38.6, Steps=68211, Training iteration=53"
2019-10-02 12:46:28.893,"Training> Name=main_level/agent, Worker=0, Episode=1065, Total reward=20.35, Steps=68120, Training iteration=53"
2019-10-02 12:46:25.892,"Training> Name=main_level/agent, Worker=0, Episode=1064, Total reward=37.86, Steps=68072, Training iteration=53"
2019-10-02 12:46:18.889,"Training> Name=main_level/agent, Worker=0, Episode=1063, Total reward=48.31, Steps=67987, Training iteration=53"
2019-10-02 12:46:12.888,"Training> Name=main_level/agent, Worker=0, Episode=1062, Total reward=43.05, Steps=67896, Training iteration=53"
2019-10-02 12:46:08.886,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_53.pb
2019-10-02 12:46:08.886,"Training> Name=main_level/agent, Worker=0, Episode=1061, Total reward=34.76, Steps=67803, Training iteration=53"
2019-10-02 12:45:57.883,Uploaded 3 files for checkpoint 53
2019-10-02 12:45:57.883,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:45:57.883,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:45:55.883,"Policy training> Surrogate loss=-0.09948806464672089, KL divergence=0.04689938575029373, Entropy=1.338792085647583, training epoch=9, learning_rate=3e-05"
2019-10-02 12:45:55.883,Checkpoint> Saving in path=['./checkpoint/53_Step-67721.ckpt']
2019-10-02 12:45:42.752,"Policy training> Surrogate loss=-0.0965798944234848, KL divergence=0.04544960334897041, Entropy=1.331261396408081, training epoch=8, learning_rate=3e-05"
2019-10-02 12:45:30.274,"Policy training> Surrogate loss=-0.09398190677165985, KL divergence=0.043163612484931946, Entropy=1.3495161533355713, training epoch=7, learning_rate=3e-05"
2019-10-02 12:45:16.723,"Policy training> Surrogate loss=-0.09274492412805557, KL divergence=0.04322075843811035, Entropy=1.3434319496154785, training epoch=6, learning_rate=3e-05"
2019-10-02 12:45:03.735,"Policy training> Surrogate loss=-0.08816900104284286, KL divergence=0.04007616639137268, Entropy=1.354689598083496, training epoch=5, learning_rate=3e-05"
2019-10-02 12:44:50.095,"Policy training> Surrogate loss=-0.0811651423573494, KL divergence=0.04033031314611435, Entropy=1.3638603687286377, training epoch=4, learning_rate=3e-05"
2019-10-02 12:44:37.594,"Policy training> Surrogate loss=-0.07686492800712585, KL divergence=0.03816695511341095, Entropy=1.3487640619277954, training epoch=3, learning_rate=3e-05"
2019-10-02 12:44:23.742,"Policy training> Surrogate loss=-0.0569702573120594, KL divergence=0.03888731077313423, Entropy=1.3708515167236328, training epoch=2, learning_rate=3e-05"
2019-10-02 12:44:09.943,"Policy training> Surrogate loss=-0.03282545134425163, KL divergence=0.04556720331311226, Entropy=1.3500210046768188, training epoch=1, learning_rate=3e-05"
2019-10-02 12:43:57.857,"Policy training> Surrogate loss=0.01590786501765251, KL divergence=0.03015420399606228, Entropy=1.3197821378707886, training epoch=0, learning_rate=3e-05"
2019-10-02 12:43:41.812,"Training> Name=main_level/agent, Worker=0, Episode=1060, Total reward=43.04, Steps=67721, Training iteration=52"
2019-10-02 12:43:35.810,"Training> Name=main_level/agent, Worker=0, Episode=1059, Total reward=15.33, Steps=67640, Training iteration=52"
2019-10-02 12:43:32.809,"Training> Name=main_level/agent, Worker=0, Episode=1058, Total reward=65.16, Steps=67601, Training iteration=52"
2019-10-02 12:43:22.806,"Training> Name=main_level/agent, Worker=0, Episode=1057, Total reward=64.05, Steps=67460, Training iteration=52"
2019-10-02 12:43:12.804,"Training> Name=main_level/agent, Worker=0, Episode=1056, Total reward=23.64, Steps=67314, Training iteration=52"
2019-10-02 12:43:08.802,"Training> Name=main_level/agent, Worker=0, Episode=1055, Total reward=140.1, Steps=67261, Training iteration=52"
2019-10-02 12:42:53.798,"Training> Name=main_level/agent, Worker=0, Episode=1054, Total reward=141.0, Steps=67053, Training iteration=52"
2019-10-02 12:42:38.794,"Training> Name=main_level/agent, Worker=0, Episode=1053, Total reward=12.7, Steps=66831, Training iteration=52"
2019-10-02 12:42:35.793,"Training> Name=main_level/agent, Worker=0, Episode=1052, Total reward=152.79, Steps=66788, Training iteration=52"
2019-10-02 12:42:20.789,"Training> Name=main_level/agent, Worker=0, Episode=1051, Total reward=85.14, Steps=66585, Training iteration=52"
2019-10-02 12:42:07.785,"Training> Name=main_level/agent, Worker=0, Episode=1050, Total reward=18.73, Steps=66393, Training iteration=52"
2019-10-02 12:42:03.784,"Training> Name=main_level/agent, Worker=0, Episode=1049, Total reward=69.76, Steps=66338, Training iteration=52"
2019-10-02 12:41:52.780,"Training> Name=main_level/agent, Worker=0, Episode=1048, Total reward=165.34, Steps=66183, Training iteration=52"
2019-10-02 12:41:36.776,"Training> Name=main_level/agent, Worker=0, Episode=1047, Total reward=20.97, Steps=65954, Training iteration=52"
2019-10-02 12:41:32.774,"Training> Name=main_level/agent, Worker=0, Episode=1046, Total reward=16.97, Steps=65908, Training iteration=52"
2019-10-02 12:41:29.773,"Training> Name=main_level/agent, Worker=0, Episode=1045, Total reward=19.98, Steps=65856, Training iteration=52"
2019-10-02 12:41:25.772,"Training> Name=main_level/agent, Worker=0, Episode=1044, Total reward=122.92, Steps=65804, Training iteration=52"
2019-10-02 12:41:09.767,"Training> Name=main_level/agent, Worker=0, Episode=1043, Total reward=109.77, Steps=65581, Training iteration=52"
2019-10-02 12:40:55.763,"Training> Name=main_level/agent, Worker=0, Episode=1042, Total reward=56.87, Steps=65382, Training iteration=52"
2019-10-02 12:40:45.760,"Training> Name=main_level/agent, Worker=0, Episode=1041, Total reward=46.54, Steps=65243, Training iteration=52"
2019-10-02 12:40:36.758,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_52.pb
2019-10-02 12:40:35.757,Uploaded 3 files for checkpoint 52
2019-10-02 12:40:35.757,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:40:35.757,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:40:33.757,Checkpoint> Saving in path=['./checkpoint/52_Step-65140.ckpt']
2019-10-02 12:40:32.756,"Policy training> Surrogate loss=-0.12755528092384338, KL divergence=0.04972252622246742, Entropy=1.39879310131073, training epoch=9, learning_rate=3e-05"
2019-10-02 12:40:23.493,"Policy training> Surrogate loss=-0.1176794245839119, KL divergence=0.04777362197637558, Entropy=1.4087611436843872, training epoch=8, learning_rate=3e-05"
2019-10-02 12:40:13.928,"Policy training> Surrogate loss=-0.11419177800416946, KL divergence=0.04474499076604843, Entropy=1.4147893190383911, training epoch=7, learning_rate=3e-05"
2019-10-02 12:40:04.727,"Policy training> Surrogate loss=-0.1119294986128807, KL divergence=0.042576637119054794, Entropy=1.4101605415344238, training epoch=6, learning_rate=3e-05"
2019-10-02 12:39:54.444,"Policy training> Surrogate loss=-0.10610414296388626, KL divergence=0.04085589572787285, Entropy=1.4309980869293213, training epoch=5, learning_rate=3e-05"
2019-10-02 12:39:46.189,"Policy training> Surrogate loss=-0.09795381873846054, KL divergence=0.04017197713255882, Entropy=1.4284075498580933, training epoch=4, learning_rate=3e-05"
2019-10-02 12:39:35.881,"Policy training> Surrogate loss=-0.08630727231502533, KL divergence=0.035971809178590775, Entropy=1.4590764045715332, training epoch=3, learning_rate=3e-05"
2019-10-02 12:39:26.502,"Policy training> Surrogate loss=-0.07100492715835571, KL divergence=0.03862718865275383, Entropy=1.4614841938018799, training epoch=2, learning_rate=3e-05"
2019-10-02 12:39:17.293,"Policy training> Surrogate loss=-0.04538919776678085, KL divergence=0.03740981966257095, Entropy=1.4473059177398682, training epoch=1, learning_rate=3e-05"
2019-10-02 12:39:07.702,"Policy training> Surrogate loss=0.015086319297552109, KL divergence=0.018749700859189034, Entropy=1.4818768501281738, training epoch=0, learning_rate=3e-05"
2019-10-02 12:38:55.846,"Training> Name=main_level/agent, Worker=0, Episode=1040, Total reward=51.59, Steps=65140, Training iteration=51"
2019-10-02 12:38:49.844,"Training> Name=main_level/agent, Worker=0, Episode=1039, Total reward=46.11, Steps=65049, Training iteration=51"
2019-10-02 12:38:41.842,"Training> Name=main_level/agent, Worker=0, Episode=1038, Total reward=58.68, Steps=64943, Training iteration=51"
2019-10-02 12:38:32.839,"Training> Name=main_level/agent, Worker=0, Episode=1037, Total reward=94.46, Steps=64818, Training iteration=51"
2019-10-02 12:38:18.835,"Training> Name=main_level/agent, Worker=0, Episode=1036, Total reward=86.26, Steps=64613, Training iteration=51"
2019-10-02 12:38:05.831,"Training> Name=main_level/agent, Worker=0, Episode=1035, Total reward=26.04, Steps=64426, Training iteration=51"
2019-10-02 12:38:01.830,"Training> Name=main_level/agent, Worker=0, Episode=1034, Total reward=9.46, Steps=64369, Training iteration=51"
2019-10-02 12:37:59.829,"Training> Name=main_level/agent, Worker=0, Episode=1033, Total reward=82.67, Steps=64352, Training iteration=51"
2019-10-02 12:37:45.825,"Training> Name=main_level/agent, Worker=0, Episode=1032, Total reward=29.4, Steps=64160, Training iteration=51"
2019-10-02 12:37:39.823,"Training> Name=main_level/agent, Worker=0, Episode=1031, Total reward=19.28, Steps=64080, Training iteration=51"
2019-10-02 12:37:36.822,"Training> Name=main_level/agent, Worker=0, Episode=1030, Total reward=43.72, Steps=64039, Training iteration=51"
2019-10-02 12:37:29.820,"Training> Name=main_level/agent, Worker=0, Episode=1029, Total reward=5.43, Steps=63948, Training iteration=51"
2019-10-02 12:37:27.820,"Training> Name=main_level/agent, Worker=0, Episode=1028, Total reward=17.82, Steps=63930, Training iteration=51"
2019-10-02 12:37:23.818,"Training> Name=main_level/agent, Worker=0, Episode=1027, Total reward=19.43, Steps=63874, Training iteration=51"
2019-10-02 12:37:20.817,"Training> Name=main_level/agent, Worker=0, Episode=1026, Total reward=28.06, Steps=63829, Training iteration=51"
2019-10-02 12:37:15.816,"Training> Name=main_level/agent, Worker=0, Episode=1025, Total reward=23.87, Steps=63765, Training iteration=51"
2019-10-02 12:37:09.814,"Training> Name=main_level/agent, Worker=0, Episode=1024, Total reward=7.54, Steps=63680, Training iteration=51"
2019-10-02 12:37:07.813,"Training> Name=main_level/agent, Worker=0, Episode=1023, Total reward=24.52, Steps=63664, Training iteration=51"
2019-10-02 12:37:03.812,"Training> Name=main_level/agent, Worker=0, Episode=1022, Total reward=155.78, Steps=63604, Training iteration=51"
2019-10-02 12:36:49.808,"Training> Name=main_level/agent, Worker=0, Episode=1021, Total reward=57.84, Steps=63410, Training iteration=51"
2019-10-02 12:36:36.805,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_51.pb
2019-10-02 12:36:36.804,Uploaded 3 files for checkpoint 51
2019-10-02 12:36:36.804,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:36:36.804,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:36:33.803,"Policy training> Surrogate loss=-0.10312887281179428, KL divergence=0.044204290956258774, Entropy=1.4342052936553955, training epoch=9, learning_rate=3e-05"
2019-10-02 12:36:33.803,Checkpoint> Saving in path=['./checkpoint/51_Step-63251.ckpt']
2019-10-02 12:36:21.872,"Policy training> Surrogate loss=-0.10106094181537628, KL divergence=0.04497663676738739, Entropy=1.4309419393539429, training epoch=8, learning_rate=3e-05"
2019-10-02 12:36:08.714,"Policy training> Surrogate loss=-0.09958399087190628, KL divergence=0.04231305792927742, Entropy=1.4474258422851562, training epoch=7, learning_rate=3e-05"
2019-10-02 12:35:56.644,"Policy training> Surrogate loss=-0.1045636460185051, KL divergence=0.04000500589609146, Entropy=1.4505398273468018, training epoch=6, learning_rate=3e-05"
2019-10-02 12:35:45.020,"Policy training> Surrogate loss=-0.09353704005479813, KL divergence=0.03856552764773369, Entropy=1.465396523475647, training epoch=5, learning_rate=3e-05"
2019-10-02 12:35:33.114,"Policy training> Surrogate loss=-0.08706653863191605, KL divergence=0.03834366425871849, Entropy=1.459990382194519, training epoch=4, learning_rate=3e-05"
2019-10-02 12:35:20.225,"Policy training> Surrogate loss=-0.075603187084198, KL divergence=0.03678082302212715, Entropy=1.4874958992004395, training epoch=3, learning_rate=3e-05"
2019-10-02 12:35:08.445,"Policy training> Surrogate loss=-0.0627780631184578, KL divergence=0.03353771194815636, Entropy=1.4888452291488647, training epoch=2, learning_rate=3e-05"
2019-10-02 12:34:55.394,"Policy training> Surrogate loss=-0.04470520466566086, KL divergence=0.03873146325349808, Entropy=1.479124665260315, training epoch=1, learning_rate=3e-05"
2019-10-02 12:34:43.002,"Policy training> Surrogate loss=0.010978875681757927, KL divergence=0.019884278997778893, Entropy=1.4868587255477905, training epoch=0, learning_rate=3e-05"
2019-10-02 12:34:29.213,"Training> Name=main_level/agent, Worker=0, Episode=1020, Total reward=166.18, Steps=63251, Training iteration=50"
2019-10-02 12:34:13.180,"Training> Name=main_level/agent, Worker=0, Episode=1019, Total reward=72.56, Steps=63028, Training iteration=50"
2019-10-02 12:34:02.177,"Training> Name=main_level/agent, Worker=0, Episode=1018, Total reward=25.56, Steps=62868, Training iteration=50"
2019-10-02 12:33:58.176,"Training> Name=main_level/agent, Worker=0, Episode=1017, Total reward=124.87, Steps=62810, Training iteration=50"
2019-10-02 12:33:44.171,"Training> Name=main_level/agent, Worker=0, Episode=1016, Total reward=59.5, Steps=62619, Training iteration=50"
2019-10-02 12:33:36.169,"Training> Name=main_level/agent, Worker=0, Episode=1015, Total reward=81.84, Steps=62500, Training iteration=50"
2019-10-02 12:33:23.165,"Training> Name=main_level/agent, Worker=0, Episode=1014, Total reward=51.1, Steps=62317, Training iteration=50"
2019-10-02 12:33:15.163,"Training> Name=main_level/agent, Worker=0, Episode=1013, Total reward=148.5, Steps=62209, Training iteration=50"
2019-10-02 12:33:00.159,"Training> Name=main_level/agent, Worker=0, Episode=1012, Total reward=29.04, Steps=61996, Training iteration=50"
2019-10-02 12:32:56.158,"Training> Name=main_level/agent, Worker=0, Episode=1011, Total reward=51.01, Steps=61931, Training iteration=50"
2019-10-02 12:32:48.155,"Training> Name=main_level/agent, Worker=0, Episode=1010, Total reward=9.21, Steps=61819, Training iteration=50"
2019-10-02 12:32:46.155,"Training> Name=main_level/agent, Worker=0, Episode=1009, Total reward=59.28, Steps=61802, Training iteration=50"
2019-10-02 12:32:36.152,"Training> Name=main_level/agent, Worker=0, Episode=1008, Total reward=80.53, Steps=61665, Training iteration=50"
2019-10-02 12:32:22.148,"Training> Name=main_level/agent, Worker=0, Episode=1007, Total reward=31.8, Steps=61461, Training iteration=50"
2019-10-02 12:32:15.146,"Training> Name=main_level/agent, Worker=0, Episode=1006, Total reward=37.62, Steps=61362, Training iteration=50"
2019-10-02 12:32:09.144,"Training> Name=main_level/agent, Worker=0, Episode=1005, Total reward=28.48, Steps=61277, Training iteration=50"
2019-10-02 12:32:03.142,"Training> Name=main_level/agent, Worker=0, Episode=1004, Total reward=24.36, Steps=61221, Training iteration=50"
2019-10-02 12:31:59.141,"Training> Name=main_level/agent, Worker=0, Episode=1003, Total reward=12.58, Steps=61162, Training iteration=50"
2019-10-02 12:31:57.141,"Training> Name=main_level/agent, Worker=0, Episode=1002, Total reward=50.91, Steps=61142, Training iteration=50"
2019-10-02 12:31:48.138,"Training> Name=main_level/agent, Worker=0, Episode=1001, Total reward=73.55, Steps=61011, Training iteration=50"
2019-10-02 12:31:34.134,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:31:34.134,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:31:34.134,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_50.pb
2019-10-02 12:31:33.133,Uploaded 3 files for checkpoint 50
2019-10-02 12:31:32.133,Checkpoint> Saving in path=['./checkpoint/50_Step-60840.ckpt']
2019-10-02 12:31:31.133,"Policy training> Surrogate loss=-0.11262601613998413, KL divergence=0.04840589314699173, Entropy=1.4687076807022095, training epoch=9, learning_rate=3e-05"
2019-10-02 12:31:19.317,"Policy training> Surrogate loss=-0.10950803756713867, KL divergence=0.044362328946590424, Entropy=1.4778474569320679, training epoch=8, learning_rate=3e-05"
2019-10-02 12:31:07.916,"Policy training> Surrogate loss=-0.10257843881845474, KL divergence=0.04318464547395706, Entropy=1.4784557819366455, training epoch=7, learning_rate=3e-05"
2019-10-02 12:30:55.396,"Policy training> Surrogate loss=-0.10571125149726868, KL divergence=0.04348260536789894, Entropy=1.4990742206573486, training epoch=6, learning_rate=3e-05"
2019-10-02 12:30:44.097,"Policy training> Surrogate loss=-0.08848374336957932, KL divergence=0.040713727474212646, Entropy=1.4944816827774048, training epoch=5, learning_rate=3e-05"
2019-10-02 12:30:33.364,"Policy training> Surrogate loss=-0.08271293342113495, KL divergence=0.039178989827632904, Entropy=1.5078201293945312, training epoch=4, learning_rate=3e-05"
2019-10-02 12:30:21.740,"Policy training> Surrogate loss=-0.07774871587753296, KL divergence=0.04175616800785065, Entropy=1.5196387767791748, training epoch=3, learning_rate=3e-05"
2019-10-02 12:30:10.595,"Policy training> Surrogate loss=-0.0614517480134964, KL divergence=0.04705013707280159, Entropy=1.5084466934204102, training epoch=2, learning_rate=3e-05"
2019-10-02 12:29:58.502,"Policy training> Surrogate loss=-0.03516433760523796, KL divergence=0.047458674758672714, Entropy=1.5116339921951294, training epoch=1, learning_rate=3e-05"
2019-10-02 12:29:46.522,"Policy training> Surrogate loss=0.011230897158384323, KL divergence=0.021528055891394615, Entropy=1.489385962486267, training epoch=0, learning_rate=3e-05"
2019-10-02 12:29:33.112,"Training> Name=main_level/agent, Worker=0, Episode=1000, Total reward=14.19, Steps=60840, Training iteration=49"
2019-10-02 12:29:31.111,"Training> Name=main_level/agent, Worker=0, Episode=999, Total reward=116.92, Steps=60812, Training iteration=49"
2019-10-02 12:29:16.107,"Training> Name=main_level/agent, Worker=0, Episode=998, Total reward=23.4, Steps=60603, Training iteration=49"
2019-10-02 12:29:12.106,"Training> Name=main_level/agent, Worker=0, Episode=997, Total reward=10.0, Steps=60559, Training iteration=49"
2019-10-02 12:29:11.106,"Training> Name=main_level/agent, Worker=0, Episode=996, Total reward=60.63, Steps=60538, Training iteration=49"
2019-10-02 12:29:02.103,"Training> Name=main_level/agent, Worker=0, Episode=995, Total reward=102.29, Steps=60421, Training iteration=49"
2019-10-02 12:28:47.099,"Training> Name=main_level/agent, Worker=0, Episode=994, Total reward=32.07, Steps=60202, Training iteration=49"
2019-10-02 12:28:42.097,"Training> Name=main_level/agent, Worker=0, Episode=993, Total reward=30.94, Steps=60137, Training iteration=49"
2019-10-02 12:28:36.095,"Training> Name=main_level/agent, Worker=0, Episode=992, Total reward=56.12, Steps=60060, Training iteration=49"
2019-10-02 12:28:28.093,"Training> Name=main_level/agent, Worker=0, Episode=991, Total reward=66.55, Steps=59941, Training iteration=49"
2019-10-02 12:28:18.090,"Training> Name=main_level/agent, Worker=0, Episode=990, Total reward=62.6, Steps=59793, Training iteration=49"
2019-10-02 12:28:09.087,"Training> Name=main_level/agent, Worker=0, Episode=989, Total reward=28.95, Steps=59677, Training iteration=49"
2019-10-02 12:28:04.086,"Training> Name=main_level/agent, Worker=0, Episode=988, Total reward=38.68, Steps=59601, Training iteration=49"
2019-10-02 12:27:56.084,"Training> Name=main_level/agent, Worker=0, Episode=987, Total reward=35.59, Steps=59497, Training iteration=49"
2019-10-02 12:27:50.082,"Training> Name=main_level/agent, Worker=0, Episode=986, Total reward=21.9, Steps=59408, Training iteration=49"
2019-10-02 12:27:46.080,"Training> Name=main_level/agent, Worker=0, Episode=985, Total reward=99.27, Steps=59355, Training iteration=49"
2019-10-02 12:27:32.077,"Training> Name=main_level/agent, Worker=0, Episode=984, Total reward=130.15, Steps=59155, Training iteration=49"
2019-10-02 12:27:16.072,"Training> Name=main_level/agent, Worker=0, Episode=983, Total reward=27.35, Steps=58936, Training iteration=49"
2019-10-02 12:27:13.071,"Training> Name=main_level/agent, Worker=0, Episode=982, Total reward=59.9, Steps=58892, Training iteration=49"
2019-10-02 12:27:04.068,"Training> Name=main_level/agent, Worker=0, Episode=981, Total reward=122.04, Steps=58764, Training iteration=49"
2019-10-02 12:26:49.064,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_49.pb
2019-10-02 12:26:48.063,Uploaded 3 files for checkpoint 49
2019-10-02 12:26:48.063,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:26:48.063,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:26:45.062,"Policy training> Surrogate loss=-0.09873562306165695, KL divergence=0.04338663071393967, Entropy=1.466084361076355, training epoch=9, learning_rate=3e-05"
2019-10-02 12:26:45.062,Checkpoint> Saving in path=['./checkpoint/49_Step-58561.ckpt']
2019-10-02 12:26:33.312,"Policy training> Surrogate loss=-0.10401660203933716, KL divergence=0.04120459780097008, Entropy=1.4684118032455444, training epoch=8, learning_rate=3e-05"
2019-10-02 12:26:22.585,"Policy training> Surrogate loss=-0.09915978461503983, KL divergence=0.040227293968200684, Entropy=1.475193977355957, training epoch=7, learning_rate=3e-05"
2019-10-02 12:26:11.496,"Policy training> Surrogate loss=-0.08903127163648605, KL divergence=0.039443135261535645, Entropy=1.485559105873108, training epoch=6, learning_rate=3e-05"
2019-10-02 12:26:01.220,"Policy training> Surrogate loss=-0.08520746976137161, KL divergence=0.039192475378513336, Entropy=1.4837027788162231, training epoch=5, learning_rate=3e-05"
2019-10-02 12:25:49.934,"Policy training> Surrogate loss=-0.081180140376091, KL divergence=0.036836471408605576, Entropy=1.4905157089233398, training epoch=4, learning_rate=3e-05"
2019-10-02 12:25:38.302,"Policy training> Surrogate loss=-0.07542337477207184, KL divergence=0.03681590035557747, Entropy=1.4969562292099, training epoch=3, learning_rate=3e-05"
2019-10-02 12:25:27.303,"Policy training> Surrogate loss=-0.05643173307180405, KL divergence=0.036050330847501755, Entropy=1.5148853063583374, training epoch=2, learning_rate=3e-05"
2019-10-02 12:25:16.392,"Policy training> Surrogate loss=-0.038020092993974686, KL divergence=0.044670894742012024, Entropy=1.4967708587646484, training epoch=1, learning_rate=3e-05"
2019-10-02 12:25:05.711,"Policy training> Surrogate loss=0.012225189246237278, KL divergence=0.023997310549020767, Entropy=1.4687522649765015, training epoch=0, learning_rate=3e-05"
2019-10-02 12:24:52.345,"Training> Name=main_level/agent, Worker=0, Episode=980, Total reward=73.19, Steps=58561, Training iteration=48"
2019-10-02 12:24:42.342,"Training> Name=main_level/agent, Worker=0, Episode=979, Total reward=17.56, Steps=58407, Training iteration=48"
2019-10-02 12:24:38.341,"Training> Name=main_level/agent, Worker=0, Episode=978, Total reward=145.92, Steps=58364, Training iteration=48"
2019-10-02 12:24:24.337,"Training> Name=main_level/agent, Worker=0, Episode=977, Total reward=37.75, Steps=58161, Training iteration=48"
2019-10-02 12:24:19.335,"Training> Name=main_level/agent, Worker=0, Episode=976, Total reward=30.64, Steps=58098, Training iteration=48"
2019-10-02 12:24:15.334,"Training> Name=main_level/agent, Worker=0, Episode=975, Total reward=73.85, Steps=58032, Training iteration=48"
2019-10-02 12:24:03.331,"Training> Name=main_level/agent, Worker=0, Episode=974, Total reward=33.44, Steps=57864, Training iteration=48"
2019-10-02 12:23:58.329,"Training> Name=main_level/agent, Worker=0, Episode=973, Total reward=93.12, Steps=57797, Training iteration=48"
2019-10-02 12:23:44.325,"Training> Name=main_level/agent, Worker=0, Episode=972, Total reward=44.57, Steps=57594, Training iteration=48"
2019-10-02 12:23:37.323,"Training> Name=main_level/agent, Worker=0, Episode=971, Total reward=17.62, Steps=57500, Training iteration=48"
2019-10-02 12:23:33.322,"Training> Name=main_level/agent, Worker=0, Episode=970, Total reward=12.29, Steps=57458, Training iteration=48"
2019-10-02 12:23:31.321,"Training> Name=main_level/agent, Worker=0, Episode=969, Total reward=49.8, Steps=57433, Training iteration=48"
2019-10-02 12:23:24.319,"Training> Name=main_level/agent, Worker=0, Episode=968, Total reward=70.26, Steps=57327, Training iteration=48"
2019-10-02 12:23:13.316,"Training> Name=main_level/agent, Worker=0, Episode=967, Total reward=125.21, Steps=57171, Training iteration=48"
2019-10-02 12:22:59.312,"Training> Name=main_level/agent, Worker=0, Episode=966, Total reward=40.4, Steps=56969, Training iteration=48"
2019-10-02 12:22:52.310,"Training> Name=main_level/agent, Worker=0, Episode=965, Total reward=15.62, Steps=56874, Training iteration=48"
2019-10-02 12:22:48.309,"Training> Name=main_level/agent, Worker=0, Episode=964, Total reward=31.83, Steps=56823, Training iteration=48"
2019-10-02 12:22:42.307,"Training> Name=main_level/agent, Worker=0, Episode=963, Total reward=36.55, Steps=56751, Training iteration=48"
2019-10-02 12:22:36.305,"Training> Name=main_level/agent, Worker=0, Episode=962, Total reward=132.67, Steps=56670, Training iteration=48"
2019-10-02 12:22:22.301,"Training> Name=main_level/agent, Worker=0, Episode=961, Total reward=57.53, Steps=56461, Training iteration=48"
2019-10-02 12:22:12.298,Uploaded 3 files for checkpoint 48
2019-10-02 12:22:12.298,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:22:12.298,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:22:12.298,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_48.pb
2019-10-02 12:22:10.297,Checkpoint> Saving in path=['./checkpoint/48_Step-56349.ckpt']
2019-10-02 12:22:09.297,"Policy training> Surrogate loss=-0.11011604964733124, KL divergence=0.04292182996869087, Entropy=1.525985598564148, training epoch=9, learning_rate=3e-05"
2019-10-02 12:22:00.108,"Policy training> Surrogate loss=-0.11129508912563324, KL divergence=0.040356364101171494, Entropy=1.523560643196106, training epoch=8, learning_rate=3e-05"
2019-10-02 12:21:50.386,"Policy training> Surrogate loss=-0.1043296605348587, KL divergence=0.04139404371380806, Entropy=1.5201197862625122, training epoch=7, learning_rate=3e-05"
2019-10-02 12:21:40.066,"Policy training> Surrogate loss=-0.10240616649389267, KL divergence=0.03949352726340294, Entropy=1.5400751829147339, training epoch=6, learning_rate=3e-05"
2019-10-02 12:21:29.798,"Policy training> Surrogate loss=-0.0957263931632042, KL divergence=0.037643421441316605, Entropy=1.5356758832931519, training epoch=5, learning_rate=3e-05"
2019-10-02 12:21:19.457,"Policy training> Surrogate loss=-0.08591136336326599, KL divergence=0.03661571070551872, Entropy=1.568104863166809, training epoch=4, learning_rate=3e-05"
2019-10-02 12:21:09.670,"Policy training> Surrogate loss=-0.07721449434757233, KL divergence=0.03504136949777603, Entropy=1.5461629629135132, training epoch=3, learning_rate=3e-05"
2019-10-02 12:20:59.350,"Policy training> Surrogate loss=-0.06560591608285904, KL divergence=0.0344679020345211, Entropy=1.5584737062454224, training epoch=2, learning_rate=3e-05"
2019-10-02 12:20:50.128,"Policy training> Surrogate loss=-0.040069278329610825, KL divergence=0.03842335566878319, Entropy=1.5677517652511597, training epoch=1, learning_rate=3e-05"
2019-10-02 12:20:39.806,"Policy training> Surrogate loss=0.010334047488868237, KL divergence=0.018342116847634315, Entropy=1.5229243040084839, training epoch=0, learning_rate=3e-05"
2019-10-02 12:20:28.410,"Training> Name=main_level/agent, Worker=0, Episode=960, Total reward=55.19, Steps=56349, Training iteration=47"
2019-10-02 12:20:20.374,"Training> Name=main_level/agent, Worker=0, Episode=959, Total reward=20.28, Steps=56235, Training iteration=47"
2019-10-02 12:20:16.373,"Training> Name=main_level/agent, Worker=0, Episode=958, Total reward=29.75, Steps=56188, Training iteration=47"
2019-10-02 12:20:12.372,"Training> Name=main_level/agent, Worker=0, Episode=957, Total reward=57.76, Steps=56132, Training iteration=47"
2019-10-02 12:20:04.369,"Training> Name=main_level/agent, Worker=0, Episode=956, Total reward=10.36, Steps=56026, Training iteration=47"
2019-10-02 12:20:02.369,"Training> Name=main_level/agent, Worker=0, Episode=955, Total reward=52.56, Steps=56000, Training iteration=47"
2019-10-02 12:19:54.366,"Training> Name=main_level/agent, Worker=0, Episode=954, Total reward=23.35, Steps=55886, Training iteration=47"
2019-10-02 12:19:50.365,"Training> Name=main_level/agent, Worker=0, Episode=953, Total reward=18.71, Steps=55841, Training iteration=47"
2019-10-02 12:19:46.364,"Training> Name=main_level/agent, Worker=0, Episode=952, Total reward=15.34, Steps=55781, Training iteration=47"
2019-10-02 12:19:43.363,"Training> Name=main_level/agent, Worker=0, Episode=951, Total reward=72.22, Steps=55740, Training iteration=47"
2019-10-02 12:19:32.360,"Training> Name=main_level/agent, Worker=0, Episode=950, Total reward=21.18, Steps=55585, Training iteration=47"
2019-10-02 12:19:28.358,"Training> Name=main_level/agent, Worker=0, Episode=949, Total reward=91.91, Steps=55531, Training iteration=47"
2019-10-02 12:19:13.354,"Training> Name=main_level/agent, Worker=0, Episode=948, Total reward=22.74, Steps=55326, Training iteration=47"
2019-10-02 12:19:08.353,"Training> Name=main_level/agent, Worker=0, Episode=947, Total reward=20.52, Steps=55258, Training iteration=47"
2019-10-02 12:19:03.351,"Training> Name=main_level/agent, Worker=0, Episode=946, Total reward=41.5, Steps=55200, Training iteration=47"
2019-10-02 12:18:55.349,"Training> Name=main_level/agent, Worker=0, Episode=945, Total reward=21.19, Steps=55086, Training iteration=47"
2019-10-02 12:18:50.347,"Training> Name=main_level/agent, Worker=0, Episode=944, Total reward=36.97, Steps=55022, Training iteration=47"
2019-10-02 12:18:44.345,"Training> Name=main_level/agent, Worker=0, Episode=943, Total reward=87.87, Steps=54931, Training iteration=47"
2019-10-02 12:18:31.342,"Training> Name=main_level/agent, Worker=0, Episode=942, Total reward=42.63, Steps=54742, Training iteration=47"
2019-10-02 12:18:24.339,"Training> Name=main_level/agent, Worker=0, Episode=941, Total reward=142.14, Steps=54651, Training iteration=47"
2019-10-02 12:18:07.335,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:18:07.335,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_47.pb
2019-10-02 12:18:07.334,Uploaded 3 files for checkpoint 47
2019-10-02 12:18:07.334,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:18:05.334,Checkpoint> Saving in path=['./checkpoint/47_Step-54425.ckpt']
2019-10-02 12:18:04.333,"Policy training> Surrogate loss=-0.11679066717624664, KL divergence=0.045918889343738556, Entropy=1.5204458236694336, training epoch=9, learning_rate=3e-05"
2019-10-02 12:17:56.002,"Policy training> Surrogate loss=-0.11400473862886429, KL divergence=0.04417504370212555, Entropy=1.517659306526184, training epoch=8, learning_rate=3e-05"
2019-10-02 12:17:47.780,"Policy training> Surrogate loss=-0.11606060713529587, KL divergence=0.04392388463020325, Entropy=1.5266025066375732, training epoch=7, learning_rate=3e-05"
2019-10-02 12:17:40.477,"Policy training> Surrogate loss=-0.10854178667068481, KL divergence=0.04423046484589577, Entropy=1.5235651731491089, training epoch=6, learning_rate=3e-05"
2019-10-02 12:17:32.208,"Policy training> Surrogate loss=-0.1023738831281662, KL divergence=0.0422128289937973, Entropy=1.5438728332519531, training epoch=5, learning_rate=3e-05"
2019-10-02 12:17:23.490,"Policy training> Surrogate loss=-0.0964200496673584, KL divergence=0.03731903061270714, Entropy=1.5447033643722534, training epoch=4, learning_rate=3e-05"
2019-10-02 12:17:15.317,"Policy training> Surrogate loss=-0.09014690667390823, KL divergence=0.03930215537548065, Entropy=1.535886526107788, training epoch=3, learning_rate=3e-05"
2019-10-02 12:17:07.851,"Policy training> Surrogate loss=-0.0701623484492302, KL divergence=0.03277752548456192, Entropy=1.5616364479064941, training epoch=2, learning_rate=3e-05"
2019-10-02 12:16:59.770,"Policy training> Surrogate loss=-0.04425891488790512, KL divergence=0.03832324221730232, Entropy=1.5475289821624756, training epoch=1, learning_rate=3e-05"
2019-10-02 12:16:51.413,"Policy training> Surrogate loss=0.015129480510950089, KL divergence=0.020008690655231476, Entropy=1.5245811939239502, training epoch=0, learning_rate=3e-05"
2019-10-02 12:16:41.217,"Training> Name=main_level/agent, Worker=0, Episode=940, Total reward=34.3, Steps=54425, Training iteration=46"
2019-10-02 12:16:36.210,"Training> Name=main_level/agent, Worker=0, Episode=939, Total reward=19.62, Steps=54360, Training iteration=46"
2019-10-02 12:16:33.209,"Training> Name=main_level/agent, Worker=0, Episode=938, Total reward=26.28, Steps=54319, Training iteration=46"
2019-10-02 12:16:29.208,"Training> Name=main_level/agent, Worker=0, Episode=937, Total reward=25.52, Steps=54270, Training iteration=46"
2019-10-02 12:16:24.206,"Training> Name=main_level/agent, Worker=0, Episode=936, Total reward=26.29, Steps=54223, Training iteration=46"
2019-10-02 12:16:20.205,"Training> Name=main_level/agent, Worker=0, Episode=935, Total reward=39.14, Steps=54162, Training iteration=46"
2019-10-02 12:16:14.203,"Training> Name=main_level/agent, Worker=0, Episode=934, Total reward=84.99, Steps=54085, Training iteration=46"
2019-10-02 12:16:02.199,"Training> Name=main_level/agent, Worker=0, Episode=933, Total reward=79.96, Steps=53902, Training iteration=46"
2019-10-02 12:15:48.195,"Training> Name=main_level/agent, Worker=0, Episode=932, Total reward=9.06, Steps=53706, Training iteration=46"
2019-10-02 12:15:45.194,"Training> Name=main_level/agent, Worker=0, Episode=931, Total reward=36.94, Steps=53673, Training iteration=46"
2019-10-02 12:15:39.193,"Training> Name=main_level/agent, Worker=0, Episode=930, Total reward=10.16, Steps=53590, Training iteration=46"
2019-10-02 12:15:37.192,"Training> Name=main_level/agent, Worker=0, Episode=929, Total reward=117.33, Steps=53569, Training iteration=46"
2019-10-02 12:15:23.188,"Training> Name=main_level/agent, Worker=0, Episode=928, Total reward=13.51, Steps=53357, Training iteration=46"
2019-10-02 12:15:19.187,"Training> Name=main_level/agent, Worker=0, Episode=927, Total reward=14.26, Steps=53312, Training iteration=46"
2019-10-02 12:15:16.186,"Training> Name=main_level/agent, Worker=0, Episode=926, Total reward=17.79, Steps=53273, Training iteration=46"
2019-10-02 12:15:12.185,"Training> Name=main_level/agent, Worker=0, Episode=925, Total reward=11.9, Steps=53221, Training iteration=46"
2019-10-02 12:15:09.184,"Training> Name=main_level/agent, Worker=0, Episode=924, Total reward=26.66, Steps=53186, Training iteration=46"
2019-10-02 12:15:05.183,"Training> Name=main_level/agent, Worker=0, Episode=923, Total reward=93.98, Steps=53125, Training iteration=46"
2019-10-02 12:14:51.178,"Training> Name=main_level/agent, Worker=0, Episode=922, Total reward=30.75, Steps=52924, Training iteration=46"
2019-10-02 12:14:46.177,"Training> Name=main_level/agent, Worker=0, Episode=921, Total reward=32.67, Steps=52859, Training iteration=46"
2019-10-02 12:14:40.174,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_46.pb
2019-10-02 12:14:39.173,Uploaded 3 files for checkpoint 46
2019-10-02 12:14:39.173,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:14:39.173,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:14:37.173,Checkpoint> Saving in path=['./checkpoint/46_Step-52793.ckpt']
2019-10-02 12:14:37.172,"Policy training> Surrogate loss=-0.1089182049036026, KL divergence=0.04849331080913544, Entropy=1.5400469303131104, training epoch=9, learning_rate=3e-05"
2019-10-02 12:14:26.662,"Policy training> Surrogate loss=-0.10702499747276306, KL divergence=0.04804828017950058, Entropy=1.5207865238189697, training epoch=8, learning_rate=3e-05"
2019-10-02 12:14:15.518,"Policy training> Surrogate loss=-0.10738354176282883, KL divergence=0.044983215630054474, Entropy=1.5397049188613892, training epoch=7, learning_rate=3e-05"
2019-10-02 12:14:05.127,"Policy training> Surrogate loss=-0.10058173537254333, KL divergence=0.04450300335884094, Entropy=1.5561563968658447, training epoch=6, learning_rate=3e-05"
2019-10-02 12:13:54.543,"Policy training> Surrogate loss=-0.09344004094600677, KL divergence=0.041420795023441315, Entropy=1.5481138229370117, training epoch=5, learning_rate=3e-05"
2019-10-02 12:13:43.676,"Policy training> Surrogate loss=-0.08693724870681763, KL divergence=0.03980468213558197, Entropy=1.548386573791504, training epoch=4, learning_rate=3e-05"
2019-10-02 12:13:33.490,"Policy training> Surrogate loss=-0.07578694820404053, KL divergence=0.03772707283496857, Entropy=1.5742967128753662, training epoch=3, learning_rate=3e-05"
2019-10-02 12:13:23.221,"Policy training> Surrogate loss=-0.06502100825309753, KL divergence=0.034430131316185, Entropy=1.569095492362976, training epoch=2, learning_rate=3e-05"
2019-10-02 12:13:12.637,"Policy training> Surrogate loss=-0.04425514116883278, KL divergence=0.03635679930448532, Entropy=1.5455085039138794, training epoch=1, learning_rate=3e-05"
2019-10-02 12:13:02.682,"Policy training> Surrogate loss=0.012822285294532776, KL divergence=0.022183958441019058, Entropy=1.5445265769958496, training epoch=0, learning_rate=3e-05"
2019-10-02 12:12:51.242,"Training> Name=main_level/agent, Worker=0, Episode=920, Total reward=29.26, Steps=52793, Training iteration=45"
2019-10-02 12:12:47.240,"Training> Name=main_level/agent, Worker=0, Episode=919, Total reward=16.85, Steps=52739, Training iteration=45"
2019-10-02 12:12:43.239,"Training> Name=main_level/agent, Worker=0, Episode=918, Total reward=55.52, Steps=52700, Training iteration=45"
2019-10-02 12:12:34.236,"Training> Name=main_level/agent, Worker=0, Episode=917, Total reward=29.41, Steps=52567, Training iteration=45"
2019-10-02 12:12:29.235,"Training> Name=main_level/agent, Worker=0, Episode=916, Total reward=63.56, Steps=52508, Training iteration=45"
2019-10-02 12:12:19.232,"Training> Name=main_level/agent, Worker=0, Episode=915, Total reward=74.05, Steps=52367, Training iteration=45"
2019-10-02 12:12:07.228,"Training> Name=main_level/agent, Worker=0, Episode=914, Total reward=36.34, Steps=52196, Training iteration=45"
2019-10-02 12:12:02.227,"Training> Name=main_level/agent, Worker=0, Episode=913, Total reward=33.29, Steps=52120, Training iteration=45"
2019-10-02 12:11:56.225,"Training> Name=main_level/agent, Worker=0, Episode=912, Total reward=29.13, Steps=52035, Training iteration=45"
2019-10-02 12:11:51.223,"Training> Name=main_level/agent, Worker=0, Episode=911, Total reward=19.89, Steps=51966, Training iteration=45"
2019-10-02 12:11:47.222,"Training> Name=main_level/agent, Worker=0, Episode=910, Total reward=49.97, Steps=51925, Training iteration=45"
2019-10-02 12:11:38.219,"Training> Name=main_level/agent, Worker=0, Episode=909, Total reward=7.3, Steps=51791, Training iteration=45"
2019-10-02 12:11:36.219,"Training> Name=main_level/agent, Worker=0, Episode=908, Total reward=127.54, Steps=51773, Training iteration=45"
2019-10-02 12:11:21.214,"Training> Name=main_level/agent, Worker=0, Episode=907, Total reward=52.09, Steps=51547, Training iteration=45"
2019-10-02 12:11:11.211,"Training> Name=main_level/agent, Worker=0, Episode=906, Total reward=31.19, Steps=51408, Training iteration=45"
2019-10-02 12:11:05.210,"Training> Name=main_level/agent, Worker=0, Episode=905, Total reward=13.54, Steps=51333, Training iteration=45"
2019-10-02 12:11:02.209,"Training> Name=main_level/agent, Worker=0, Episode=904, Total reward=93.65, Steps=51298, Training iteration=45"
2019-10-02 12:10:48.205,"Training> Name=main_level/agent, Worker=0, Episode=903, Total reward=141.84, Steps=51105, Training iteration=45"
2019-10-02 12:10:34.201,"Training> Name=main_level/agent, Worker=0, Episode=902, Total reward=39.59, Steps=50903, Training iteration=45"
2019-10-02 12:10:28.199,"Training> Name=main_level/agent, Worker=0, Episode=901, Total reward=55.55, Steps=50820, Training iteration=45"
2019-10-02 12:10:19.196,Uploaded 3 files for checkpoint 45
2019-10-02 12:10:19.196,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:10:19.196,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:10:19.196,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_45.pb
2019-10-02 12:10:07.192,Checkpoint> Saving in path=['./checkpoint/45_Step-50708.ckpt']
2019-10-02 12:10:06.192,"Policy training> Surrogate loss=-0.11629172414541245, KL divergence=0.041432395577430725, Entropy=1.563503384590149, training epoch=9, learning_rate=3e-05"
2019-10-02 12:09:58.724,"Policy training> Surrogate loss=-0.1202651783823967, KL divergence=0.03969813510775566, Entropy=1.5800790786743164, training epoch=8, learning_rate=3e-05"
2019-10-02 12:09:50.938,"Policy training> Surrogate loss=-0.11563759297132492, KL divergence=0.03869739547371864, Entropy=1.5660139322280884, training epoch=7, learning_rate=3e-05"
2019-10-02 12:09:42.737,"Policy training> Surrogate loss=-0.1045098826289177, KL divergence=0.037753015756607056, Entropy=1.581642746925354, training epoch=6, learning_rate=3e-05"
2019-10-02 12:09:35.644,"Policy training> Surrogate loss=-0.10537698119878769, KL divergence=0.038158077746629715, Entropy=1.5756746530532837, training epoch=5, learning_rate=3e-05"
2019-10-02 12:09:28.302,"Policy training> Surrogate loss=-0.10082396864891052, KL divergence=0.03668830171227455, Entropy=1.6035410165786743, training epoch=4, learning_rate=3e-05"
2019-10-02 12:09:19.940,"Policy training> Surrogate loss=-0.0816047415137291, KL divergence=0.037417810410261154, Entropy=1.5893017053604126, training epoch=3, learning_rate=3e-05"
2019-10-02 12:09:12.285,"Policy training> Surrogate loss=-0.06490224599838257, KL divergence=0.039129164069890976, Entropy=1.5996437072753906, training epoch=2, learning_rate=3e-05"
2019-10-02 12:09:04.839,"Policy training> Surrogate loss=-0.04462480545043945, KL divergence=0.04298410937190056, Entropy=1.5761569738388062, training epoch=1, learning_rate=3e-05"
2019-10-02 12:08:57.641,"Policy training> Surrogate loss=0.008381903171539307, KL divergence=0.018826769664883614, Entropy=1.59323251247406, training epoch=0, learning_rate=3e-05"
2019-10-02 12:08:48.085,"Training> Name=main_level/agent, Worker=0, Episode=900, Total reward=42.47, Steps=50708, Training iteration=44"
2019-10-02 12:08:42.083,"Training> Name=main_level/agent, Worker=0, Episode=899, Total reward=17.31, Steps=50625, Training iteration=44"
2019-10-02 12:08:39.082,"Training> Name=main_level/agent, Worker=0, Episode=898, Total reward=19.53, Steps=50588, Training iteration=44"
2019-10-02 12:08:35.081,"Training> Name=main_level/agent, Worker=0, Episode=897, Total reward=25.88, Steps=50545, Training iteration=44"
2019-10-02 12:08:31.080,"Training> Name=main_level/agent, Worker=0, Episode=896, Total reward=43.09, Steps=50493, Training iteration=44"
2019-10-02 12:08:24.077,"Training> Name=main_level/agent, Worker=0, Episode=895, Total reward=18.87, Steps=50396, Training iteration=44"
2019-10-02 12:08:21.076,"Training> Name=main_level/agent, Worker=0, Episode=894, Total reward=56.77, Steps=50352, Training iteration=44"
2019-10-02 12:08:12.074,"Training> Name=main_level/agent, Worker=0, Episode=893, Total reward=4.66, Steps=50221, Training iteration=44"
2019-10-02 12:08:09.073,"Training> Name=main_level/agent, Worker=0, Episode=892, Total reward=76.1, Steps=50192, Training iteration=44"
2019-10-02 12:07:57.069,"Training> Name=main_level/agent, Worker=0, Episode=891, Total reward=21.76, Steps=50019, Training iteration=44"
2019-10-02 12:07:53.068,"Training> Name=main_level/agent, Worker=0, Episode=890, Total reward=20.15, Steps=49972, Training iteration=44"
2019-10-02 12:07:50.067,"Training> Name=main_level/agent, Worker=0, Episode=889, Total reward=9.12, Steps=49922, Training iteration=44"
2019-10-02 12:07:48.066,"Training> Name=main_level/agent, Worker=0, Episode=888, Total reward=19.33, Steps=49901, Training iteration=44"
2019-10-02 12:07:44.065,"Training> Name=main_level/agent, Worker=0, Episode=887, Total reward=18.67, Steps=49859, Training iteration=44"
2019-10-02 12:07:41.064,"Training> Name=main_level/agent, Worker=0, Episode=886, Total reward=62.64, Steps=49816, Training iteration=44"
2019-10-02 12:07:29.060,"Training> Name=main_level/agent, Worker=0, Episode=885, Total reward=40.19, Steps=49656, Training iteration=44"
2019-10-02 12:07:23.058,"Training> Name=main_level/agent, Worker=0, Episode=884, Total reward=63.65, Steps=49568, Training iteration=44"
2019-10-02 12:07:12.055,"Training> Name=main_level/agent, Worker=0, Episode=883, Total reward=193.59, Steps=49415, Training iteration=44"
2019-10-02 12:06:57.051,"Training> Name=main_level/agent, Worker=0, Episode=882, Total reward=18.44, Steps=49200, Training iteration=44"
2019-10-02 12:06:54.050,"Training> Name=main_level/agent, Worker=0, Episode=881, Total reward=28.59, Steps=49166, Training iteration=44"
2019-10-02 12:06:48.048,Uploaded 3 files for checkpoint 44
2019-10-02 12:06:48.048,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:06:48.048,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:06:48.048,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_44.pb
2019-10-02 12:06:46.047,Checkpoint> Saving in path=['./checkpoint/44_Step-49116.ckpt']
2019-10-02 12:06:45.047,"Policy training> Surrogate loss=-0.11317767947912216, KL divergence=0.04096347093582153, Entropy=1.5973135232925415, training epoch=9, learning_rate=3e-05"
2019-10-02 12:06:34.601,"Policy training> Surrogate loss=-0.11001985520124435, KL divergence=0.03974322974681854, Entropy=1.598104476928711, training epoch=8, learning_rate=3e-05"
2019-10-02 12:06:23.313,"Policy training> Surrogate loss=-0.10478049516677856, KL divergence=0.03865581750869751, Entropy=1.61072838306427, training epoch=7, learning_rate=3e-05"
2019-10-02 12:06:12.190,"Policy training> Surrogate loss=-0.09832137078046799, KL divergence=0.037316951900720596, Entropy=1.6111799478530884, training epoch=6, learning_rate=3e-05"
2019-10-02 12:06:01.145,"Policy training> Surrogate loss=-0.0947960764169693, KL divergence=0.03706064820289612, Entropy=1.6399967670440674, training epoch=5, learning_rate=3e-05"
2019-10-02 12:05:49.886,"Policy training> Surrogate loss=-0.08535031974315643, KL divergence=0.03872448205947876, Entropy=1.6231919527053833, training epoch=4, learning_rate=3e-05"
2019-10-02 12:05:39.421,"Policy training> Surrogate loss=-0.08001480996608734, KL divergence=0.03622996434569359, Entropy=1.6329625844955444, training epoch=3, learning_rate=3e-05"
2019-10-02 12:05:27.995,"Policy training> Surrogate loss=-0.06486184895038605, KL divergence=0.03321646898984909, Entropy=1.6355234384536743, training epoch=2, learning_rate=3e-05"
2019-10-02 12:05:18.166,"Policy training> Surrogate loss=-0.04038144648075104, KL divergence=0.03670934587717056, Entropy=1.6551839113235474, training epoch=1, learning_rate=3e-05"
2019-10-02 12:05:06.792,"Policy training> Surrogate loss=0.015243747271597385, KL divergence=0.026058100163936615, Entropy=1.6110858917236328, training epoch=0, learning_rate=3e-05"
2019-10-02 12:04:53.709,"Training> Name=main_level/agent, Worker=0, Episode=880, Total reward=28.08, Steps=49116, Training iteration=43"
2019-10-02 12:04:48.708,"Training> Name=main_level/agent, Worker=0, Episode=879, Total reward=24.28, Steps=49056, Training iteration=43"
2019-10-02 12:04:44.707,"Training> Name=main_level/agent, Worker=0, Episode=878, Total reward=36.36, Steps=49003, Training iteration=43"
2019-10-02 12:04:38.705,"Training> Name=main_level/agent, Worker=0, Episode=877, Total reward=102.0, Steps=48914, Training iteration=43"
2019-10-02 12:04:23.701,"Training> Name=main_level/agent, Worker=0, Episode=876, Total reward=152.95, Steps=48705, Training iteration=43"
2019-10-02 12:04:10.697,"Training> Name=main_level/agent, Worker=0, Episode=875, Total reward=48.23, Steps=48508, Training iteration=43"
2019-10-02 12:04:02.695,"Training> Name=main_level/agent, Worker=0, Episode=874, Total reward=27.73, Steps=48399, Training iteration=43"
2019-10-02 12:03:57.693,"Training> Name=main_level/agent, Worker=0, Episode=873, Total reward=24.88, Steps=48335, Training iteration=43"
2019-10-02 12:03:52.692,"Training> Name=main_level/agent, Worker=0, Episode=872, Total reward=75.43, Steps=48267, Training iteration=43"
2019-10-02 12:03:41.689,"Training> Name=main_level/agent, Worker=0, Episode=871, Total reward=67.82, Steps=48103, Training iteration=43"
2019-10-02 12:03:31.686,"Training> Name=main_level/agent, Worker=0, Episode=870, Total reward=12.86, Steps=47967, Training iteration=43"
2019-10-02 12:03:29.685,"Training> Name=main_level/agent, Worker=0, Episode=869, Total reward=8.8, Steps=47939, Training iteration=43"
2019-10-02 12:03:25.684,"Training> Name=main_level/agent, Worker=0, Episode=868, Total reward=24.49, Steps=47912, Training iteration=43"
2019-10-02 12:03:20.682,"Training> Name=main_level/agent, Worker=0, Episode=867, Total reward=55.31, Steps=47839, Training iteration=43"
2019-10-02 12:03:10.679,"Training> Name=main_level/agent, Worker=0, Episode=866, Total reward=23.68, Steps=47698, Training iteration=43"
2019-10-02 12:03:06.678,"Training> Name=main_level/agent, Worker=0, Episode=865, Total reward=135.83, Steps=47640, Training iteration=43"
2019-10-02 12:02:50.673,"Training> Name=main_level/agent, Worker=0, Episode=864, Total reward=25.85, Steps=47413, Training iteration=43"
2019-10-02 12:02:45.672,"Training> Name=main_level/agent, Worker=0, Episode=863, Total reward=64.72, Steps=47346, Training iteration=43"
2019-10-02 12:02:35.669,"Training> Name=main_level/agent, Worker=0, Episode=862, Total reward=39.36, Steps=47200, Training iteration=43"
2019-10-02 12:02:29.667,"Training> Name=main_level/agent, Worker=0, Episode=861, Total reward=62.73, Steps=47123, Training iteration=43"
2019-10-02 12:02:17.663,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_43.pb
2019-10-02 12:02:16.663,Uploaded 3 files for checkpoint 43
2019-10-02 12:02:16.663,INFO:tensorflow:Froze 11 variables.
2019-10-02 12:02:16.663,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 12:02:14.662,"Policy training> Surrogate loss=-0.10722576081752777, KL divergence=0.03893732279539108, Entropy=1.5451233386993408, training epoch=9, learning_rate=3e-05"
2019-10-02 12:02:14.662,Checkpoint> Saving in path=['./checkpoint/43_Step-46975.ckpt']
2019-10-02 12:02:05.144,"Policy training> Surrogate loss=-0.10216303169727325, KL divergence=0.037118442356586456, Entropy=1.551175832748413, training epoch=8, learning_rate=3e-05"
2019-10-02 12:01:56.965,"Policy training> Surrogate loss=-0.10067181289196014, KL divergence=0.036408450454473495, Entropy=1.557092547416687, training epoch=7, learning_rate=3e-05"
2019-10-02 12:01:47.490,"Policy training> Surrogate loss=-0.09547636657953262, KL divergence=0.03529862314462662, Entropy=1.555460810661316, training epoch=6, learning_rate=3e-05"
2019-10-02 12:01:37.714,"Policy training> Surrogate loss=-0.09034550935029984, KL divergence=0.03422901779413223, Entropy=1.563782811164856, training epoch=5, learning_rate=3e-05"
2019-10-02 12:01:28.221,"Policy training> Surrogate loss=-0.08116272836923599, KL divergence=0.033605530858039856, Entropy=1.5737802982330322, training epoch=4, learning_rate=3e-05"
2019-10-02 12:01:19.638,"Policy training> Surrogate loss=-0.07343336194753647, KL divergence=0.03528134897351265, Entropy=1.5794912576675415, training epoch=3, learning_rate=3e-05"
2019-10-02 12:01:10.034,"Policy training> Surrogate loss=-0.06130256876349449, KL divergence=0.030693640932440758, Entropy=1.5793119668960571, training epoch=2, learning_rate=3e-05"
2019-10-02 12:00:59.891,"Policy training> Surrogate loss=-0.03378116339445114, KL divergence=0.03344006836414337, Entropy=1.578624963760376, training epoch=1, learning_rate=3e-05"
2019-10-02 12:00:51.094,"Policy training> Surrogate loss=0.012353986501693726, KL divergence=0.020989596843719482, Entropy=1.5532008409500122, training epoch=0, learning_rate=3e-05"
2019-10-02 12:00:40.138,"Training> Name=main_level/agent, Worker=0, Episode=860, Total reward=23.5, Steps=46975, Training iteration=42"
2019-10-02 12:00:36.131,"Training> Name=main_level/agent, Worker=0, Episode=859, Total reward=45.71, Steps=46921, Training iteration=42"
2019-10-02 12:00:29.129,"Training> Name=main_level/agent, Worker=0, Episode=858, Total reward=59.08, Steps=46823, Training iteration=42"
2019-10-02 12:00:20.126,"Training> Name=main_level/agent, Worker=0, Episode=857, Total reward=30.47, Steps=46694, Training iteration=42"
2019-10-02 12:00:15.125,"Training> Name=main_level/agent, Worker=0, Episode=856, Total reward=42.22, Steps=46630, Training iteration=42"
2019-10-02 12:00:08.123,"Training> Name=main_level/agent, Worker=0, Episode=855, Total reward=31.92, Steps=46542, Training iteration=42"
2019-10-02 12:00:03.121,"Training> Name=main_level/agent, Worker=0, Episode=854, Total reward=27.12, Steps=46471, Training iteration=42"
2019-10-02 11:59:59.120,"Training> Name=main_level/agent, Worker=0, Episode=853, Total reward=8.31, Steps=46412, Training iteration=42"
2019-10-02 11:59:56.119,"Training> Name=main_level/agent, Worker=0, Episode=852, Total reward=12.34, Steps=46375, Training iteration=42"
2019-10-02 11:59:52.118,"Training> Name=main_level/agent, Worker=0, Episode=851, Total reward=32.03, Steps=46332, Training iteration=42"
2019-10-02 11:59:48.116,"Training> Name=main_level/agent, Worker=0, Episode=850, Total reward=11.02, Steps=46266, Training iteration=42"
2019-10-02 11:59:46.116,"Training> Name=main_level/agent, Worker=0, Episode=849, Total reward=190.3, Steps=46245, Training iteration=42"
2019-10-02 11:59:31.112,"Training> Name=main_level/agent, Worker=0, Episode=848, Total reward=77.82, Steps=46028, Training iteration=42"
2019-10-02 11:59:19.109,"Training> Name=main_level/agent, Worker=0, Episode=847, Total reward=66.29, Steps=45855, Training iteration=42"
2019-10-02 11:59:08.106,"Training> Name=main_level/agent, Worker=0, Episode=846, Total reward=49.23, Steps=45715, Training iteration=42"
2019-10-02 11:59:01.104,"Training> Name=main_level/agent, Worker=0, Episode=845, Total reward=49.09, Steps=45608, Training iteration=42"
2019-10-02 11:58:52.101,"Training> Name=main_level/agent, Worker=0, Episode=844, Total reward=30.18, Steps=45484, Training iteration=42"
2019-10-02 11:58:47.100,"Training> Name=main_level/agent, Worker=0, Episode=843, Total reward=39.27, Steps=45417, Training iteration=42"
2019-10-02 11:58:40.098,"Training> Name=main_level/agent, Worker=0, Episode=842, Total reward=25.97, Steps=45330, Training iteration=42"
2019-10-02 11:58:36.096,"Training> Name=main_level/agent, Worker=0, Episode=841, Total reward=52.91, Steps=45269, Training iteration=42"
2019-10-02 11:58:28.094,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:58:28.094,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:58:28.094,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_42.pb
2019-10-02 11:58:27.094,Uploaded 3 files for checkpoint 42
2019-10-02 11:58:25.093,"Policy training> Surrogate loss=-0.11379063874483109, KL divergence=0.03816695138812065, Entropy=1.611717700958252, training epoch=9, learning_rate=3e-05"
2019-10-02 11:58:25.093,Checkpoint> Saving in path=['./checkpoint/42_Step-45172.ckpt']
2019-10-02 11:58:15.797,"Policy training> Surrogate loss=-0.11080826073884964, KL divergence=0.03636402636766434, Entropy=1.6055705547332764, training epoch=8, learning_rate=3e-05"
2019-10-02 11:58:06.904,"Policy training> Surrogate loss=-0.09674820303916931, KL divergence=0.036604054272174835, Entropy=1.6078120470046997, training epoch=7, learning_rate=3e-05"
2019-10-02 11:57:56.588,"Policy training> Surrogate loss=-0.09929066151380539, KL divergence=0.034329112619161606, Entropy=1.6177306175231934, training epoch=6, learning_rate=3e-05"
2019-10-02 11:57:47.428,"Policy training> Surrogate loss=-0.09720028936862946, KL divergence=0.03290324658155441, Entropy=1.6244298219680786, training epoch=5, learning_rate=3e-05"
2019-10-02 11:57:38.166,"Policy training> Surrogate loss=-0.08755511045455933, KL divergence=0.032221149653196335, Entropy=1.6214617490768433, training epoch=4, learning_rate=3e-05"
2019-10-02 11:57:28.327,"Policy training> Surrogate loss=-0.07449635118246078, KL divergence=0.03386836498975754, Entropy=1.6359204053878784, training epoch=3, learning_rate=3e-05"
2019-10-02 11:57:18.323,"Policy training> Surrogate loss=-0.06215973570942879, KL divergence=0.034370481967926025, Entropy=1.6335177421569824, training epoch=2, learning_rate=3e-05"
2019-10-02 11:57:09.580,"Policy training> Surrogate loss=-0.03678189590573311, KL divergence=0.035405028611421585, Entropy=1.6312788724899292, training epoch=1, learning_rate=3e-05"
2019-10-02 11:56:59.353,"Policy training> Surrogate loss=0.015338988043367863, KL divergence=0.018109949305653572, Entropy=1.6142817735671997, training epoch=0, learning_rate=3e-05"
2019-10-02 11:56:48.199,"Training> Name=main_level/agent, Worker=0, Episode=840, Total reward=123.93, Steps=45172, Training iteration=41"
2019-10-02 11:56:34.195,"Training> Name=main_level/agent, Worker=0, Episode=839, Total reward=34.19, Steps=44972, Training iteration=41"
2019-10-02 11:56:29.193,"Training> Name=main_level/agent, Worker=0, Episode=838, Total reward=122.29, Steps=44898, Training iteration=41"
2019-10-02 11:56:15.189,"Training> Name=main_level/agent, Worker=0, Episode=837, Total reward=30.09, Steps=44706, Training iteration=41"
2019-10-02 11:56:11.188,"Training> Name=main_level/agent, Worker=0, Episode=836, Total reward=45.99, Steps=44656, Training iteration=41"
2019-10-02 11:56:05.186,"Training> Name=main_level/agent, Worker=0, Episode=835, Total reward=65.23, Steps=44568, Training iteration=41"
2019-10-02 11:55:56.184,"Training> Name=main_level/agent, Worker=0, Episode=834, Total reward=49.23, Steps=44441, Training iteration=41"
2019-10-02 11:55:49.182,"Training> Name=main_level/agent, Worker=0, Episode=833, Total reward=6.03, Steps=44347, Training iteration=41"
2019-10-02 11:55:46.181,"Training> Name=main_level/agent, Worker=0, Episode=832, Total reward=62.13, Steps=44313, Training iteration=41"
2019-10-02 11:55:37.178,"Training> Name=main_level/agent, Worker=0, Episode=831, Total reward=23.36, Steps=44185, Training iteration=41"
2019-10-02 11:55:33.177,"Training> Name=main_level/agent, Worker=0, Episode=830, Total reward=171.89, Steps=44131, Training iteration=41"
2019-10-02 11:55:19.173,"Training> Name=main_level/agent, Worker=0, Episode=829, Total reward=2.35, Steps=43925, Training iteration=41"
2019-10-02 11:55:17.172,"Training> Name=main_level/agent, Worker=0, Episode=828, Total reward=114.68, Steps=43908, Training iteration=41"
2019-10-02 11:55:03.169,"Training> Name=main_level/agent, Worker=0, Episode=827, Total reward=20.14, Steps=43697, Training iteration=41"
2019-10-02 11:54:59.167,"Training> Name=main_level/agent, Worker=0, Episode=826, Total reward=26.24, Steps=43648, Training iteration=41"
2019-10-02 11:54:54.166,"Training> Name=main_level/agent, Worker=0, Episode=825, Total reward=44.18, Steps=43592, Training iteration=41"
2019-10-02 11:54:47.164,"Training> Name=main_level/agent, Worker=0, Episode=824, Total reward=25.8, Steps=43484, Training iteration=41"
2019-10-02 11:54:43.163,"Training> Name=main_level/agent, Worker=0, Episode=823, Total reward=39.16, Steps=43436, Training iteration=41"
2019-10-02 11:54:37.161,"Training> Name=main_level/agent, Worker=0, Episode=822, Total reward=15.58, Steps=43354, Training iteration=41"
2019-10-02 11:54:35.160,"Training> Name=main_level/agent, Worker=0, Episode=821, Total reward=29.67, Steps=43324, Training iteration=41"
2019-10-02 11:54:29.158,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_41.pb
2019-10-02 11:54:28.158,Uploaded 3 files for checkpoint 41
2019-10-02 11:54:28.158,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:54:28.158,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:54:24.156,Checkpoint> Saving in path=['./checkpoint/41_Step-43260.ckpt']
2019-10-02 11:54:23.156,"Policy training> Surrogate loss=-0.12892989814281464, KL divergence=0.040788210928440094, Entropy=1.6416438817977905, training epoch=9, learning_rate=3e-05"
2019-10-02 11:54:15.811,"Policy training> Surrogate loss=-0.12493883818387985, KL divergence=0.03974221274256706, Entropy=1.6445302963256836, training epoch=8, learning_rate=3e-05"
2019-10-02 11:54:08.582,"Policy training> Surrogate loss=-0.12230359762907028, KL divergence=0.03794226050376892, Entropy=1.637506127357483, training epoch=7, learning_rate=3e-05"
2019-10-02 11:54:01.105,"Policy training> Surrogate loss=-0.11313297599554062, KL divergence=0.03521916642785072, Entropy=1.661888599395752, training epoch=6, learning_rate=3e-05"
2019-10-02 11:53:53.908,"Policy training> Surrogate loss=-0.10272116214036942, KL divergence=0.03606926277279854, Entropy=1.648780345916748, training epoch=5, learning_rate=3e-05"
2019-10-02 11:53:45.418,"Policy training> Surrogate loss=-0.10147418081760406, KL divergence=0.031638242304325104, Entropy=1.6701644659042358, training epoch=4, learning_rate=3e-05"
2019-10-02 11:53:38.284,"Policy training> Surrogate loss=-0.0901976227760315, KL divergence=0.03130192309617996, Entropy=1.6700867414474487, training epoch=3, learning_rate=3e-05"
2019-10-02 11:53:31.028,"Policy training> Surrogate loss=-0.07228069007396698, KL divergence=0.029401089996099472, Entropy=1.6786342859268188, training epoch=2, learning_rate=3e-05"
2019-10-02 11:53:24.219,"Policy training> Surrogate loss=-0.04719143360853195, KL divergence=0.034842267632484436, Entropy=1.6785109043121338, training epoch=1, learning_rate=3e-05"
2019-10-02 11:53:16.122,"Policy training> Surrogate loss=0.011590302921831608, KL divergence=0.020452039316296577, Entropy=1.6606868505477905, training epoch=0, learning_rate=3e-05"
2019-10-02 11:53:06.950,"Training> Name=main_level/agent, Worker=0, Episode=820, Total reward=79.57, Steps=43260, Training iteration=40"
2019-10-02 11:52:54.946,"Training> Name=main_level/agent, Worker=0, Episode=819, Total reward=28.18, Steps=43080, Training iteration=40"
2019-10-02 11:52:49.944,"Training> Name=main_level/agent, Worker=0, Episode=818, Total reward=18.8, Steps=43006, Training iteration=40"
2019-10-02 11:52:45.943,"Training> Name=main_level/agent, Worker=0, Episode=817, Total reward=27.82, Steps=42959, Training iteration=40"
2019-10-02 11:52:41.942,"Training> Name=main_level/agent, Worker=0, Episode=816, Total reward=21.85, Steps=42910, Training iteration=40"
2019-10-02 11:52:37.941,"Training> Name=main_level/agent, Worker=0, Episode=815, Total reward=36.11, Steps=42859, Training iteration=40"
2019-10-02 11:52:31.939,"Training> Name=main_level/agent, Worker=0, Episode=814, Total reward=31.73, Steps=42766, Training iteration=40"
2019-10-02 11:52:25.937,"Training> Name=main_level/agent, Worker=0, Episode=813, Total reward=23.59, Steps=42699, Training iteration=40"
2019-10-02 11:52:20.935,"Training> Name=main_level/agent, Worker=0, Episode=812, Total reward=12.1, Steps=42630, Training iteration=40"
2019-10-02 11:52:17.934,"Training> Name=main_level/agent, Worker=0, Episode=811, Total reward=129.79, Steps=42598, Training iteration=40"
2019-10-02 11:52:04.930,"Training> Name=main_level/agent, Worker=0, Episode=810, Total reward=44.27, Steps=42408, Training iteration=40"
2019-10-02 11:51:56.928,"Training> Name=main_level/agent, Worker=0, Episode=809, Total reward=10.34, Steps=42292, Training iteration=40"
2019-10-02 11:51:54.927,"Training> Name=main_level/agent, Worker=0, Episode=808, Total reward=35.64, Steps=42266, Training iteration=40"
2019-10-02 11:51:47.926,"Training> Name=main_level/agent, Worker=0, Episode=807, Total reward=4.68, Steps=42164, Training iteration=40"
2019-10-02 11:51:45.925,"Training> Name=main_level/agent, Worker=0, Episode=806, Total reward=21.1, Steps=42145, Training iteration=40"
2019-10-02 11:51:42.924,"Training> Name=main_level/agent, Worker=0, Episode=805, Total reward=16.23, Steps=42106, Training iteration=40"
2019-10-02 11:51:39.923,"Training> Name=main_level/agent, Worker=0, Episode=804, Total reward=39.42, Steps=42069, Training iteration=40"
2019-10-02 11:51:32.921,"Training> Name=main_level/agent, Worker=0, Episode=803, Total reward=48.08, Steps=41970, Training iteration=40"
2019-10-02 11:51:24.918,"Training> Name=main_level/agent, Worker=0, Episode=802, Total reward=39.65, Steps=41855, Training iteration=40"
2019-10-02 11:51:19.917,"Training> Name=main_level/agent, Worker=0, Episode=801, Total reward=11.64, Steps=41787, Training iteration=40"
2019-10-02 11:51:15.916,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_40.pb
2019-10-02 11:51:14.915,Uploaded 3 files for checkpoint 40
2019-10-02 11:51:14.915,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:51:14.915,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:51:11.914,"Policy training> Surrogate loss=-0.10216622799634933, KL divergence=0.04219662770628929, Entropy=1.6400275230407715, training epoch=9, learning_rate=3e-05"
2019-10-02 11:51:11.914,Checkpoint> Saving in path=['./checkpoint/40_Step-41766.ckpt']
2019-10-02 11:51:04.086,"Policy training> Surrogate loss=-0.10943802446126938, KL divergence=0.04049088805913925, Entropy=1.6456420421600342, training epoch=8, learning_rate=3e-05"
2019-10-02 11:50:55.522,"Policy training> Surrogate loss=-0.10247679799795151, KL divergence=0.03966724872589111, Entropy=1.6347192525863647, training epoch=7, learning_rate=3e-05"
2019-10-02 11:50:46.446,"Policy training> Surrogate loss=-0.10092277079820633, KL divergence=0.03821726143360138, Entropy=1.6579132080078125, training epoch=6, learning_rate=3e-05"
2019-10-02 11:50:37.287,"Policy training> Surrogate loss=-0.09094531834125519, KL divergence=0.03698134049773216, Entropy=1.6401054859161377, training epoch=5, learning_rate=3e-05"
2019-10-02 11:50:29.410,"Policy training> Surrogate loss=-0.0912933349609375, KL divergence=0.03412258252501488, Entropy=1.6648304462432861, training epoch=4, learning_rate=3e-05"
2019-10-02 11:50:20.930,"Policy training> Surrogate loss=-0.07966026663780212, KL divergence=0.035567283630371094, Entropy=1.6444261074066162, training epoch=3, learning_rate=3e-05"
2019-10-02 11:50:11.634,"Policy training> Surrogate loss=-0.06757695972919464, KL divergence=0.031168507412075996, Entropy=1.6904242038726807, training epoch=2, learning_rate=3e-05"
2019-10-02 11:50:02.582,"Policy training> Surrogate loss=-0.030892405658960342, KL divergence=0.03162268176674843, Entropy=1.6722289323806763, training epoch=1, learning_rate=3e-05"
2019-10-02 11:49:55.218,"Policy training> Surrogate loss=0.006536573171615601, KL divergence=0.018942885100841522, Entropy=1.6661815643310547, training epoch=0, learning_rate=3e-05"
2019-10-02 11:49:45.442,"Training> Name=main_level/agent, Worker=0, Episode=800, Total reward=16.62, Steps=41766, Training iteration=39"
2019-10-02 11:49:42.430,"Training> Name=main_level/agent, Worker=0, Episode=799, Total reward=10.73, Steps=41735, Training iteration=39"
2019-10-02 11:49:39.429,"Training> Name=main_level/agent, Worker=0, Episode=798, Total reward=199.89, Steps=41708, Training iteration=39"
2019-10-02 11:49:26.425,"Training> Name=main_level/agent, Worker=0, Episode=797, Total reward=59.01, Steps=41513, Training iteration=39"
2019-10-02 11:49:17.423,"Training> Name=main_level/agent, Worker=0, Episode=796, Total reward=57.04, Steps=41386, Training iteration=39"
2019-10-02 11:49:08.420,"Training> Name=main_level/agent, Worker=0, Episode=795, Total reward=55.71, Steps=41266, Training iteration=39"
2019-10-02 11:49:00.418,"Training> Name=main_level/agent, Worker=0, Episode=794, Total reward=72.74, Steps=41155, Training iteration=39"
2019-10-02 11:48:49.414,"Training> Name=main_level/agent, Worker=0, Episode=793, Total reward=74.21, Steps=40989, Training iteration=39"
2019-10-02 11:48:37.411,"Training> Name=main_level/agent, Worker=0, Episode=792, Total reward=73.75, Steps=40821, Training iteration=39"
2019-10-02 11:48:26.407,"Training> Name=main_level/agent, Worker=0, Episode=791, Total reward=20.83, Steps=40660, Training iteration=39"
2019-10-02 11:48:22.406,"Training> Name=main_level/agent, Worker=0, Episode=790, Total reward=13.93, Steps=40614, Training iteration=39"
2019-10-02 11:48:20.406,"Training> Name=main_level/agent, Worker=0, Episode=789, Total reward=26.74, Steps=40590, Training iteration=39"
2019-10-02 11:48:16.404,"Training> Name=main_level/agent, Worker=0, Episode=788, Total reward=15.94, Steps=40527, Training iteration=39"
2019-10-02 11:48:13.403,"Training> Name=main_level/agent, Worker=0, Episode=787, Total reward=31.97, Steps=40503, Training iteration=39"
2019-10-02 11:48:08.402,"Training> Name=main_level/agent, Worker=0, Episode=786, Total reward=37.61, Steps=40432, Training iteration=39"
2019-10-02 11:48:01.399,"Training> Name=main_level/agent, Worker=0, Episode=785, Total reward=24.07, Steps=40329, Training iteration=39"
2019-10-02 11:47:56.398,"Training> Name=main_level/agent, Worker=0, Episode=784, Total reward=20.02, Steps=40265, Training iteration=39"
2019-10-02 11:47:53.397,"Training> Name=main_level/agent, Worker=0, Episode=783, Total reward=16.44, Steps=40227, Training iteration=39"
2019-10-02 11:47:49.396,"Training> Name=main_level/agent, Worker=0, Episode=782, Total reward=24.38, Steps=40179, Training iteration=39"
2019-10-02 11:47:46.395,"Training> Name=main_level/agent, Worker=0, Episode=781, Total reward=41.5, Steps=40131, Training iteration=39"
2019-10-02 11:47:39.393,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_39.pb
2019-10-02 11:47:38.392,Uploaded 3 files for checkpoint 39
2019-10-02 11:47:38.392,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:47:38.392,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:47:36.391,Checkpoint> Saving in path=['./checkpoint/39_Step-40055.ckpt']
2019-10-02 11:47:35.391,"Policy training> Surrogate loss=-0.12320735305547714, KL divergence=0.0455351397395134, Entropy=1.6811952590942383, training epoch=9, learning_rate=3e-05"
2019-10-02 11:47:28.590,"Policy training> Surrogate loss=-0.11972680687904358, KL divergence=0.04536129906773567, Entropy=1.6728230714797974, training epoch=8, learning_rate=3e-05"
2019-10-02 11:47:21.507,"Policy training> Surrogate loss=-0.11309441179037094, KL divergence=0.041182272136211395, Entropy=1.69072687625885, training epoch=7, learning_rate=3e-05"
2019-10-02 11:47:15.216,"Policy training> Surrogate loss=-0.1083642765879631, KL divergence=0.0419142059981823, Entropy=1.6987814903259277, training epoch=6, learning_rate=3e-05"
2019-10-02 11:47:07.859,"Policy training> Surrogate loss=-0.10559175908565521, KL divergence=0.037010177969932556, Entropy=1.700440764427185, training epoch=5, learning_rate=3e-05"
2019-10-02 11:47:00.614,"Policy training> Surrogate loss=-0.09557493776082993, KL divergence=0.037018440663814545, Entropy=1.7059361934661865, training epoch=4, learning_rate=3e-05"
2019-10-02 11:46:53.449,"Policy training> Surrogate loss=-0.08648339658975601, KL divergence=0.03871317580342293, Entropy=1.705554723739624, training epoch=3, learning_rate=3e-05"
2019-10-02 11:46:47.043,"Policy training> Surrogate loss=-0.06555052846670151, KL divergence=0.039775073528289795, Entropy=1.7314661741256714, training epoch=2, learning_rate=3e-05"
2019-10-02 11:46:39.775,"Policy training> Surrogate loss=-0.03650397062301636, KL divergence=0.03881456330418587, Entropy=1.7253161668777466, training epoch=1, learning_rate=3e-05"
2019-10-02 11:46:32.266,"Policy training> Surrogate loss=0.015466692857444286, KL divergence=0.02293342538177967, Entropy=1.6921147108078003, training epoch=0, learning_rate=3e-05"
2019-10-02 11:46:25.535,"Training> Name=main_level/agent, Worker=0, Episode=780, Total reward=9.38, Steps=40055, Training iteration=38"
2019-10-02 11:46:23.330,"Training> Name=main_level/agent, Worker=0, Episode=779, Total reward=17.59, Steps=40035, Training iteration=38"
2019-10-02 11:46:19.329,"Training> Name=main_level/agent, Worker=0, Episode=778, Total reward=44.08, Steps=39991, Training iteration=38"
2019-10-02 11:46:14.327,"Training> Name=main_level/agent, Worker=0, Episode=777, Total reward=56.82, Steps=39921, Training iteration=38"
2019-10-02 11:46:06.325,"Training> Name=main_level/agent, Worker=0, Episode=776, Total reward=6.41, Steps=39810, Training iteration=38"
2019-10-02 11:46:05.324,"Training> Name=main_level/agent, Worker=0, Episode=775, Total reward=36.17, Steps=39794, Training iteration=38"
2019-10-02 11:45:59.322,"Training> Name=main_level/agent, Worker=0, Episode=774, Total reward=26.76, Steps=39717, Training iteration=38"
2019-10-02 11:45:54.321,"Training> Name=main_level/agent, Worker=0, Episode=773, Total reward=6.72, Steps=39655, Training iteration=38"
2019-10-02 11:45:52.320,"Training> Name=main_level/agent, Worker=0, Episode=772, Total reward=11.17, Steps=39633, Training iteration=38"
2019-10-02 11:45:49.319,"Training> Name=main_level/agent, Worker=0, Episode=771, Total reward=20.24, Steps=39602, Training iteration=38"
2019-10-02 11:45:46.318,"Training> Name=main_level/agent, Worker=0, Episode=770, Total reward=29.93, Steps=39556, Training iteration=38"
2019-10-02 11:45:41.317,"Training> Name=main_level/agent, Worker=0, Episode=769, Total reward=23.53, Steps=39484, Training iteration=38"
2019-10-02 11:45:36.315,"Training> Name=main_level/agent, Worker=0, Episode=768, Total reward=10.1, Steps=39420, Training iteration=38"
2019-10-02 11:45:33.314,"Training> Name=main_level/agent, Worker=0, Episode=767, Total reward=81.15, Steps=39391, Training iteration=38"
2019-10-02 11:45:20.311,"Training> Name=main_level/agent, Worker=0, Episode=766, Total reward=115.97, Steps=39206, Training iteration=38"
2019-10-02 11:45:06.306,"Training> Name=main_level/agent, Worker=0, Episode=765, Total reward=21.79, Steps=39002, Training iteration=38"
2019-10-02 11:45:03.306,"Training> Name=main_level/agent, Worker=0, Episode=764, Total reward=29.83, Steps=38954, Training iteration=38"
2019-10-02 11:44:58.304,"Training> Name=main_level/agent, Worker=0, Episode=763, Total reward=29.32, Steps=38886, Training iteration=38"
2019-10-02 11:44:53.303,"Training> Name=main_level/agent, Worker=0, Episode=762, Total reward=50.67, Steps=38831, Training iteration=38"
2019-10-02 11:44:47.301,"Training> Name=main_level/agent, Worker=0, Episode=761, Total reward=21.86, Steps=38748, Training iteration=38"
2019-10-02 11:44:43.300,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_38.pb
2019-10-02 11:44:42.299,Uploaded 3 files for checkpoint 38
2019-10-02 11:44:42.299,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:44:42.299,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:44:32.296,Checkpoint> Saving in path=['./checkpoint/38_Step-38711.ckpt']
2019-10-02 11:44:31.296,"Policy training> Surrogate loss=-0.11236967146396637, KL divergence=0.03691273182630539, Entropy=1.6696306467056274, training epoch=9, learning_rate=3e-05"
2019-10-02 11:44:23.751,"Policy training> Surrogate loss=-0.10771900415420532, KL divergence=0.037909649312496185, Entropy=1.6919492483139038, training epoch=8, learning_rate=3e-05"
2019-10-02 11:44:17.265,"Policy training> Surrogate loss=-0.1050347164273262, KL divergence=0.03352523222565651, Entropy=1.6772934198379517, training epoch=7, learning_rate=3e-05"
2019-10-02 11:44:09.823,"Policy training> Surrogate loss=-0.09669973701238632, KL divergence=0.03255031257867813, Entropy=1.7053346633911133, training epoch=6, learning_rate=3e-05"
2019-10-02 11:44:01.726,"Policy training> Surrogate loss=-0.09719090908765793, KL divergence=0.030323589220643044, Entropy=1.6869845390319824, training epoch=5, learning_rate=3e-05"
2019-10-02 11:43:53.522,"Policy training> Surrogate loss=-0.08848153799772263, KL divergence=0.032135892659425735, Entropy=1.6759727001190186, training epoch=4, learning_rate=3e-05"
2019-10-02 11:43:46.899,"Policy training> Surrogate loss=-0.0769234299659729, KL divergence=0.03222543001174927, Entropy=1.7077968120574951, training epoch=3, learning_rate=3e-05"
2019-10-02 11:43:39.166,"Policy training> Surrogate loss=-0.06858324259519577, KL divergence=0.028634393587708473, Entropy=1.6992026567459106, training epoch=2, learning_rate=3e-05"
2019-10-02 11:43:32.018,"Policy training> Surrogate loss=-0.044636182487010956, KL divergence=0.02372991479933262, Entropy=1.7004984617233276, training epoch=1, learning_rate=3e-05"
2019-10-02 11:43:23.215,"Policy training> Surrogate loss=0.010757796466350555, KL divergence=0.016702741384506226, Entropy=1.6638293266296387, training epoch=0, learning_rate=3e-05"
2019-10-02 11:43:14.981,"Training> Name=main_level/agent, Worker=0, Episode=760, Total reward=20.26, Steps=38711, Training iteration=37"
2019-10-02 11:43:10.958,"Training> Name=main_level/agent, Worker=0, Episode=759, Total reward=20.28, Steps=38668, Training iteration=37"
2019-10-02 11:43:06.957,"Training> Name=main_level/agent, Worker=0, Episode=758, Total reward=28.72, Steps=38617, Training iteration=37"
2019-10-02 11:43:02.956,"Training> Name=main_level/agent, Worker=0, Episode=757, Total reward=63.95, Steps=38559, Training iteration=37"
2019-10-02 11:42:52.953,"Training> Name=main_level/agent, Worker=0, Episode=756, Total reward=27.54, Steps=38421, Training iteration=37"
2019-10-02 11:42:47.951,"Training> Name=main_level/agent, Worker=0, Episode=755, Total reward=16.28, Steps=38360, Training iteration=37"
2019-10-02 11:42:44.950,"Training> Name=main_level/agent, Worker=0, Episode=754, Total reward=61.78, Steps=38322, Training iteration=37"
2019-10-02 11:42:36.948,"Training> Name=main_level/agent, Worker=0, Episode=753, Total reward=65.1, Steps=38209, Training iteration=37"
2019-10-02 11:42:24.945,"Training> Name=main_level/agent, Worker=0, Episode=752, Total reward=32.16, Steps=38039, Training iteration=37"
2019-10-02 11:42:18.943,"Training> Name=main_level/agent, Worker=0, Episode=751, Total reward=69.2, Steps=37959, Training iteration=37"
2019-10-02 11:42:07.940,"Training> Name=main_level/agent, Worker=0, Episode=750, Total reward=66.09, Steps=37801, Training iteration=37"
2019-10-02 11:41:58.937,"Training> Name=main_level/agent, Worker=0, Episode=749, Total reward=6.94, Steps=37669, Training iteration=37"
2019-10-02 11:41:57.937,"Training> Name=main_level/agent, Worker=0, Episode=748, Total reward=23.46, Steps=37653, Training iteration=37"
2019-10-02 11:41:53.935,"Training> Name=main_level/agent, Worker=0, Episode=747, Total reward=8.51, Steps=37609, Training iteration=37"
2019-10-02 11:41:51.935,"Training> Name=main_level/agent, Worker=0, Episode=746, Total reward=20.22, Steps=37586, Training iteration=37"
2019-10-02 11:41:47.934,"Training> Name=main_level/agent, Worker=0, Episode=745, Total reward=32.87, Steps=37532, Training iteration=37"
2019-10-02 11:41:42.932,"Training> Name=main_level/agent, Worker=0, Episode=744, Total reward=22.54, Steps=37455, Training iteration=37"
2019-10-02 11:41:38.931,"Training> Name=main_level/agent, Worker=0, Episode=743, Total reward=20.71, Steps=37403, Training iteration=37"
2019-10-02 11:41:33.929,"Training> Name=main_level/agent, Worker=0, Episode=742, Total reward=35.49, Steps=37354, Training iteration=37"
2019-10-02 11:41:28.928,"Training> Name=main_level/agent, Worker=0, Episode=741, Total reward=20.0, Steps=37284, Training iteration=37"
2019-10-02 11:41:24.927,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_37.pb
2019-10-02 11:41:23.926,Uploaded 3 files for checkpoint 37
2019-10-02 11:41:23.926,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:41:23.926,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:41:21.925,Checkpoint> Saving in path=['./checkpoint/37_Step-37239.ckpt']
2019-10-02 11:41:20.925,"Policy training> Surrogate loss=-0.09906817972660065, KL divergence=0.03580045327544212, Entropy=1.750474452972412, training epoch=9, learning_rate=3e-05"
2019-10-02 11:41:11.263,"Policy training> Surrogate loss=-0.10472094267606735, KL divergence=0.03426147252321243, Entropy=1.7312254905700684, training epoch=8, learning_rate=3e-05"
2019-10-02 11:41:01.982,"Policy training> Surrogate loss=-0.09625192731618881, KL divergence=0.03223202005028725, Entropy=1.7427818775177002, training epoch=7, learning_rate=3e-05"
2019-10-02 11:40:51.111,"Policy training> Surrogate loss=-0.09313119947910309, KL divergence=0.03136981278657913, Entropy=1.7506393194198608, training epoch=6, learning_rate=3e-05"
2019-10-02 11:40:41.742,"Policy training> Surrogate loss=-0.09350273013114929, KL divergence=0.028994133695960045, Entropy=1.7513500452041626, training epoch=5, learning_rate=3e-05"
2019-10-02 11:40:31.491,"Policy training> Surrogate loss=-0.07706926017999649, KL divergence=0.02851790376007557, Entropy=1.756115436553955, training epoch=4, learning_rate=3e-05"
2019-10-02 11:40:21.360,"Policy training> Surrogate loss=-0.06922145932912827, KL divergence=0.026839397847652435, Entropy=1.738465666770935, training epoch=3, learning_rate=3e-05"
2019-10-02 11:40:11.682,"Policy training> Surrogate loss=-0.05582122132182121, KL divergence=0.02612578310072422, Entropy=1.7665544748306274, training epoch=2, learning_rate=3e-05"
2019-10-02 11:40:01.156,"Policy training> Surrogate loss=-0.04310331493616104, KL divergence=0.02683473750948906, Entropy=1.7500462532043457, training epoch=1, learning_rate=3e-05"
2019-10-02 11:39:51.758,"Policy training> Surrogate loss=0.008558124303817749, KL divergence=0.018770940601825714, Entropy=1.7334513664245605, training epoch=0, learning_rate=3e-05"
2019-10-02 11:39:40.455,"Training> Name=main_level/agent, Worker=0, Episode=740, Total reward=66.21, Steps=37239, Training iteration=36"
2019-10-02 11:39:30.452,"Training> Name=main_level/agent, Worker=0, Episode=739, Total reward=34.34, Steps=37096, Training iteration=36"
2019-10-02 11:39:24.450,"Training> Name=main_level/agent, Worker=0, Episode=738, Total reward=54.95, Steps=37015, Training iteration=36"
2019-10-02 11:39:16.447,"Training> Name=main_level/agent, Worker=0, Episode=737, Total reward=86.12, Steps=36906, Training iteration=36"
2019-10-02 11:39:06.444,"Training> Name=main_level/agent, Worker=0, Episode=736, Total reward=36.67, Steps=36755, Training iteration=36"
2019-10-02 11:39:00.443,"Training> Name=main_level/agent, Worker=0, Episode=735, Total reward=31.54, Steps=36677, Training iteration=36"
2019-10-02 11:38:54.441,"Training> Name=main_level/agent, Worker=0, Episode=734, Total reward=55.5, Steps=36597, Training iteration=36"
2019-10-02 11:38:46.438,"Training> Name=main_level/agent, Worker=0, Episode=733, Total reward=53.52, Steps=36483, Training iteration=36"
2019-10-02 11:38:37.436,"Training> Name=main_level/agent, Worker=0, Episode=732, Total reward=198.44, Steps=36360, Training iteration=36"
2019-10-02 11:38:22.431,"Training> Name=main_level/agent, Worker=0, Episode=731, Total reward=62.85, Steps=36135, Training iteration=36"
2019-10-02 11:38:13.429,"Training> Name=main_level/agent, Worker=0, Episode=730, Total reward=11.31, Steps=36005, Training iteration=36"
2019-10-02 11:38:11.428,"Training> Name=main_level/agent, Worker=0, Episode=729, Total reward=13.77, Steps=35986, Training iteration=36"
2019-10-02 11:38:08.427,"Training> Name=main_level/agent, Worker=0, Episode=728, Total reward=18.85, Steps=35956, Training iteration=36"
2019-10-02 11:38:05.426,"Training> Name=main_level/agent, Worker=0, Episode=727, Total reward=16.09, Steps=35916, Training iteration=36"
2019-10-02 11:38:02.425,"Training> Name=main_level/agent, Worker=0, Episode=726, Total reward=30.12, Steps=35878, Training iteration=36"
2019-10-02 11:37:57.423,"Training> Name=main_level/agent, Worker=0, Episode=725, Total reward=74.4, Steps=35810, Training iteration=36"
2019-10-02 11:37:44.419,"Training> Name=main_level/agent, Worker=0, Episode=724, Total reward=37.78, Steps=35633, Training iteration=36"
2019-10-02 11:37:39.418,"Training> Name=main_level/agent, Worker=0, Episode=723, Total reward=27.53, Steps=35560, Training iteration=36"
2019-10-02 11:37:35.416,"Training> Name=main_level/agent, Worker=0, Episode=722, Total reward=59.44, Steps=35504, Training iteration=36"
2019-10-02 11:37:25.414,"Training> Name=main_level/agent, Worker=0, Episode=721, Total reward=47.59, Steps=35369, Training iteration=36"
2019-10-02 11:37:16.410,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_36.pb
2019-10-02 11:37:15.410,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:37:15.410,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:37:15.409,Uploaded 3 files for checkpoint 36
2019-10-02 11:37:13.409,"Policy training> Surrogate loss=-0.10713521391153336, KL divergence=0.03855690732598305, Entropy=1.717655062675476, training epoch=9, learning_rate=3e-05"
2019-10-02 11:37:13.409,Checkpoint> Saving in path=['./checkpoint/36_Step-35262.ckpt']
2019-10-02 11:37:04.226,"Policy training> Surrogate loss=-0.1055707111954689, KL divergence=0.03904622420668602, Entropy=1.7391759157180786, training epoch=8, learning_rate=3e-05"
2019-10-02 11:36:54.789,"Policy training> Surrogate loss=-0.10503195226192474, KL divergence=0.039181485772132874, Entropy=1.7214738130569458, training epoch=7, learning_rate=3e-05"
2019-10-02 11:36:46.462,"Policy training> Surrogate loss=-0.09458566457033157, KL divergence=0.035156819969415665, Entropy=1.7358111143112183, training epoch=6, learning_rate=3e-05"
2019-10-02 11:36:38.033,"Policy training> Surrogate loss=-0.10076157003641129, KL divergence=0.03393715247511864, Entropy=1.7239848375320435, training epoch=5, learning_rate=3e-05"
2019-10-02 11:36:29.826,"Policy training> Surrogate loss=-0.09057549387216568, KL divergence=0.035039111971855164, Entropy=1.7253050804138184, training epoch=4, learning_rate=3e-05"
2019-10-02 11:36:20.934,"Policy training> Surrogate loss=-0.06771055608987808, KL divergence=0.03528120368719101, Entropy=1.7429733276367188, training epoch=3, learning_rate=3e-05"
2019-10-02 11:36:11.661,"Policy training> Surrogate loss=-0.06274586170911789, KL divergence=0.034348104149103165, Entropy=1.7376446723937988, training epoch=2, learning_rate=3e-05"
2019-10-02 11:36:03.276,"Policy training> Surrogate loss=-0.04031800851225853, KL divergence=0.03715859726071358, Entropy=1.7327936887741089, training epoch=1, learning_rate=3e-05"
2019-10-02 11:35:54.806,"Policy training> Surrogate loss=0.007658291142433882, KL divergence=0.02287636138498783, Entropy=1.724491834640503, training epoch=0, learning_rate=3e-05"
2019-10-02 11:35:44.879,"Training> Name=main_level/agent, Worker=0, Episode=720, Total reward=27.32, Steps=35262, Training iteration=35"
2019-10-02 11:35:39.875,"Training> Name=main_level/agent, Worker=0, Episode=719, Total reward=16.84, Steps=35202, Training iteration=35"
2019-10-02 11:35:36.874,"Training> Name=main_level/agent, Worker=0, Episode=718, Total reward=15.48, Steps=35160, Training iteration=35"
2019-10-02 11:35:34.874,"Training> Name=main_level/agent, Worker=0, Episode=717, Total reward=32.12, Steps=35136, Training iteration=35"
2019-10-02 11:35:29.872,"Training> Name=main_level/agent, Worker=0, Episode=716, Total reward=69.27, Steps=35069, Training iteration=35"
2019-10-02 11:35:18.869,"Training> Name=main_level/agent, Worker=0, Episode=715, Total reward=67.93, Steps=34916, Training iteration=35"
2019-10-02 11:35:08.866,"Training> Name=main_level/agent, Worker=0, Episode=714, Total reward=57.98, Steps=34772, Training iteration=35"
2019-10-02 11:35:00.864,"Training> Name=main_level/agent, Worker=0, Episode=713, Total reward=6.12, Steps=34656, Training iteration=35"
2019-10-02 11:34:58.863,"Training> Name=main_level/agent, Worker=0, Episode=712, Total reward=12.57, Steps=34634, Training iteration=35"
2019-10-02 11:34:55.862,"Training> Name=main_level/agent, Worker=0, Episode=711, Total reward=165.99, Steps=34601, Training iteration=35"
2019-10-02 11:34:40.858,"Training> Name=main_level/agent, Worker=0, Episode=710, Total reward=52.9, Steps=34387, Training iteration=35"
2019-10-02 11:34:32.855,"Training> Name=main_level/agent, Worker=0, Episode=709, Total reward=4.32, Steps=34263, Training iteration=35"
2019-10-02 11:34:30.855,"Training> Name=main_level/agent, Worker=0, Episode=708, Total reward=34.17, Steps=34247, Training iteration=35"
2019-10-02 11:34:23.852,"Training> Name=main_level/agent, Worker=0, Episode=707, Total reward=14.68, Steps=34149, Training iteration=35"
2019-10-02 11:34:20.851,"Training> Name=main_level/agent, Worker=0, Episode=706, Total reward=126.25, Steps=34112, Training iteration=35"
2019-10-02 11:34:05.847,"Training> Name=main_level/agent, Worker=0, Episode=705, Total reward=16.28, Steps=33895, Training iteration=35"
2019-10-02 11:34:02.847,"Training> Name=main_level/agent, Worker=0, Episode=704, Total reward=30.67, Steps=33850, Training iteration=35"
2019-10-02 11:33:56.845,"Training> Name=main_level/agent, Worker=0, Episode=703, Total reward=22.16, Steps=33775, Training iteration=35"
2019-10-02 11:33:51.843,"Training> Name=main_level/agent, Worker=0, Episode=702, Total reward=31.57, Steps=33718, Training iteration=35"
2019-10-02 11:33:46.842,"Training> Name=main_level/agent, Worker=0, Episode=701, Total reward=49.43, Steps=33652, Training iteration=35"
2019-10-02 11:33:37.839,Uploaded 3 files for checkpoint 35
2019-10-02 11:33:37.839,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:33:37.839,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:33:37.839,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_35.pb
2019-10-02 11:33:32.837,Checkpoint> Saving in path=['./checkpoint/35_Step-33549.ckpt']
2019-10-02 11:33:31.837,"Policy training> Surrogate loss=-0.10627007484436035, KL divergence=0.03766583278775215, Entropy=1.7461284399032593, training epoch=9, learning_rate=3e-05"
2019-10-02 11:33:24.738,"Policy training> Surrogate loss=-0.10503419488668442, KL divergence=0.03697044029831886, Entropy=1.745789647102356, training epoch=8, learning_rate=3e-05"
2019-10-02 11:33:16.206,"Policy training> Surrogate loss=-0.09887085109949112, KL divergence=0.03387811779975891, Entropy=1.7547054290771484, training epoch=7, learning_rate=3e-05"
2019-10-02 11:33:08.812,"Policy training> Surrogate loss=-0.0965830609202385, KL divergence=0.03246840462088585, Entropy=1.760535717010498, training epoch=6, learning_rate=3e-05"
2019-10-02 11:33:00.422,"Policy training> Surrogate loss=-0.08806365728378296, KL divergence=0.031845562160015106, Entropy=1.7508641481399536, training epoch=5, learning_rate=3e-05"
2019-10-02 11:32:52.224,"Policy training> Surrogate loss=-0.08356001228094101, KL divergence=0.029338909313082695, Entropy=1.7550721168518066, training epoch=4, learning_rate=3e-05"
2019-10-02 11:32:45.017,"Policy training> Surrogate loss=-0.07420192658901215, KL divergence=0.02932821959257126, Entropy=1.7752127647399902, training epoch=3, learning_rate=3e-05"
2019-10-02 11:32:37.502,"Policy training> Surrogate loss=-0.06258191913366318, KL divergence=0.031155725941061974, Entropy=1.7286120653152466, training epoch=2, learning_rate=3e-05"
2019-10-02 11:32:29.180,"Policy training> Surrogate loss=-0.029297584667801857, KL divergence=0.03808719292283058, Entropy=1.7951740026474, training epoch=1, learning_rate=3e-05"
2019-10-02 11:32:22.122,"Policy training> Surrogate loss=0.017190488055348396, KL divergence=0.018539685755968094, Entropy=1.709424614906311, training epoch=0, learning_rate=3e-05"
2019-10-02 11:32:12.195,"Training> Name=main_level/agent, Worker=0, Episode=700, Total reward=71.66, Steps=33549, Training iteration=34"
2019-10-02 11:32:01.192,"Training> Name=main_level/agent, Worker=0, Episode=699, Total reward=18.21, Steps=33385, Training iteration=34"
2019-10-02 11:31:57.190,"Training> Name=main_level/agent, Worker=0, Episode=698, Total reward=37.81, Steps=33341, Training iteration=34"
2019-10-02 11:31:53.189,"Training> Name=main_level/agent, Worker=0, Episode=697, Total reward=10.0, Steps=33280, Training iteration=34"
2019-10-02 11:31:51.188,"Training> Name=main_level/agent, Worker=0, Episode=696, Total reward=84.24, Steps=33262, Training iteration=34"
2019-10-02 11:31:38.185,"Training> Name=main_level/agent, Worker=0, Episode=695, Total reward=20.33, Steps=33073, Training iteration=34"
2019-10-02 11:31:33.183,"Training> Name=main_level/agent, Worker=0, Episode=694, Total reward=47.2, Steps=33018, Training iteration=34"
2019-10-02 11:31:27.181,"Training> Name=main_level/agent, Worker=0, Episode=693, Total reward=17.94, Steps=32923, Training iteration=34"
2019-10-02 11:31:22.180,"Training> Name=main_level/agent, Worker=0, Episode=692, Total reward=12.08, Steps=32862, Training iteration=34"
2019-10-02 11:31:19.179,"Training> Name=main_level/agent, Worker=0, Episode=691, Total reward=18.2, Steps=32830, Training iteration=34"
2019-10-02 11:31:16.178,"Training> Name=main_level/agent, Worker=0, Episode=690, Total reward=35.19, Steps=32778, Training iteration=34"
2019-10-02 11:31:09.175,"Training> Name=main_level/agent, Worker=0, Episode=689, Total reward=2.17, Steps=32687, Training iteration=34"
2019-10-02 11:31:08.175,"Training> Name=main_level/agent, Worker=0, Episode=688, Total reward=57.37, Steps=32674, Training iteration=34"
2019-10-02 11:30:57.172,"Training> Name=main_level/agent, Worker=0, Episode=687, Total reward=24.17, Steps=32519, Training iteration=34"
2019-10-02 11:30:52.170,"Training> Name=main_level/agent, Worker=0, Episode=686, Total reward=32.52, Steps=32456, Training iteration=34"
2019-10-02 11:30:47.169,"Training> Name=main_level/agent, Worker=0, Episode=685, Total reward=22.36, Steps=32387, Training iteration=34"
2019-10-02 11:30:43.167,"Training> Name=main_level/agent, Worker=0, Episode=684, Total reward=17.21, Steps=32332, Training iteration=34"
2019-10-02 11:30:40.166,"Training> Name=main_level/agent, Worker=0, Episode=683, Total reward=10.44, Steps=32299, Training iteration=34"
2019-10-02 11:30:38.166,"Training> Name=main_level/agent, Worker=0, Episode=682, Total reward=28.73, Steps=32273, Training iteration=34"
2019-10-02 11:30:33.164,"Training> Name=main_level/agent, Worker=0, Episode=681, Total reward=193.46, Steps=32210, Training iteration=34"
2019-10-02 11:30:20.161,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_34.pb
2019-10-02 11:30:17.160,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:30:17.159,Uploaded 3 files for checkpoint 34
2019-10-02 11:30:17.159,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:30:14.158,"Policy training> Surrogate loss=-0.12610913813114166, KL divergence=0.03837244585156441, Entropy=1.7224866151809692, training epoch=9, learning_rate=3e-05"
2019-10-02 11:30:14.158,Checkpoint> Saving in path=['./checkpoint/34_Step-32008.ckpt']
2019-10-02 11:30:08.999,"Policy training> Surrogate loss=-0.11887968331575394, KL divergence=0.03709586337208748, Entropy=1.7201073169708252, training epoch=8, learning_rate=3e-05"
2019-10-02 11:30:03.090,"Policy training> Surrogate loss=-0.1199261024594307, KL divergence=0.0357893630862236, Entropy=1.727001428604126, training epoch=7, learning_rate=3e-05"
2019-10-02 11:29:57.436,"Policy training> Surrogate loss=-0.10915833711624146, KL divergence=0.03866991028189659, Entropy=1.7380317449569702, training epoch=6, learning_rate=3e-05"
2019-10-02 11:29:51.758,"Policy training> Surrogate loss=-0.09800848364830017, KL divergence=0.0331125371158123, Entropy=1.750403642654419, training epoch=5, learning_rate=3e-05"
2019-10-02 11:29:45.988,"Policy training> Surrogate loss=-0.0921521782875061, KL divergence=0.03251146897673607, Entropy=1.7499359846115112, training epoch=4, learning_rate=3e-05"
2019-10-02 11:29:40.946,"Policy training> Surrogate loss=-0.08467047661542892, KL divergence=0.03175074979662895, Entropy=1.7621965408325195, training epoch=3, learning_rate=3e-05"
2019-10-02 11:29:34.646,"Policy training> Surrogate loss=-0.0645347535610199, KL divergence=0.030408084392547607, Entropy=1.7709619998931885, training epoch=2, learning_rate=3e-05"
2019-10-02 11:29:29.045,"Policy training> Surrogate loss=-0.036098502576351166, KL divergence=0.03327333554625511, Entropy=1.7581663131713867, training epoch=1, learning_rate=3e-05"
2019-10-02 11:29:23.923,"Policy training> Surrogate loss=0.018557295203208923, KL divergence=0.013547152280807495, Entropy=1.7774068117141724, training epoch=0, learning_rate=3e-05"
2019-10-02 11:29:17.926,"Training> Name=main_level/agent, Worker=0, Episode=680, Total reward=25.92, Steps=32008, Training iteration=33"
2019-10-02 11:29:13.921,"Training> Name=main_level/agent, Worker=0, Episode=679, Total reward=18.28, Steps=31958, Training iteration=33"
2019-10-02 11:29:09.920,"Training> Name=main_level/agent, Worker=0, Episode=678, Total reward=16.7, Steps=31913, Training iteration=33"
2019-10-02 11:29:06.919,"Training> Name=main_level/agent, Worker=0, Episode=677, Total reward=51.95, Steps=31877, Training iteration=33"
2019-10-02 11:28:59.917,"Training> Name=main_level/agent, Worker=0, Episode=676, Total reward=22.65, Steps=31772, Training iteration=33"
2019-10-02 11:28:55.915,"Training> Name=main_level/agent, Worker=0, Episode=675, Total reward=14.34, Steps=31720, Training iteration=33"
2019-10-02 11:28:52.914,"Training> Name=main_level/agent, Worker=0, Episode=674, Total reward=17.21, Steps=31685, Training iteration=33"
2019-10-02 11:28:50.914,"Training> Name=main_level/agent, Worker=0, Episode=673, Total reward=16.59, Steps=31655, Training iteration=33"
2019-10-02 11:28:46.912,"Training> Name=main_level/agent, Worker=0, Episode=672, Total reward=10.62, Steps=31610, Training iteration=33"
2019-10-02 11:28:44.912,"Training> Name=main_level/agent, Worker=0, Episode=671, Total reward=30.73, Steps=31580, Training iteration=33"
2019-10-02 11:28:39.910,"Training> Name=main_level/agent, Worker=0, Episode=670, Total reward=42.67, Steps=31512, Training iteration=33"
2019-10-02 11:28:32.908,"Training> Name=main_level/agent, Worker=0, Episode=669, Total reward=61.66, Steps=31416, Training iteration=33"
2019-10-02 11:28:22.905,"Training> Name=main_level/agent, Worker=0, Episode=668, Total reward=25.55, Steps=31284, Training iteration=33"
2019-10-02 11:28:17.904,"Training> Name=main_level/agent, Worker=0, Episode=667, Total reward=20.75, Steps=31216, Training iteration=33"
2019-10-02 11:28:14.903,"Training> Name=main_level/agent, Worker=0, Episode=666, Total reward=18.08, Steps=31174, Training iteration=33"
2019-10-02 11:28:11.902,"Training> Name=main_level/agent, Worker=0, Episode=665, Total reward=16.37, Steps=31133, Training iteration=33"
2019-10-02 11:28:07.901,"Training> Name=main_level/agent, Worker=0, Episode=664, Total reward=25.01, Steps=31090, Training iteration=33"
2019-10-02 11:28:02.899,"Training> Name=main_level/agent, Worker=0, Episode=663, Total reward=20.65, Steps=31029, Training iteration=33"
2019-10-02 11:27:58.898,"Training> Name=main_level/agent, Worker=0, Episode=662, Total reward=29.15, Steps=30975, Training iteration=33"
2019-10-02 11:27:53.896,"Training> Name=main_level/agent, Worker=0, Episode=661, Total reward=20.11, Steps=30909, Training iteration=33"
2019-10-02 11:27:48.895,Uploaded 3 files for checkpoint 33
2019-10-02 11:27:48.895,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:27:48.895,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:27:48.895,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_33.pb
2019-10-02 11:27:44.893,Checkpoint> Saving in path=['./checkpoint/33_Step-30868.ckpt']
2019-10-02 11:27:43.893,"Policy training> Surrogate loss=-0.12161196768283844, KL divergence=0.039854541420936584, Entropy=1.743894338607788, training epoch=9, learning_rate=3e-05"
2019-10-02 11:27:38.798,"Policy training> Surrogate loss=-0.11983022093772888, KL divergence=0.04165477305650711, Entropy=1.7503740787506104, training epoch=8, learning_rate=3e-05"
2019-10-02 11:27:33.120,"Policy training> Surrogate loss=-0.12064214795827866, KL divergence=0.03640473634004593, Entropy=1.746222972869873, training epoch=7, learning_rate=3e-05"
2019-10-02 11:27:27.068,"Policy training> Surrogate loss=-0.11441098153591156, KL divergence=0.03612589091062546, Entropy=1.7590928077697754, training epoch=6, learning_rate=3e-05"
2019-10-02 11:27:19.998,"Policy training> Surrogate loss=-0.111152783036232, KL divergence=0.03790206462144852, Entropy=1.7528034448623657, training epoch=5, learning_rate=3e-05"
2019-10-02 11:27:13.930,"Policy training> Surrogate loss=-0.1000487431883812, KL divergence=0.03456680849194527, Entropy=1.75465726852417, training epoch=4, learning_rate=3e-05"
2019-10-02 11:27:08.808,"Policy training> Surrogate loss=-0.08274947851896286, KL divergence=0.03261067345738411, Entropy=1.7669274806976318, training epoch=3, learning_rate=3e-05"
2019-10-02 11:27:01.642,"Policy training> Surrogate loss=-0.07650565356016159, KL divergence=0.034370407462120056, Entropy=1.791654109954834, training epoch=2, learning_rate=3e-05"
2019-10-02 11:26:55.813,"Policy training> Surrogate loss=-0.045106206089258194, KL divergence=0.03235326334834099, Entropy=1.7528611421585083, training epoch=1, learning_rate=3e-05"
2019-10-02 11:26:50.750,"Policy training> Surrogate loss=0.007851053029298782, KL divergence=0.012625487521290779, Entropy=1.8391015529632568, training epoch=0, learning_rate=3e-05"
2019-10-02 11:26:43.386,"Training> Name=main_level/agent, Worker=0, Episode=660, Total reward=32.34, Steps=30868, Training iteration=32"
2019-10-02 11:26:37.384,"Training> Name=main_level/agent, Worker=0, Episode=659, Total reward=19.77, Steps=30795, Training iteration=32"
2019-10-02 11:26:34.383,"Training> Name=main_level/agent, Worker=0, Episode=658, Total reward=17.24, Steps=30750, Training iteration=32"
2019-10-02 11:26:31.382,"Training> Name=main_level/agent, Worker=0, Episode=657, Total reward=20.75, Steps=30723, Training iteration=32"
2019-10-02 11:26:28.381,"Training> Name=main_level/agent, Worker=0, Episode=656, Total reward=29.35, Steps=30683, Training iteration=32"
2019-10-02 11:26:24.380,"Training> Name=main_level/agent, Worker=0, Episode=655, Total reward=13.7, Steps=30624, Training iteration=32"
2019-10-02 11:26:21.379,"Training> Name=main_level/agent, Worker=0, Episode=654, Total reward=47.79, Steps=30596, Training iteration=32"
2019-10-02 11:26:14.377,"Training> Name=main_level/agent, Worker=0, Episode=653, Total reward=73.35, Steps=30496, Training iteration=32"
2019-10-02 11:26:04.374,"Training> Name=main_level/agent, Worker=0, Episode=652, Total reward=13.49, Steps=30354, Training iteration=32"
2019-10-02 11:26:00.373,"Training> Name=main_level/agent, Worker=0, Episode=651, Total reward=29.44, Steps=30303, Training iteration=32"
2019-10-02 11:25:55.371,"Training> Name=main_level/agent, Worker=0, Episode=650, Total reward=23.77, Steps=30229, Training iteration=32"
2019-10-02 11:25:52.370,"Training> Name=main_level/agent, Worker=0, Episode=649, Total reward=24.0, Steps=30187, Training iteration=32"
2019-10-02 11:25:47.369,"Training> Name=main_level/agent, Worker=0, Episode=648, Total reward=16.7, Steps=30128, Training iteration=32"
2019-10-02 11:25:44.368,"Training> Name=main_level/agent, Worker=0, Episode=647, Total reward=22.73, Steps=30088, Training iteration=32"
2019-10-02 11:25:40.367,"Training> Name=main_level/agent, Worker=0, Episode=646, Total reward=9.13, Steps=30035, Training iteration=32"
2019-10-02 11:25:38.366,"Training> Name=main_level/agent, Worker=0, Episode=645, Total reward=38.55, Steps=30006, Training iteration=32"
2019-10-02 11:25:31.364,"Training> Name=main_level/agent, Worker=0, Episode=644, Total reward=16.69, Steps=29920, Training iteration=32"
2019-10-02 11:25:28.363,"Training> Name=main_level/agent, Worker=0, Episode=643, Total reward=15.32, Steps=29875, Training iteration=32"
2019-10-02 11:25:25.362,"Training> Name=main_level/agent, Worker=0, Episode=642, Total reward=35.44, Steps=29842, Training iteration=32"
2019-10-02 11:25:20.360,"Training> Name=main_level/agent, Worker=0, Episode=641, Total reward=31.91, Steps=29771, Training iteration=32"
2019-10-02 11:25:13.358,Uploaded 3 files for checkpoint 32
2019-10-02 11:25:13.358,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:25:13.358,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:25:13.358,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_32.pb
2019-10-02 11:25:11.358,Checkpoint> Saving in path=['./checkpoint/32_Step-29702.ckpt']
2019-10-02 11:25:10.357,"Policy training> Surrogate loss=-0.10894462466239929, KL divergence=0.036750178784132004, Entropy=1.8024009466171265, training epoch=9, learning_rate=3e-05"
2019-10-02 11:25:01.598,"Policy training> Surrogate loss=-0.10681567341089249, KL divergence=0.036049507558345795, Entropy=1.8156890869140625, training epoch=8, learning_rate=3e-05"
2019-10-02 11:24:53.075,"Policy training> Surrogate loss=-0.09727856516838074, KL divergence=0.033397167921066284, Entropy=1.8089003562927246, training epoch=7, learning_rate=3e-05"
2019-10-02 11:24:44.254,"Policy training> Surrogate loss=-0.09338564425706863, KL divergence=0.031107963994145393, Entropy=1.8149179220199585, training epoch=6, learning_rate=3e-05"
2019-10-02 11:24:34.738,"Policy training> Surrogate loss=-0.09177868813276291, KL divergence=0.031095638871192932, Entropy=1.8225842714309692, training epoch=5, learning_rate=3e-05"
2019-10-02 11:24:26.507,"Policy training> Surrogate loss=-0.08579577505588531, KL divergence=0.027920516207814217, Entropy=1.8160905838012695, training epoch=4, learning_rate=3e-05"
2019-10-02 11:24:17.362,"Policy training> Surrogate loss=-0.08149566501379013, KL divergence=0.028325114399194717, Entropy=1.8189164400100708, training epoch=3, learning_rate=3e-05"
2019-10-02 11:24:08.062,"Policy training> Surrogate loss=-0.06668876856565475, KL divergence=0.02959803119301796, Entropy=1.8294442892074585, training epoch=2, learning_rate=3e-05"
2019-10-02 11:23:59.803,"Policy training> Surrogate loss=-0.03445135056972504, KL divergence=0.023093000054359436, Entropy=1.8199259042739868, training epoch=1, learning_rate=3e-05"
2019-10-02 11:23:50.422,"Policy training> Surrogate loss=0.008927317336201668, KL divergence=0.015093549154698849, Entropy=1.8385518789291382, training epoch=0, learning_rate=3e-05"
2019-10-02 11:23:40.097,"Training> Name=main_level/agent, Worker=0, Episode=640, Total reward=13.94, Steps=29702, Training iteration=31"
2019-10-02 11:23:38.097,"Training> Name=main_level/agent, Worker=0, Episode=639, Total reward=17.99, Steps=29677, Training iteration=31"
2019-10-02 11:23:35.096,"Training> Name=main_level/agent, Worker=0, Episode=638, Total reward=84.44, Steps=29634, Training iteration=31"
2019-10-02 11:23:22.092,"Training> Name=main_level/agent, Worker=0, Episode=637, Total reward=32.14, Steps=29456, Training iteration=31"
2019-10-02 11:23:18.091,"Training> Name=main_level/agent, Worker=0, Episode=636, Total reward=40.92, Steps=29392, Training iteration=31"
2019-10-02 11:23:12.089,"Training> Name=main_level/agent, Worker=0, Episode=635, Total reward=31.26, Steps=29310, Training iteration=31"
2019-10-02 11:23:06.087,"Training> Name=main_level/agent, Worker=0, Episode=634, Total reward=33.96, Steps=29235, Training iteration=31"
2019-10-02 11:23:01.085,"Training> Name=main_level/agent, Worker=0, Episode=633, Total reward=87.6, Steps=29161, Training iteration=31"
2019-10-02 11:22:47.081,"Training> Name=main_level/agent, Worker=0, Episode=632, Total reward=7.86, Steps=28968, Training iteration=31"
2019-10-02 11:22:45.081,"Training> Name=main_level/agent, Worker=0, Episode=631, Total reward=22.43, Steps=28942, Training iteration=31"
2019-10-02 11:22:41.079,"Training> Name=main_level/agent, Worker=0, Episode=630, Total reward=11.84, Steps=28893, Training iteration=31"
2019-10-02 11:22:39.079,"Training> Name=main_level/agent, Worker=0, Episode=629, Total reward=10.57, Steps=28870, Training iteration=31"
2019-10-02 11:22:37.078,"Training> Name=main_level/agent, Worker=0, Episode=628, Total reward=50.82, Steps=28839, Training iteration=31"
2019-10-02 11:22:29.076,"Training> Name=main_level/agent, Worker=0, Episode=627, Total reward=45.46, Steps=28722, Training iteration=31"
2019-10-02 11:22:20.073,"Training> Name=main_level/agent, Worker=0, Episode=626, Total reward=25.7, Steps=28605, Training iteration=31"
2019-10-02 11:22:15.072,"Training> Name=main_level/agent, Worker=0, Episode=625, Total reward=12.88, Steps=28539, Training iteration=31"
2019-10-02 11:22:12.071,"Training> Name=main_level/agent, Worker=0, Episode=624, Total reward=74.78, Steps=28496, Training iteration=31"
2019-10-02 11:22:00.067,"Training> Name=main_level/agent, Worker=0, Episode=623, Total reward=125.79, Steps=28324, Training iteration=31"
2019-10-02 11:21:46.064,"Training> Name=main_level/agent, Worker=0, Episode=622, Total reward=18.25, Steps=28127, Training iteration=31"
2019-10-02 11:21:43.063,"Training> Name=main_level/agent, Worker=0, Episode=621, Total reward=71.51, Steps=28090, Training iteration=31"
2019-10-02 11:21:30.059,Uploaded 3 files for checkpoint 31
2019-10-02 11:21:30.059,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:21:30.059,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:21:30.059,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_31.pb
2019-10-02 11:21:28.058,Checkpoint> Saving in path=['./checkpoint/31_Step-27921.ckpt']
2019-10-02 11:21:27.058,"Policy training> Surrogate loss=-0.12713095545768738, KL divergence=0.03272349759936333, Entropy=1.7910842895507812, training epoch=9, learning_rate=3e-05"
2019-10-02 11:21:21.344,"Policy training> Surrogate loss=-0.11856517940759659, KL divergence=0.031303130090236664, Entropy=1.8008419275283813, training epoch=8, learning_rate=3e-05"
2019-10-02 11:21:15.019,"Policy training> Surrogate loss=-0.11723463237285614, KL divergence=0.029865242540836334, Entropy=1.795989990234375, training epoch=7, learning_rate=3e-05"
2019-10-02 11:21:08.859,"Policy training> Surrogate loss=-0.10747212916612625, KL divergence=0.028677796944975853, Entropy=1.8035264015197754, training epoch=6, learning_rate=3e-05"
2019-10-02 11:21:02.564,"Policy training> Surrogate loss=-0.10298158973455429, KL divergence=0.028632430359721184, Entropy=1.7969177961349487, training epoch=5, learning_rate=3e-05"
2019-10-02 11:20:57.557,"Policy training> Surrogate loss=-0.09534495323896408, KL divergence=0.027069825679063797, Entropy=1.8003484010696411, training epoch=4, learning_rate=3e-05"
2019-10-02 11:20:51.282,"Policy training> Surrogate loss=-0.08280636370182037, KL divergence=0.028614157810807228, Entropy=1.8194899559020996, training epoch=3, learning_rate=3e-05"
2019-10-02 11:20:44.601,"Policy training> Surrogate loss=-0.06798628717660904, KL divergence=0.026530517265200615, Entropy=1.8069745302200317, training epoch=2, learning_rate=3e-05"
2019-10-02 11:20:38.515,"Policy training> Surrogate loss=-0.03995848819613457, KL divergence=0.02543705515563488, Entropy=1.7990329265594482, training epoch=1, learning_rate=3e-05"
2019-10-02 11:20:31.962,"Policy training> Surrogate loss=0.015807924792170525, KL divergence=0.012218169867992401, Entropy=1.7567516565322876, training epoch=0, learning_rate=3e-05"
2019-10-02 11:20:24.629,"Training> Name=main_level/agent, Worker=0, Episode=620, Total reward=25.16, Steps=27921, Training iteration=30"
2019-10-02 11:20:20.446,"Training> Name=main_level/agent, Worker=0, Episode=619, Total reward=9.0, Steps=27871, Training iteration=30"
2019-10-02 11:20:18.446,"Training> Name=main_level/agent, Worker=0, Episode=618, Total reward=23.29, Steps=27847, Training iteration=30"
2019-10-02 11:20:14.444,"Training> Name=main_level/agent, Worker=0, Episode=617, Total reward=29.13, Steps=27802, Training iteration=30"
2019-10-02 11:20:11.443,"Training> Name=main_level/agent, Worker=0, Episode=616, Total reward=37.74, Steps=27754, Training iteration=30"
2019-10-02 11:20:05.442,"Training> Name=main_level/agent, Worker=0, Episode=615, Total reward=35.16, Steps=27673, Training iteration=30"
2019-10-02 11:19:59.440,"Training> Name=main_level/agent, Worker=0, Episode=614, Total reward=24.32, Steps=27595, Training iteration=30"
2019-10-02 11:19:55.439,"Training> Name=main_level/agent, Worker=0, Episode=613, Total reward=33.32, Steps=27548, Training iteration=30"
2019-10-02 11:19:50.437,"Training> Name=main_level/agent, Worker=0, Episode=612, Total reward=31.51, Steps=27466, Training iteration=30"
2019-10-02 11:19:44.435,"Training> Name=main_level/agent, Worker=0, Episode=611, Total reward=38.61, Steps=27391, Training iteration=30"
2019-10-02 11:19:38.434,"Training> Name=main_level/agent, Worker=0, Episode=610, Total reward=13.27, Steps=27314, Training iteration=30"
2019-10-02 11:19:36.433,"Training> Name=main_level/agent, Worker=0, Episode=609, Total reward=58.23, Steps=27293, Training iteration=30"
2019-10-02 11:19:27.430,"Training> Name=main_level/agent, Worker=0, Episode=608, Total reward=9.84, Steps=27156, Training iteration=30"
2019-10-02 11:19:25.429,"Training> Name=main_level/agent, Worker=0, Episode=607, Total reward=12.01, Steps=27136, Training iteration=30"
2019-10-02 11:19:22.428,"Training> Name=main_level/agent, Worker=0, Episode=606, Total reward=18.86, Steps=27106, Training iteration=30"
2019-10-02 11:19:19.427,"Training> Name=main_level/agent, Worker=0, Episode=605, Total reward=16.07, Steps=27056, Training iteration=30"
2019-10-02 11:19:15.426,"Training> Name=main_level/agent, Worker=0, Episode=604, Total reward=26.83, Steps=27012, Training iteration=30"
2019-10-02 11:19:10.425,"Training> Name=main_level/agent, Worker=0, Episode=603, Total reward=71.09, Steps=26940, Training iteration=30"
2019-10-02 11:18:57.421,"Training> Name=main_level/agent, Worker=0, Episode=602, Total reward=10.68, Steps=26763, Training iteration=30"
2019-10-02 11:18:56.421,"Training> Name=main_level/agent, Worker=0, Episode=601, Total reward=25.63, Steps=26745, Training iteration=30"
2019-10-02 11:18:51.419,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_30.pb
2019-10-02 11:18:50.419,Uploaded 3 files for checkpoint 30
2019-10-02 11:18:50.419,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:18:50.419,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:18:48.418,"Policy training> Surrogate loss=-0.10369101911783218, KL divergence=0.03843707591295242, Entropy=1.7585220336914062, training epoch=9, learning_rate=3e-05"
2019-10-02 11:18:48.418,Checkpoint> Saving in path=['./checkpoint/30_Step-26696.ckpt']
2019-10-02 11:18:41.328,"Policy training> Surrogate loss=-0.1105751097202301, KL divergence=0.03965288773179054, Entropy=1.754845380783081, training epoch=8, learning_rate=3e-05"
2019-10-02 11:18:34.543,"Policy training> Surrogate loss=-0.10522952675819397, KL divergence=0.03837542235851288, Entropy=1.7713141441345215, training epoch=7, learning_rate=3e-05"
2019-10-02 11:18:28.533,"Policy training> Surrogate loss=-0.09777404367923737, KL divergence=0.03489764407277107, Entropy=1.7856690883636475, training epoch=6, learning_rate=3e-05"
2019-10-02 11:18:21.432,"Policy training> Surrogate loss=-0.09587066620588303, KL divergence=0.03337768465280533, Entropy=1.7847529649734497, training epoch=5, learning_rate=3e-05"
2019-10-02 11:18:15.236,"Policy training> Surrogate loss=-0.09528892487287521, KL divergence=0.033713199198246, Entropy=1.7835131883621216, training epoch=4, learning_rate=3e-05"
2019-10-02 11:18:08.566,"Policy training> Surrogate loss=-0.08098864555358887, KL divergence=0.032211363315582275, Entropy=1.7927347421646118, training epoch=3, learning_rate=3e-05"
2019-10-02 11:18:01.957,"Policy training> Surrogate loss=-0.06492587178945541, KL divergence=0.032622747123241425, Entropy=1.7937445640563965, training epoch=2, learning_rate=3e-05"
2019-10-02 11:17:54.474,"Policy training> Surrogate loss=-0.04979703575372696, KL divergence=0.028872918337583542, Entropy=1.8216722011566162, training epoch=1, learning_rate=3e-05"
2019-10-02 11:17:47.450,"Policy training> Surrogate loss=0.004501931834965944, KL divergence=0.01899131014943123, Entropy=1.7934321165084839, training epoch=0, learning_rate=3e-05"
2019-10-02 11:17:38.962,"Training> Name=main_level/agent, Worker=0, Episode=600, Total reward=40.42, Steps=26696, Training iteration=29"
2019-10-02 11:17:32.961,"Training> Name=main_level/agent, Worker=0, Episode=599, Total reward=17.94, Steps=26605, Training iteration=29"
2019-10-02 11:17:28.959,"Training> Name=main_level/agent, Worker=0, Episode=598, Total reward=23.79, Steps=26559, Training iteration=29"
2019-10-02 11:17:25.958,"Training> Name=main_level/agent, Worker=0, Episode=597, Total reward=15.2, Steps=26514, Training iteration=29"
2019-10-02 11:17:22.957,"Training> Name=main_level/agent, Worker=0, Episode=596, Total reward=46.28, Steps=26481, Training iteration=29"
2019-10-02 11:17:15.955,"Training> Name=main_level/agent, Worker=0, Episode=595, Total reward=39.37, Steps=26388, Training iteration=29"
2019-10-02 11:17:09.953,"Training> Name=main_level/agent, Worker=0, Episode=594, Total reward=18.69, Steps=26307, Training iteration=29"
2019-10-02 11:17:07.953,"Training> Name=main_level/agent, Worker=0, Episode=593, Total reward=36.48, Steps=26273, Training iteration=29"
2019-10-02 11:17:01.951,"Training> Name=main_level/agent, Worker=0, Episode=592, Total reward=16.2, Steps=26193, Training iteration=29"
2019-10-02 11:16:57.949,"Training> Name=main_level/agent, Worker=0, Episode=591, Total reward=39.21, Steps=26136, Training iteration=29"
2019-10-02 11:16:50.947,"Training> Name=main_level/agent, Worker=0, Episode=590, Total reward=15.07, Steps=26048, Training iteration=29"
2019-10-02 11:16:46.946,"Training> Name=main_level/agent, Worker=0, Episode=589, Total reward=14.22, Steps=26014, Training iteration=29"
2019-10-02 11:16:43.945,"Training> Name=main_level/agent, Worker=0, Episode=588, Total reward=29.46, Steps=25979, Training iteration=29"
2019-10-02 11:16:37.943,"Training> Name=main_level/agent, Worker=0, Episode=587, Total reward=16.27, Steps=25906, Training iteration=29"
2019-10-02 11:16:34.942,"Training> Name=main_level/agent, Worker=0, Episode=586, Total reward=85.56, Steps=25863, Training iteration=29"
2019-10-02 11:16:21.939,"Training> Name=main_level/agent, Worker=0, Episode=585, Total reward=42.85, Steps=25680, Training iteration=29"
2019-10-02 11:16:13.936,"Training> Name=main_level/agent, Worker=0, Episode=584, Total reward=46.82, Steps=25556, Training iteration=29"
2019-10-02 11:16:06.934,"Training> Name=main_level/agent, Worker=0, Episode=583, Total reward=44.45, Steps=25469, Training iteration=29"
2019-10-02 11:16:00.932,"Training> Name=main_level/agent, Worker=0, Episode=582, Total reward=16.92, Steps=25379, Training iteration=29"
2019-10-02 11:15:57.931,"Training> Name=main_level/agent, Worker=0, Episode=581, Total reward=25.42, Steps=25339, Training iteration=29"
2019-10-02 11:15:52.930,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_29.pb
2019-10-02 11:15:51.929,Uploaded 3 files for checkpoint 29
2019-10-02 11:15:51.929,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:15:51.929,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:15:49.929,Checkpoint> Saving in path=['./checkpoint/29_Step-25297.ckpt']
2019-10-02 11:15:49.928,"Policy training> Surrogate loss=-0.11726345866918564, KL divergence=0.04056357592344284, Entropy=1.832754373550415, training epoch=9, learning_rate=3e-05"
2019-10-02 11:15:42.744,"Policy training> Surrogate loss=-0.11418590694665909, KL divergence=0.03790072351694107, Entropy=1.8160285949707031, training epoch=8, learning_rate=3e-05"
2019-10-02 11:15:36.215,"Policy training> Surrogate loss=-0.11261656135320663, KL divergence=0.0373971089720726, Entropy=1.8268826007843018, training epoch=7, learning_rate=3e-05"
2019-10-02 11:15:30.163,"Policy training> Surrogate loss=-0.10602220147848129, KL divergence=0.035534266382455826, Entropy=1.8218551874160767, training epoch=6, learning_rate=3e-05"
2019-10-02 11:15:24.419,"Policy training> Surrogate loss=-0.10241879522800446, KL divergence=0.0318082794547081, Entropy=1.8305518627166748, training epoch=5, learning_rate=3e-05"
2019-10-02 11:15:16.338,"Policy training> Surrogate loss=-0.09215761721134186, KL divergence=0.027268311008810997, Entropy=1.8516013622283936, training epoch=4, learning_rate=3e-05"
2019-10-02 11:15:09.758,"Policy training> Surrogate loss=-0.09088713675737381, KL divergence=0.030707472935318947, Entropy=1.8198503255844116, training epoch=3, learning_rate=3e-05"
2019-10-02 11:15:04.722,"Policy training> Surrogate loss=-0.0786610096693039, KL divergence=0.02826691046357155, Entropy=1.8364198207855225, training epoch=2, learning_rate=3e-05"
2019-10-02 11:14:57.758,"Policy training> Surrogate loss=-0.043090514838695526, KL divergence=0.02440767176449299, Entropy=1.852651596069336, training epoch=1, learning_rate=3e-05"
2019-10-02 11:14:50.998,"Policy training> Surrogate loss=0.00840293150395155, KL divergence=0.012571357190608978, Entropy=1.8406751155853271, training epoch=0, learning_rate=3e-05"
2019-10-02 11:14:42.796,"Training> Name=main_level/agent, Worker=0, Episode=580, Total reward=24.19, Steps=25297, Training iteration=28"
2019-10-02 11:14:38.781,"Training> Name=main_level/agent, Worker=0, Episode=579, Total reward=6.93, Steps=25247, Training iteration=28"
2019-10-02 11:14:36.780,"Training> Name=main_level/agent, Worker=0, Episode=578, Total reward=11.13, Steps=25225, Training iteration=28"
2019-10-02 11:14:35.779,"Training> Name=main_level/agent, Worker=0, Episode=577, Total reward=15.51, Steps=25206, Training iteration=28"
2019-10-02 11:14:32.778,"Training> Name=main_level/agent, Worker=0, Episode=576, Total reward=42.88, Steps=25175, Training iteration=28"
2019-10-02 11:14:26.777,"Training> Name=main_level/agent, Worker=0, Episode=575, Total reward=6.31, Steps=25083, Training iteration=28"
2019-10-02 11:14:23.776,"Training> Name=main_level/agent, Worker=0, Episode=574, Total reward=40.3, Steps=25065, Training iteration=28"
2019-10-02 11:14:17.774,"Training> Name=main_level/agent, Worker=0, Episode=573, Total reward=52.96, Steps=24980, Training iteration=28"
2019-10-02 11:14:09.772,"Training> Name=main_level/agent, Worker=0, Episode=572, Total reward=14.91, Steps=24862, Training iteration=28"
2019-10-02 11:14:06.771,"Training> Name=main_level/agent, Worker=0, Episode=571, Total reward=40.72, Steps=24820, Training iteration=28"
2019-10-02 11:14:00.769,"Training> Name=main_level/agent, Worker=0, Episode=570, Total reward=80.34, Steps=24737, Training iteration=28"
2019-10-02 11:13:48.765,"Training> Name=main_level/agent, Worker=0, Episode=569, Total reward=21.03, Steps=24576, Training iteration=28"
2019-10-02 11:13:45.764,"Training> Name=main_level/agent, Worker=0, Episode=568, Total reward=17.69, Steps=24527, Training iteration=28"
2019-10-02 11:13:41.763,"Training> Name=main_level/agent, Worker=0, Episode=567, Total reward=24.5, Steps=24477, Training iteration=28"
2019-10-02 11:13:37.762,"Training> Name=main_level/agent, Worker=0, Episode=566, Total reward=53.4, Steps=24423, Training iteration=28"
2019-10-02 11:13:27.759,"Training> Name=main_level/agent, Worker=0, Episode=565, Total reward=36.72, Steps=24281, Training iteration=28"
2019-10-02 11:13:19.757,"Training> Name=main_level/agent, Worker=0, Episode=564, Total reward=32.13, Steps=24176, Training iteration=28"
2019-10-02 11:13:14.755,"Training> Name=main_level/agent, Worker=0, Episode=563, Total reward=25.2, Steps=24112, Training iteration=28"
2019-10-02 11:13:10.754,"Training> Name=main_level/agent, Worker=0, Episode=562, Total reward=19.9, Steps=24058, Training iteration=28"
2019-10-02 11:13:08.753,"Training> Name=main_level/agent, Worker=0, Episode=561, Total reward=24.48, Steps=24024, Training iteration=28"
2019-10-02 11:13:02.751,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:13:02.751,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_28.pb
2019-10-02 11:13:01.751,Uploaded 3 files for checkpoint 28
2019-10-02 11:13:01.751,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:12:59.750,Checkpoint> Saving in path=['./checkpoint/28_Step-23974.ckpt']
2019-10-02 11:12:58.750,"Policy training> Surrogate loss=-0.13620512187480927, KL divergence=0.03283717855811119, Entropy=1.9138574600219727, training epoch=9, learning_rate=3e-05"
2019-10-02 11:12:53.178,"Policy training> Surrogate loss=-0.12550359964370728, KL divergence=0.033296920359134674, Entropy=1.9109100103378296, training epoch=8, learning_rate=3e-05"
2019-10-02 11:12:48.161,"Policy training> Surrogate loss=-0.1239432618021965, KL divergence=0.030541853979229927, Entropy=1.916102409362793, training epoch=7, learning_rate=3e-05"
2019-10-02 11:12:42.880,"Policy training> Surrogate loss=-0.110631063580513, KL divergence=0.03030358999967575, Entropy=1.9203585386276245, training epoch=6, learning_rate=3e-05"
2019-10-02 11:12:36.773,"Policy training> Surrogate loss=-0.10854989290237427, KL divergence=0.028621161356568336, Entropy=1.9349614381790161, training epoch=5, learning_rate=3e-05"
2019-10-02 11:12:31.609,"Policy training> Surrogate loss=-0.09948176145553589, KL divergence=0.029269639402627945, Entropy=1.9227304458618164, training epoch=4, learning_rate=3e-05"
2019-10-02 11:12:26.572,"Policy training> Surrogate loss=-0.08587536960840225, KL divergence=0.028074074536561966, Entropy=1.9270431995391846, training epoch=3, learning_rate=3e-05"
2019-10-02 11:12:19.750,"Policy training> Surrogate loss=-0.07589109987020493, KL divergence=0.03046875074505806, Entropy=1.9197545051574707, training epoch=2, learning_rate=3e-05"
2019-10-02 11:12:14.960,"Policy training> Surrogate loss=-0.046748485416173935, KL divergence=0.03402518853545189, Entropy=1.9074608087539673, training epoch=1, learning_rate=3e-05"
2019-10-02 11:12:10.354,"Policy training> Surrogate loss=0.003867983352392912, KL divergence=0.015525110997259617, Entropy=1.9000511169433594, training epoch=0, learning_rate=3e-05"
2019-10-02 11:12:02.608,"Training> Name=main_level/agent, Worker=0, Episode=560, Total reward=28.77, Steps=23974, Training iteration=27"
2019-10-02 11:11:58.606,"Training> Name=main_level/agent, Worker=0, Episode=559, Total reward=17.1, Steps=23923, Training iteration=27"
2019-10-02 11:11:55.605,"Training> Name=main_level/agent, Worker=0, Episode=558, Total reward=50.34, Steps=23884, Training iteration=27"
2019-10-02 11:11:48.603,"Training> Name=main_level/agent, Worker=0, Episode=557, Total reward=9.88, Steps=23786, Training iteration=27"
2019-10-02 11:11:46.602,"Training> Name=main_level/agent, Worker=0, Episode=556, Total reward=20.06, Steps=23766, Training iteration=27"
2019-10-02 11:11:43.601,"Training> Name=main_level/agent, Worker=0, Episode=555, Total reward=40.24, Steps=23724, Training iteration=27"
2019-10-02 11:11:37.599,"Training> Name=main_level/agent, Worker=0, Episode=554, Total reward=43.54, Steps=23642, Training iteration=27"
2019-10-02 11:11:30.597,"Training> Name=main_level/agent, Worker=0, Episode=553, Total reward=24.52, Steps=23545, Training iteration=27"
2019-10-02 11:11:25.596,"Training> Name=main_level/agent, Worker=0, Episode=552, Total reward=10.41, Steps=23482, Training iteration=27"
2019-10-02 11:11:23.595,"Training> Name=main_level/agent, Worker=0, Episode=551, Total reward=24.33, Steps=23459, Training iteration=27"
2019-10-02 11:11:18.594,"Training> Name=main_level/agent, Worker=0, Episode=550, Total reward=35.91, Steps=23398, Training iteration=27"
2019-10-02 11:11:13.592,"Training> Name=main_level/agent, Worker=0, Episode=549, Total reward=14.45, Steps=23324, Training iteration=27"
2019-10-02 11:11:10.591,"Training> Name=main_level/agent, Worker=0, Episode=548, Total reward=25.13, Steps=23288, Training iteration=27"
2019-10-02 11:11:06.590,"Training> Name=main_level/agent, Worker=0, Episode=547, Total reward=6.43, Steps=23236, Training iteration=27"
2019-10-02 11:11:04.589,"Training> Name=main_level/agent, Worker=0, Episode=546, Total reward=34.01, Steps=23216, Training iteration=27"
2019-10-02 11:10:59.588,"Training> Name=main_level/agent, Worker=0, Episode=545, Total reward=17.39, Steps=23147, Training iteration=27"
2019-10-02 11:10:55.587,"Training> Name=main_level/agent, Worker=0, Episode=544, Total reward=40.98, Steps=23103, Training iteration=27"
2019-10-02 11:10:49.585,"Training> Name=main_level/agent, Worker=0, Episode=543, Total reward=22.81, Steps=23011, Training iteration=27"
2019-10-02 11:10:46.584,"Training> Name=main_level/agent, Worker=0, Episode=542, Total reward=35.08, Steps=22976, Training iteration=27"
2019-10-02 11:10:40.582,"Training> Name=main_level/agent, Worker=0, Episode=541, Total reward=23.09, Steps=22899, Training iteration=27"
2019-10-02 11:10:35.581,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_27.pb
2019-10-02 11:10:34.580,Uploaded 3 files for checkpoint 27
2019-10-02 11:10:34.580,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:10:34.580,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:10:33.580,Checkpoint> Saving in path=['./checkpoint/27_Step-22853.ckpt']
2019-10-02 11:10:32.579,"Policy training> Surrogate loss=-0.11429411172866821, KL divergence=0.03178931400179863, Entropy=1.9478200674057007, training epoch=9, learning_rate=3e-05"
2019-10-02 11:10:26.054,"Policy training> Surrogate loss=-0.11390519142150879, KL divergence=0.03143630921840668, Entropy=1.9529590606689453, training epoch=8, learning_rate=3e-05"
2019-10-02 11:10:18.858,"Policy training> Surrogate loss=-0.10648437589406967, KL divergence=0.03337983042001724, Entropy=1.9535850286483765, training epoch=7, learning_rate=3e-05"
2019-10-02 11:10:12.837,"Policy training> Surrogate loss=-0.10307200998067856, KL divergence=0.029014524072408676, Entropy=1.9439561367034912, training epoch=6, learning_rate=3e-05"
2019-10-02 11:10:05.458,"Policy training> Surrogate loss=-0.09070192277431488, KL divergence=0.028549304232001305, Entropy=1.947333574295044, training epoch=5, learning_rate=3e-05"
2019-10-02 11:09:59.360,"Policy training> Surrogate loss=-0.08883072435855865, KL divergence=0.027307191863656044, Entropy=1.966535210609436, training epoch=4, learning_rate=3e-05"
2019-10-02 11:09:53.183,"Policy training> Surrogate loss=-0.07875939458608627, KL divergence=0.031229406595230103, Entropy=1.95403254032135, training epoch=3, learning_rate=3e-05"
2019-10-02 11:09:45.471,"Policy training> Surrogate loss=-0.06480149179697037, KL divergence=0.027586523443460464, Entropy=1.9655921459197998, training epoch=2, learning_rate=3e-05"
2019-10-02 11:09:39.422,"Policy training> Surrogate loss=-0.04822622239589691, KL divergence=0.02566266618669033, Entropy=1.965537667274475, training epoch=1, learning_rate=3e-05"
2019-10-02 11:09:33.323,"Policy training> Surrogate loss=0.012738430872559547, KL divergence=0.021035457029938698, Entropy=1.9539821147918701, training epoch=0, learning_rate=3e-05"
2019-10-02 11:09:24.445,"Training> Name=main_level/agent, Worker=0, Episode=540, Total reward=58.59, Steps=22853, Training iteration=26"
2019-10-02 11:09:16.443,"Training> Name=main_level/agent, Worker=0, Episode=539, Total reward=11.21, Steps=22746, Training iteration=26"
2019-10-02 11:09:14.442,"Training> Name=main_level/agent, Worker=0, Episode=538, Total reward=27.1, Steps=22721, Training iteration=26"
2019-10-02 11:09:10.441,"Training> Name=main_level/agent, Worker=0, Episode=537, Total reward=23.86, Steps=22666, Training iteration=26"
2019-10-02 11:09:06.440,"Training> Name=main_level/agent, Worker=0, Episode=536, Total reward=8.07, Steps=22621, Training iteration=26"
2019-10-02 11:09:05.440,"Training> Name=main_level/agent, Worker=0, Episode=535, Total reward=34.09, Steps=22600, Training iteration=26"
2019-10-02 11:08:59.438,"Training> Name=main_level/agent, Worker=0, Episode=534, Total reward=47.71, Steps=22530, Training iteration=26"
2019-10-02 11:08:52.436,"Training> Name=main_level/agent, Worker=0, Episode=533, Total reward=43.38, Steps=22426, Training iteration=26"
2019-10-02 11:08:46.434,"Training> Name=main_level/agent, Worker=0, Episode=532, Total reward=32.38, Steps=22337, Training iteration=26"
2019-10-02 11:08:41.432,"Training> Name=main_level/agent, Worker=0, Episode=531, Total reward=62.11, Steps=22268, Training iteration=26"
2019-10-02 11:08:31.429,"Training> Name=main_level/agent, Worker=0, Episode=530, Total reward=14.42, Steps=22133, Training iteration=26"
2019-10-02 11:08:29.429,"Training> Name=main_level/agent, Worker=0, Episode=529, Total reward=29.15, Steps=22108, Training iteration=26"
2019-10-02 11:08:24.427,"Training> Name=main_level/agent, Worker=0, Episode=528, Total reward=17.02, Steps=22051, Training iteration=26"
2019-10-02 11:08:21.426,"Training> Name=main_level/agent, Worker=0, Episode=527, Total reward=7.59, Steps=22009, Training iteration=26"
2019-10-02 11:08:19.425,"Training> Name=main_level/agent, Worker=0, Episode=526, Total reward=28.39, Steps=21987, Training iteration=26"
2019-10-02 11:08:14.424,"Training> Name=main_level/agent, Worker=0, Episode=525, Total reward=31.41, Steps=21913, Training iteration=26"
2019-10-02 11:08:08.422,"Training> Name=main_level/agent, Worker=0, Episode=524, Total reward=29.49, Steps=21844, Training iteration=26"
2019-10-02 11:08:03.421,"Training> Name=main_level/agent, Worker=0, Episode=523, Total reward=35.19, Steps=21773, Training iteration=26"
2019-10-02 11:07:57.419,"Training> Name=main_level/agent, Worker=0, Episode=522, Total reward=50.83, Steps=21691, Training iteration=26"
2019-10-02 11:07:49.416,"Training> Name=main_level/agent, Worker=0, Episode=521, Total reward=14.26, Steps=21576, Training iteration=26"
2019-10-02 11:07:46.415,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_26.pb
2019-10-02 11:07:45.415,Uploaded 3 files for checkpoint 26
2019-10-02 11:07:45.415,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:07:45.415,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:07:43.414,"Policy training> Surrogate loss=-0.11910894513130188, KL divergence=0.031263869255781174, Entropy=1.9504536390304565, training epoch=9, learning_rate=3e-05"
2019-10-02 11:07:43.414,Checkpoint> Saving in path=['./checkpoint/26_Step-21551.ckpt']
2019-10-02 11:07:37.286,"Policy training> Surrogate loss=-0.11724516749382019, KL divergence=0.02987636625766754, Entropy=1.9708366394042969, training epoch=8, learning_rate=3e-05"
2019-10-02 11:07:31.771,"Policy training> Surrogate loss=-0.11381210386753082, KL divergence=0.030595509335398674, Entropy=1.954986810684204, training epoch=7, learning_rate=3e-05"
2019-10-02 11:07:25.607,"Policy training> Surrogate loss=-0.1105811819434166, KL divergence=0.026647165417671204, Entropy=1.9608172178268433, training epoch=6, learning_rate=3e-05"
2019-10-02 11:07:20.221,"Policy training> Surrogate loss=-0.10802114754915237, KL divergence=0.02488396316766739, Entropy=1.984297752380371, training epoch=5, learning_rate=3e-05"
2019-10-02 11:07:14.898,"Policy training> Surrogate loss=-0.09633289277553558, KL divergence=0.022674888372421265, Entropy=1.9716860055923462, training epoch=4, learning_rate=3e-05"
2019-10-02 11:07:09.576,"Policy training> Surrogate loss=-0.08884981274604797, KL divergence=0.02144082821905613, Entropy=1.9831697940826416, training epoch=3, learning_rate=3e-05"
2019-10-02 11:07:04.177,"Policy training> Surrogate loss=-0.0690549910068512, KL divergence=0.018963798880577087, Entropy=1.9827051162719727, training epoch=2, learning_rate=3e-05"
2019-10-02 11:06:57.898,"Policy training> Surrogate loss=-0.038182441145181656, KL divergence=0.018225181847810745, Entropy=1.9687771797180176, training epoch=1, learning_rate=3e-05"
2019-10-02 11:06:52.678,"Policy training> Surrogate loss=0.007276648189872503, KL divergence=0.009559628553688526, Entropy=1.9723716974258423, training epoch=0, learning_rate=3e-05"
2019-10-02 11:06:45.638,"Training> Name=main_level/agent, Worker=0, Episode=520, Total reward=33.84, Steps=21551, Training iteration=25"
2019-10-02 11:06:40.637,"Training> Name=main_level/agent, Worker=0, Episode=519, Total reward=15.68, Steps=21480, Training iteration=25"
2019-10-02 11:06:37.636,"Training> Name=main_level/agent, Worker=0, Episode=518, Total reward=60.05, Steps=21445, Training iteration=25"
2019-10-02 11:06:28.633,"Training> Name=main_level/agent, Worker=0, Episode=517, Total reward=32.12, Steps=21313, Training iteration=25"
2019-10-02 11:06:23.631,"Training> Name=main_level/agent, Worker=0, Episode=516, Total reward=34.39, Steps=21244, Training iteration=25"
2019-10-02 11:06:17.630,"Training> Name=main_level/agent, Worker=0, Episode=515, Total reward=11.61, Steps=21167, Training iteration=25"
2019-10-02 11:06:14.629,"Training> Name=main_level/agent, Worker=0, Episode=514, Total reward=18.18, Steps=21135, Training iteration=25"
2019-10-02 11:06:11.628,"Training> Name=main_level/agent, Worker=0, Episode=513, Total reward=22.84, Steps=21095, Training iteration=25"
2019-10-02 11:06:07.627,"Training> Name=main_level/agent, Worker=0, Episode=512, Total reward=34.15, Steps=21037, Training iteration=25"
2019-10-02 11:06:01.625,"Training> Name=main_level/agent, Worker=0, Episode=511, Total reward=17.33, Steps=20961, Training iteration=25"
2019-10-02 11:05:59.624,"Training> Name=main_level/agent, Worker=0, Episode=510, Total reward=9.65, Steps=20928, Training iteration=25"
2019-10-02 11:05:57.623,"Training> Name=main_level/agent, Worker=0, Episode=509, Total reward=14.61, Steps=20908, Training iteration=25"
2019-10-02 11:05:54.622,"Training> Name=main_level/agent, Worker=0, Episode=508, Total reward=27.24, Steps=20872, Training iteration=25"
2019-10-02 11:05:50.621,"Training> Name=main_level/agent, Worker=0, Episode=507, Total reward=16.71, Steps=20820, Training iteration=25"
2019-10-02 11:05:47.620,"Training> Name=main_level/agent, Worker=0, Episode=506, Total reward=12.36, Steps=20780, Training iteration=25"
2019-10-02 11:05:44.619,"Training> Name=main_level/agent, Worker=0, Episode=505, Total reward=16.56, Steps=20744, Training iteration=25"
2019-10-02 11:05:40.618,"Training> Name=main_level/agent, Worker=0, Episode=504, Total reward=16.17, Steps=20701, Training iteration=25"
2019-10-02 11:05:37.617,"Training> Name=main_level/agent, Worker=0, Episode=503, Total reward=29.17, Steps=20667, Training iteration=25"
2019-10-02 11:05:34.616,"Training> Name=main_level/agent, Worker=0, Episode=502, Total reward=31.93, Steps=20620, Training iteration=25"
2019-10-02 11:05:29.614,"Training> Name=main_level/agent, Worker=0, Episode=501, Total reward=51.63, Steps=20557, Training iteration=25"
2019-10-02 11:05:20.612,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_25.pb
2019-10-02 11:05:19.611,Uploaded 3 files for checkpoint 25
2019-10-02 11:05:19.611,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:05:19.611,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:05:17.610,"Policy training> Surrogate loss=-0.11644966900348663, KL divergence=0.028262538835406303, Entropy=2.0082695484161377, training epoch=9, learning_rate=3e-05"
2019-10-02 11:05:17.610,Checkpoint> Saving in path=['./checkpoint/25_Step-20451.ckpt']
2019-10-02 11:05:13.545,"Policy training> Surrogate loss=-0.11580294370651245, KL divergence=0.02568228356540203, Entropy=2.0142409801483154, training epoch=8, learning_rate=3e-05"
2019-10-02 11:05:05.906,"Policy training> Surrogate loss=-0.10954584926366806, KL divergence=0.026646602898836136, Entropy=2.016247272491455, training epoch=7, learning_rate=3e-05"
2019-10-02 11:05:00.711,"Policy training> Surrogate loss=-0.10276710987091064, KL divergence=0.023924442008137703, Entropy=2.0076401233673096, training epoch=6, learning_rate=3e-05"
2019-10-02 11:04:55.488,"Policy training> Surrogate loss=-0.09886033833026886, KL divergence=0.02147875353693962, Entropy=2.020329236984253, training epoch=5, learning_rate=3e-05"
2019-10-02 11:04:49.617,"Policy training> Surrogate loss=-0.08636923134326935, KL divergence=0.021622218191623688, Entropy=2.0157582759857178, training epoch=4, learning_rate=3e-05"
2019-10-02 11:04:43.945,"Policy training> Surrogate loss=-0.08102287352085114, KL divergence=0.01982441544532776, Entropy=2.0275392532348633, training epoch=3, learning_rate=3e-05"
2019-10-02 11:04:38.756,"Policy training> Surrogate loss=-0.06020446494221687, KL divergence=0.01772451028227806, Entropy=2.0364959239959717, training epoch=2, learning_rate=3e-05"
2019-10-02 11:04:32.612,"Policy training> Surrogate loss=-0.038147665560245514, KL divergence=0.019098713994026184, Entropy=2.0304577350616455, training epoch=1, learning_rate=3e-05"
2019-10-02 11:04:27.531,"Policy training> Surrogate loss=0.012400081381201744, KL divergence=0.011457269079983234, Entropy=2.0443296432495117, training epoch=0, learning_rate=3e-05"
2019-10-02 11:04:21.479,"Training> Name=main_level/agent, Worker=0, Episode=500, Total reward=44.87, Steps=20451, Training iteration=24"
2019-10-02 11:04:14.476,"Training> Name=main_level/agent, Worker=0, Episode=499, Total reward=9.52, Steps=20361, Training iteration=24"
2019-10-02 11:04:12.476,"Training> Name=main_level/agent, Worker=0, Episode=498, Total reward=12.68, Steps=20336, Training iteration=24"
2019-10-02 11:04:10.475,"Training> Name=main_level/agent, Worker=0, Episode=497, Total reward=28.68, Steps=20313, Training iteration=24"
2019-10-02 11:04:05.473,"Training> Name=main_level/agent, Worker=0, Episode=496, Total reward=34.61, Steps=20252, Training iteration=24"
2019-10-02 11:04:00.472,"Training> Name=main_level/agent, Worker=0, Episode=495, Total reward=55.37, Steps=20185, Training iteration=24"
2019-10-02 11:03:51.469,"Training> Name=main_level/agent, Worker=0, Episode=494, Total reward=15.06, Steps=20055, Training iteration=24"
2019-10-02 11:03:48.468,"Training> Name=main_level/agent, Worker=0, Episode=493, Total reward=32.13, Steps=20022, Training iteration=24"
2019-10-02 11:03:42.466,"Training> Name=main_level/agent, Worker=0, Episode=492, Total reward=26.54, Steps=19940, Training iteration=24"
2019-10-02 11:03:38.465,"Training> Name=main_level/agent, Worker=0, Episode=491, Total reward=39.78, Steps=19875, Training iteration=24"
2019-10-02 11:03:31.463,"Training> Name=main_level/agent, Worker=0, Episode=490, Total reward=12.98, Steps=19790, Training iteration=24"
2019-10-02 11:03:29.462,"Training> Name=main_level/agent, Worker=0, Episode=489, Total reward=1.55, Steps=19765, Training iteration=24"
2019-10-02 11:03:28.462,"Training> Name=main_level/agent, Worker=0, Episode=488, Total reward=4.24, Steps=19752, Training iteration=24"
2019-10-02 11:03:26.462,"Training> Name=main_level/agent, Worker=0, Episode=487, Total reward=9.4, Steps=19730, Training iteration=24"
2019-10-02 11:03:24.461,"Training> Name=main_level/agent, Worker=0, Episode=486, Total reward=19.58, Steps=19705, Training iteration=24"
2019-10-02 11:03:19.459,"Training> Name=main_level/agent, Worker=0, Episode=485, Total reward=25.23, Steps=19646, Training iteration=24"
2019-10-02 11:03:13.457,"Training> Name=main_level/agent, Worker=0, Episode=484, Total reward=19.97, Steps=19562, Training iteration=24"
2019-10-02 11:03:09.456,"Training> Name=main_level/agent, Worker=0, Episode=483, Total reward=32.41, Steps=19514, Training iteration=24"
2019-10-02 11:03:05.455,"Training> Name=main_level/agent, Worker=0, Episode=482, Total reward=38.83, Steps=19456, Training iteration=24"
2019-10-02 11:02:59.454,"Training> Name=main_level/agent, Worker=0, Episode=481, Total reward=9.85, Steps=19379, Training iteration=24"
2019-10-02 11:02:57.453,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_24.pb
2019-10-02 11:02:56.452,Uploaded 3 files for checkpoint 24
2019-10-02 11:02:56.452,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:02:56.452,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:02:54.452,Checkpoint> Saving in path=['./checkpoint/24_Step-19360.ckpt']
2019-10-02 11:02:54.451,"Policy training> Surrogate loss=-0.11451957374811172, KL divergence=0.030872344970703125, Entropy=2.0481762886047363, training epoch=9, learning_rate=3e-05"
2019-10-02 11:02:47.280,"Policy training> Surrogate loss=-0.11433780938386917, KL divergence=0.0300513356924057, Entropy=2.0628931522369385, training epoch=8, learning_rate=3e-05"
2019-10-02 11:02:41.234,"Policy training> Surrogate loss=-0.10445564240217209, KL divergence=0.029980318620800972, Entropy=2.0504069328308105, training epoch=7, learning_rate=3e-05"
2019-10-02 11:02:33.823,"Policy training> Surrogate loss=-0.0979062169790268, KL divergence=0.02813979610800743, Entropy=2.053074359893799, training epoch=6, learning_rate=3e-05"
2019-10-02 11:02:27.051,"Policy training> Surrogate loss=-0.09057976305484772, KL divergence=0.027715617790818214, Entropy=2.0609331130981445, training epoch=5, learning_rate=3e-05"
2019-10-02 11:02:20.604,"Policy training> Surrogate loss=-0.09084747731685638, KL divergence=0.024588093161582947, Entropy=2.0619730949401855, training epoch=4, learning_rate=3e-05"
2019-10-02 11:02:14.619,"Policy training> Surrogate loss=-0.07451456040143967, KL divergence=0.02591761201620102, Entropy=2.0650367736816406, training epoch=3, learning_rate=3e-05"
2019-10-02 11:02:08.022,"Policy training> Surrogate loss=-0.06986751407384872, KL divergence=0.02441592700779438, Entropy=2.046384572982788, training epoch=2, learning_rate=3e-05"
2019-10-02 11:02:00.946,"Policy training> Surrogate loss=-0.04194384440779686, KL divergence=0.021827176213264465, Entropy=2.04050612449646, training epoch=1, learning_rate=3e-05"
2019-10-02 11:01:53.621,"Policy training> Surrogate loss=0.01039674412459135, KL divergence=0.008575547486543655, Entropy=2.0731635093688965, training epoch=0, learning_rate=3e-05"
2019-10-02 11:01:45.786,"Training> Name=main_level/agent, Worker=0, Episode=480, Total reward=25.99, Steps=19360, Training iteration=23"
2019-10-02 11:01:41.778,"Training> Name=main_level/agent, Worker=0, Episode=479, Total reward=26.22, Steps=19305, Training iteration=23"
2019-10-02 11:01:36.777,"Training> Name=main_level/agent, Worker=0, Episode=478, Total reward=50.57, Steps=19248, Training iteration=23"
2019-10-02 11:01:29.774,"Training> Name=main_level/agent, Worker=0, Episode=477, Total reward=16.08, Steps=19142, Training iteration=23"
2019-10-02 11:01:26.774,"Training> Name=main_level/agent, Worker=0, Episode=476, Total reward=26.05, Steps=19109, Training iteration=23"
2019-10-02 11:01:22.772,"Training> Name=main_level/agent, Worker=0, Episode=475, Total reward=20.5, Steps=19058, Training iteration=23"
2019-10-02 11:01:18.771,"Training> Name=main_level/agent, Worker=0, Episode=474, Total reward=41.09, Steps=19010, Training iteration=23"
2019-10-02 11:01:12.769,"Training> Name=main_level/agent, Worker=0, Episode=473, Total reward=92.79, Steps=18917, Training iteration=23"
2019-10-02 11:00:58.765,"Training> Name=main_level/agent, Worker=0, Episode=472, Total reward=28.65, Steps=18719, Training iteration=23"
2019-10-02 11:00:53.764,"Training> Name=main_level/agent, Worker=0, Episode=471, Total reward=26.75, Steps=18658, Training iteration=23"
2019-10-02 11:00:48.762,"Training> Name=main_level/agent, Worker=0, Episode=470, Total reward=17.86, Steps=18592, Training iteration=23"
2019-10-02 11:00:45.761,"Training> Name=main_level/agent, Worker=0, Episode=469, Total reward=22.72, Steps=18548, Training iteration=23"
2019-10-02 11:00:40.760,"Training> Name=main_level/agent, Worker=0, Episode=468, Total reward=11.71, Steps=18486, Training iteration=23"
2019-10-02 11:00:38.759,"Training> Name=main_level/agent, Worker=0, Episode=467, Total reward=4.56, Steps=18459, Training iteration=23"
2019-10-02 11:00:37.759,"Training> Name=main_level/agent, Worker=0, Episode=466, Total reward=15.27, Steps=18444, Training iteration=23"
2019-10-02 11:00:33.758,"Training> Name=main_level/agent, Worker=0, Episode=465, Total reward=35.22, Steps=18402, Training iteration=23"
2019-10-02 11:00:28.756,"Training> Name=main_level/agent, Worker=0, Episode=464, Total reward=22.31, Steps=18332, Training iteration=23"
2019-10-02 11:00:24.755,"Training> Name=main_level/agent, Worker=0, Episode=463, Total reward=36.49, Steps=18286, Training iteration=23"
2019-10-02 11:00:18.753,"Training> Name=main_level/agent, Worker=0, Episode=462, Total reward=45.87, Steps=18200, Training iteration=23"
2019-10-02 11:00:11.751,"Training> Name=main_level/agent, Worker=0, Episode=461, Total reward=57.12, Steps=18103, Training iteration=23"
2019-10-02 11:00:01.748,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_23.pb
2019-10-02 11:00:00.748,INFO:tensorflow:Froze 11 variables.
2019-10-02 11:00:00.748,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 11:00:00.747,Uploaded 3 files for checkpoint 23
2019-10-02 10:59:55.746,Checkpoint> Saving in path=['./checkpoint/23_Step-17985.ckpt']
2019-10-02 10:59:54.745,"Policy training> Surrogate loss=-0.11938824504613876, KL divergence=0.02880626916885376, Entropy=2.006108283996582, training epoch=9, learning_rate=3e-05"
2019-10-02 10:59:48.709,"Policy training> Surrogate loss=-0.11670983582735062, KL divergence=0.027593955397605896, Entropy=2.0219624042510986, training epoch=8, learning_rate=3e-05"
2019-10-02 10:59:43.110,"Policy training> Surrogate loss=-0.11407876014709473, KL divergence=0.028808128088712692, Entropy=2.022824287414551, training epoch=7, learning_rate=3e-05"
2019-10-02 10:59:37.469,"Policy training> Surrogate loss=-0.09990591555833817, KL divergence=0.03154808655381203, Entropy=2.008676528930664, training epoch=6, learning_rate=3e-05"
2019-10-02 10:59:31.906,"Policy training> Surrogate loss=-0.09290248900651932, KL divergence=0.02780776470899582, Entropy=2.0435357093811035, training epoch=5, learning_rate=3e-05"
2019-10-02 10:59:26.997,"Policy training> Surrogate loss=-0.09622859209775925, KL divergence=0.022495046257972717, Entropy=2.0169599056243896, training epoch=4, learning_rate=3e-05"
2019-10-02 10:59:21.554,"Policy training> Surrogate loss=-0.07965987920761108, KL divergence=0.020023783668875694, Entropy=2.049534559249878, training epoch=3, learning_rate=3e-05"
2019-10-02 10:59:16.445,"Policy training> Surrogate loss=-0.06151345744729042, KL divergence=0.027674447745084763, Entropy=1.9977978467941284, training epoch=2, learning_rate=3e-05"
2019-10-02 10:59:11.153,"Policy training> Surrogate loss=-0.04009564593434334, KL divergence=0.01624862290918827, Entropy=2.051802635192871, training epoch=1, learning_rate=3e-05"
2019-10-02 10:59:04.113,"Policy training> Surrogate loss=0.00580534478649497, KL divergence=0.013400924392044544, Entropy=2.027836561203003, training epoch=0, learning_rate=3e-05"
2019-10-02 10:58:57.410,"Training> Name=main_level/agent, Worker=0, Episode=460, Total reward=20.0, Steps=17985, Training iteration=22"
2019-10-02 10:58:55.409,"Training> Name=main_level/agent, Worker=0, Episode=459, Total reward=29.07, Steps=17950, Training iteration=22"
2019-10-02 10:58:49.407,"Training> Name=main_level/agent, Worker=0, Episode=458, Total reward=61.23, Steps=17881, Training iteration=22"
2019-10-02 10:58:41.405,"Training> Name=main_level/agent, Worker=0, Episode=457, Total reward=29.69, Steps=17756, Training iteration=22"
2019-10-02 10:58:36.403,"Training> Name=main_level/agent, Worker=0, Episode=456, Total reward=38.4, Steps=17696, Training iteration=22"
2019-10-02 10:58:31.402,"Training> Name=main_level/agent, Worker=0, Episode=455, Total reward=12.05, Steps=17626, Training iteration=22"
2019-10-02 10:58:29.401,"Training> Name=main_level/agent, Worker=0, Episode=454, Total reward=7.89, Steps=17600, Training iteration=22"
2019-10-02 10:58:27.401,"Training> Name=main_level/agent, Worker=0, Episode=453, Total reward=4.33, Steps=17582, Training iteration=22"
2019-10-02 10:58:25.400,"Training> Name=main_level/agent, Worker=0, Episode=452, Total reward=15.08, Steps=17557, Training iteration=22"
2019-10-02 10:58:22.399,"Training> Name=main_level/agent, Worker=0, Episode=451, Total reward=72.83, Steps=17517, Training iteration=22"
2019-10-02 10:58:10.395,"Training> Name=main_level/agent, Worker=0, Episode=450, Total reward=37.48, Steps=17351, Training iteration=22"
2019-10-02 10:58:04.394,"Training> Name=main_level/agent, Worker=0, Episode=449, Total reward=6.37, Steps=17274, Training iteration=22"
2019-10-02 10:58:03.393,"Training> Name=main_level/agent, Worker=0, Episode=448, Total reward=11.77, Steps=17254, Training iteration=22"
2019-10-02 10:58:00.392,"Training> Name=main_level/agent, Worker=0, Episode=447, Total reward=8.74, Steps=17228, Training iteration=22"
2019-10-02 10:57:58.391,"Training> Name=main_level/agent, Worker=0, Episode=446, Total reward=24.88, Steps=17203, Training iteration=22"
2019-10-02 10:57:53.390,"Training> Name=main_level/agent, Worker=0, Episode=445, Total reward=5.93, Steps=17133, Training iteration=22"
2019-10-02 10:57:52.390,"Training> Name=main_level/agent, Worker=0, Episode=444, Total reward=34.6, Steps=17118, Training iteration=22"
2019-10-02 10:57:46.388,"Training> Name=main_level/agent, Worker=0, Episode=443, Total reward=28.82, Steps=17040, Training iteration=22"
2019-10-02 10:57:42.387,"Training> Name=main_level/agent, Worker=0, Episode=442, Total reward=35.2, Steps=16981, Training iteration=22"
2019-10-02 10:57:36.385,"Training> Name=main_level/agent, Worker=0, Episode=441, Total reward=10.36, Steps=16910, Training iteration=22"
2019-10-02 10:57:33.384,Uploaded 3 files for checkpoint 22
2019-10-02 10:57:33.384,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:57:33.384,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:57:33.384,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_22.pb
2019-10-02 10:57:28.383,Checkpoint> Saving in path=['./checkpoint/22_Step-16892.ckpt']
2019-10-02 10:57:27.382,"Policy training> Surrogate loss=-0.11475290358066559, KL divergence=0.027832593768835068, Entropy=2.1020116806030273, training epoch=9, learning_rate=3e-05"
2019-10-02 10:57:22.238,"Policy training> Surrogate loss=-0.12440803647041321, KL divergence=0.029543034732341766, Entropy=2.0990021228790283, training epoch=8, learning_rate=3e-05"
2019-10-02 10:57:17.207,"Policy training> Surrogate loss=-0.10683973878622055, KL divergence=0.02653682231903076, Entropy=2.100311279296875, training epoch=7, learning_rate=3e-05"
2019-10-02 10:57:12.429,"Policy training> Surrogate loss=-0.12393363565206528, KL divergence=0.026524215936660767, Entropy=2.1103546619415283, training epoch=6, learning_rate=3e-05"
2019-10-02 10:57:06.514,"Policy training> Surrogate loss=-0.0941389948129654, KL divergence=0.026741517707705498, Entropy=2.109182357788086, training epoch=5, learning_rate=3e-05"
2019-10-02 10:57:01.372,"Policy training> Surrogate loss=-0.0879436805844307, KL divergence=0.023932773619890213, Entropy=2.1194868087768555, training epoch=4, learning_rate=3e-05"
2019-10-02 10:56:56.312,"Policy training> Surrogate loss=-0.06613127887248993, KL divergence=0.022908702492713928, Entropy=2.1309385299682617, training epoch=3, learning_rate=3e-05"
2019-10-02 10:56:51.205,"Policy training> Surrogate loss=-0.08058339357376099, KL divergence=0.021287119016051292, Entropy=2.13161563873291, training epoch=2, learning_rate=3e-05"
2019-10-02 10:56:46.419,"Policy training> Surrogate loss=-0.0457204133272171, KL divergence=0.016029713675379753, Entropy=2.1688308715820312, training epoch=1, learning_rate=3e-05"
2019-10-02 10:56:40.625,"Policy training> Surrogate loss=0.001239473931491375, KL divergence=0.010759670287370682, Entropy=2.1179006099700928, training epoch=0, learning_rate=3e-05"
2019-10-02 10:56:34.450,"Training> Name=main_level/agent, Worker=0, Episode=440, Total reward=18.08, Steps=16892, Training iteration=21"
2019-10-02 10:56:32.449,"Training> Name=main_level/agent, Worker=0, Episode=439, Total reward=9.55, Steps=16861, Training iteration=21"
2019-10-02 10:56:29.448,"Training> Name=main_level/agent, Worker=0, Episode=438, Total reward=17.83, Steps=16830, Training iteration=21"
2019-10-02 10:56:26.447,"Training> Name=main_level/agent, Worker=0, Episode=437, Total reward=28.57, Steps=16792, Training iteration=21"
2019-10-02 10:56:22.446,"Training> Name=main_level/agent, Worker=0, Episode=436, Total reward=12.61, Steps=16743, Training iteration=21"
2019-10-02 10:56:20.445,"Training> Name=main_level/agent, Worker=0, Episode=435, Total reward=18.83, Steps=16715, Training iteration=21"
2019-10-02 10:56:17.444,"Training> Name=main_level/agent, Worker=0, Episode=434, Total reward=45.93, Steps=16679, Training iteration=21"
2019-10-02 10:56:10.442,"Training> Name=main_level/agent, Worker=0, Episode=433, Total reward=72.99, Steps=16583, Training iteration=21"
2019-10-02 10:55:59.439,"Training> Name=main_level/agent, Worker=0, Episode=432, Total reward=16.38, Steps=16426, Training iteration=21"
2019-10-02 10:55:56.438,"Training> Name=main_level/agent, Worker=0, Episode=431, Total reward=38.21, Steps=16388, Training iteration=21"
2019-10-02 10:55:51.436,"Training> Name=main_level/agent, Worker=0, Episode=430, Total reward=13.54, Steps=16311, Training iteration=21"
2019-10-02 10:55:49.435,"Training> Name=main_level/agent, Worker=0, Episode=429, Total reward=16.73, Steps=16288, Training iteration=21"
2019-10-02 10:55:45.434,"Training> Name=main_level/agent, Worker=0, Episode=428, Total reward=24.97, Steps=16245, Training iteration=21"
2019-10-02 10:55:42.433,"Training> Name=main_level/agent, Worker=0, Episode=427, Total reward=25.4, Steps=16199, Training iteration=21"
2019-10-02 10:55:37.432,"Training> Name=main_level/agent, Worker=0, Episode=426, Total reward=13.53, Steps=16141, Training iteration=21"
2019-10-02 10:55:35.431,"Training> Name=main_level/agent, Worker=0, Episode=425, Total reward=14.06, Steps=16115, Training iteration=21"
2019-10-02 10:55:32.430,"Training> Name=main_level/agent, Worker=0, Episode=424, Total reward=13.06, Steps=16074, Training iteration=21"
2019-10-02 10:55:29.429,"Training> Name=main_level/agent, Worker=0, Episode=423, Total reward=44.24, Steps=16048, Training iteration=21"
2019-10-02 10:55:23.428,"Training> Name=main_level/agent, Worker=0, Episode=422, Total reward=26.79, Steps=15956, Training iteration=21"
2019-10-02 10:55:18.426,"Training> Name=main_level/agent, Worker=0, Episode=421, Total reward=39.16, Steps=15894, Training iteration=21"
2019-10-02 10:55:11.424,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_21.pb
2019-10-02 10:55:10.424,Uploaded 3 files for checkpoint 21
2019-10-02 10:55:10.424,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:55:10.424,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:55:06.422,"Policy training> Surrogate loss=-0.11155889183282852, KL divergence=0.02806856296956539, Entropy=2.1361746788024902, training epoch=9, learning_rate=3e-05"
2019-10-02 10:55:06.422,Checkpoint> Saving in path=['./checkpoint/21_Step-15813.ckpt']
2019-10-02 10:55:00.377,"Policy training> Surrogate loss=-0.10535075515508652, KL divergence=0.029431473463773727, Entropy=2.1425914764404297, training epoch=8, learning_rate=3e-05"
2019-10-02 10:54:53.759,"Policy training> Surrogate loss=-0.09799085557460785, KL divergence=0.0266408771276474, Entropy=2.140465259552002, training epoch=7, learning_rate=3e-05"
2019-10-02 10:54:47.247,"Policy training> Surrogate loss=-0.10041911900043488, KL divergence=0.02437300980091095, Entropy=2.166081666946411, training epoch=6, learning_rate=3e-05"
2019-10-02 10:54:39.573,"Policy training> Surrogate loss=-0.09327800571918488, KL divergence=0.02391131967306137, Entropy=2.1416070461273193, training epoch=5, learning_rate=3e-05"
2019-10-02 10:54:33.539,"Policy training> Surrogate loss=-0.0788612812757492, KL divergence=0.020902132615447044, Entropy=2.1615822315216064, training epoch=4, learning_rate=3e-05"
2019-10-02 10:54:26.073,"Policy training> Surrogate loss=-0.0775991827249527, KL divergence=0.019544942304491997, Entropy=2.1556432247161865, training epoch=3, learning_rate=3e-05"
2019-10-02 10:54:19.635,"Policy training> Surrogate loss=-0.06244679167866707, KL divergence=0.017553752288222313, Entropy=2.1565864086151123, training epoch=2, learning_rate=3e-05"
2019-10-02 10:54:13.579,"Policy training> Surrogate loss=-0.04134761542081833, KL divergence=0.013636521995067596, Entropy=2.176515579223633, training epoch=1, learning_rate=3e-05"
2019-10-02 10:54:07.704,"Policy training> Surrogate loss=0.010745210573077202, KL divergence=0.00817051064223051, Entropy=2.170384168624878, training epoch=0, learning_rate=3e-05"
2019-10-02 10:54:00.095,"Training> Name=main_level/agent, Worker=0, Episode=420, Total reward=51.14, Steps=15813, Training iteration=20"
2019-10-02 10:53:51.995,"Training> Name=main_level/agent, Worker=0, Episode=419, Total reward=11.08, Steps=15701, Training iteration=20"
2019-10-02 10:53:48.994,"Training> Name=main_level/agent, Worker=0, Episode=418, Total reward=30.99, Steps=15671, Training iteration=20"
2019-10-02 10:53:43.993,"Training> Name=main_level/agent, Worker=0, Episode=417, Total reward=55.22, Steps=15603, Training iteration=20"
2019-10-02 10:53:36.990,"Training> Name=main_level/agent, Worker=0, Episode=416, Total reward=49.12, Steps=15499, Training iteration=20"
2019-10-02 10:53:28.988,"Training> Name=main_level/agent, Worker=0, Episode=415, Total reward=13.1, Steps=15396, Training iteration=20"
2019-10-02 10:53:26.987,"Training> Name=main_level/agent, Worker=0, Episode=414, Total reward=109.92, Steps=15366, Training iteration=20"
2019-10-02 10:53:13.984,"Training> Name=main_level/agent, Worker=0, Episode=413, Total reward=27.22, Steps=15173, Training iteration=20"
2019-10-02 10:53:08.982,"Training> Name=main_level/agent, Worker=0, Episode=412, Total reward=10.32, Steps=15105, Training iteration=20"
2019-10-02 10:53:06.981,"Training> Name=main_level/agent, Worker=0, Episode=411, Total reward=27.15, Steps=15084, Training iteration=20"
2019-10-02 10:53:01.980,"Training> Name=main_level/agent, Worker=0, Episode=410, Total reward=66.18, Steps=15024, Training iteration=20"
2019-10-02 10:52:51.977,"Training> Name=main_level/agent, Worker=0, Episode=409, Total reward=7.77, Steps=14884, Training iteration=20"
2019-10-02 10:52:50.976,"Training> Name=main_level/agent, Worker=0, Episode=408, Total reward=17.37, Steps=14864, Training iteration=20"
2019-10-02 10:52:46.975,"Training> Name=main_level/agent, Worker=0, Episode=407, Total reward=22.85, Steps=14823, Training iteration=20"
2019-10-02 10:52:42.974,"Training> Name=main_level/agent, Worker=0, Episode=406, Total reward=12.09, Steps=14769, Training iteration=20"
2019-10-02 10:52:40.973,"Training> Name=main_level/agent, Worker=0, Episode=405, Total reward=12.77, Steps=14741, Training iteration=20"
2019-10-02 10:52:37.972,"Training> Name=main_level/agent, Worker=0, Episode=404, Total reward=20.8, Steps=14708, Training iteration=20"
2019-10-02 10:52:33.971,"Training> Name=main_level/agent, Worker=0, Episode=403, Total reward=14.89, Steps=14657, Training iteration=20"
2019-10-02 10:52:31.970,"Training> Name=main_level/agent, Worker=0, Episode=402, Total reward=22.2, Steps=14628, Training iteration=20"
2019-10-02 10:52:27.969,"Training> Name=main_level/agent, Worker=0, Episode=401, Total reward=44.7, Steps=14582, Training iteration=20"
2019-10-02 10:52:19.966,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_20.pb
2019-10-02 10:52:18.966,Uploaded 3 files for checkpoint 20
2019-10-02 10:52:18.966,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:52:18.966,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:52:16.965,"Policy training> Surrogate loss=-0.10905199497938156, KL divergence=0.022753341123461723, Entropy=2.2020061016082764, training epoch=9, learning_rate=3e-05"
2019-10-02 10:52:16.965,Checkpoint> Saving in path=['./checkpoint/20_Step-14490.ckpt']
2019-10-02 10:52:11.890,"Policy training> Surrogate loss=-0.11353535205125809, KL divergence=0.024901362136006355, Entropy=2.187448501586914, training epoch=8, learning_rate=3e-05"
2019-10-02 10:52:06.592,"Policy training> Surrogate loss=-0.09972735494375229, KL divergence=0.021566156297922134, Entropy=2.2111082077026367, training epoch=7, learning_rate=3e-05"
2019-10-02 10:52:01.285,"Policy training> Surrogate loss=-0.09871435910463333, KL divergence=0.022790808230638504, Entropy=2.187760829925537, training epoch=6, learning_rate=3e-05"
2019-10-02 10:51:56.152,"Policy training> Surrogate loss=-0.10249905288219452, KL divergence=0.021318694576621056, Entropy=2.2114362716674805, training epoch=5, learning_rate=3e-05"
2019-10-02 10:51:50.762,"Policy training> Surrogate loss=-0.07324833422899246, KL divergence=0.020356468856334686, Entropy=2.1936511993408203, training epoch=4, learning_rate=3e-05"
2019-10-02 10:51:45.213,"Policy training> Surrogate loss=-0.06878237426280975, KL divergence=0.020341672003269196, Entropy=2.195237159729004, training epoch=3, learning_rate=3e-05"
2019-10-02 10:51:41.108,"Policy training> Surrogate loss=-0.05655001476407051, KL divergence=0.02000816911458969, Entropy=2.217393159866333, training epoch=2, learning_rate=3e-05"
2019-10-02 10:51:34.589,"Policy training> Surrogate loss=-0.04203685373067856, KL divergence=0.016358494758605957, Entropy=2.180569648742676, training epoch=1, learning_rate=3e-05"
2019-10-02 10:51:29.422,"Policy training> Surrogate loss=-1.3164011761546135e-05, KL divergence=0.007953635416924953, Entropy=2.195807456970215, training epoch=0, learning_rate=3e-05"
2019-10-02 10:51:23.312,"Training> Name=main_level/agent, Worker=0, Episode=400, Total reward=20.24, Steps=14490, Training iteration=19"
2019-10-02 10:51:20.311,"Training> Name=main_level/agent, Worker=0, Episode=399, Total reward=2.76, Steps=14451, Training iteration=19"
2019-10-02 10:51:19.311,"Training> Name=main_level/agent, Worker=0, Episode=398, Total reward=55.16, Steps=14437, Training iteration=19"
2019-10-02 10:51:10.308,"Training> Name=main_level/agent, Worker=0, Episode=397, Total reward=17.45, Steps=14310, Training iteration=19"
2019-10-02 10:51:07.307,"Training> Name=main_level/agent, Worker=0, Episode=396, Total reward=45.42, Steps=14279, Training iteration=19"
2019-10-02 10:51:01.305,"Training> Name=main_level/agent, Worker=0, Episode=395, Total reward=22.45, Steps=14185, Training iteration=19"
2019-10-02 10:50:57.304,"Training> Name=main_level/agent, Worker=0, Episode=394, Total reward=40.16, Steps=14130, Training iteration=19"
2019-10-02 10:50:50.302,"Training> Name=main_level/agent, Worker=0, Episode=393, Total reward=14.72, Steps=14044, Training iteration=19"
2019-10-02 10:50:47.301,"Training> Name=main_level/agent, Worker=0, Episode=392, Total reward=18.45, Steps=13997, Training iteration=19"
2019-10-02 10:50:43.300,"Training> Name=main_level/agent, Worker=0, Episode=391, Total reward=21.49, Steps=13950, Training iteration=19"
2019-10-02 10:50:39.298,"Training> Name=main_level/agent, Worker=0, Episode=390, Total reward=10.75, Steps=13907, Training iteration=19"
2019-10-02 10:50:37.298,"Training> Name=main_level/agent, Worker=0, Episode=389, Total reward=27.95, Steps=13884, Training iteration=19"
2019-10-02 10:50:32.296,"Training> Name=main_level/agent, Worker=0, Episode=388, Total reward=7.05, Steps=13814, Training iteration=19"
2019-10-02 10:50:30.295,"Training> Name=main_level/agent, Worker=0, Episode=387, Total reward=4.66, Steps=13784, Training iteration=19"
2019-10-02 10:50:28.295,"Training> Name=main_level/agent, Worker=0, Episode=386, Total reward=5.6, Steps=13767, Training iteration=19"
2019-10-02 10:50:27.294,"Training> Name=main_level/agent, Worker=0, Episode=385, Total reward=15.17, Steps=13751, Training iteration=19"
2019-10-02 10:50:24.293,"Training> Name=main_level/agent, Worker=0, Episode=384, Total reward=22.02, Steps=13711, Training iteration=19"
2019-10-02 10:50:20.292,"Training> Name=main_level/agent, Worker=0, Episode=383, Total reward=37.01, Steps=13662, Training iteration=19"
2019-10-02 10:50:13.290,"Training> Name=main_level/agent, Worker=0, Episode=382, Total reward=27.67, Steps=13571, Training iteration=19"
2019-10-02 10:50:09.289,"Training> Name=main_level/agent, Worker=0, Episode=381, Total reward=36.65, Steps=13519, Training iteration=19"
2019-10-02 10:50:02.286,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_19.pb
2019-10-02 10:50:01.286,Uploaded 3 files for checkpoint 19
2019-10-02 10:50:01.286,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:50:01.286,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:49:59.285,"Policy training> Surrogate loss=-0.10788220167160034, KL divergence=0.02770192362368107, Entropy=2.2070281505584717, training epoch=9, learning_rate=3e-05"
2019-10-02 10:49:59.285,Checkpoint> Saving in path=['./checkpoint/19_Step-13434.ckpt']
2019-10-02 10:49:53.107,"Policy training> Surrogate loss=-0.10407334566116333, KL divergence=0.026176312938332558, Entropy=2.2065556049346924, training epoch=8, learning_rate=3e-05"
2019-10-02 10:49:46.937,"Policy training> Surrogate loss=-0.09686765819787979, KL divergence=0.024397186934947968, Entropy=2.216193437576294, training epoch=7, learning_rate=3e-05"
2019-10-02 10:49:40.890,"Policy training> Surrogate loss=-0.09118171781301498, KL divergence=0.0242964755743742, Entropy=2.2178659439086914, training epoch=6, learning_rate=3e-05"
2019-10-02 10:49:34.488,"Policy training> Surrogate loss=-0.08370538800954819, KL divergence=0.026029514148831367, Entropy=2.2114341259002686, training epoch=5, learning_rate=3e-05"
2019-10-02 10:49:28.437,"Policy training> Surrogate loss=-0.07405354827642441, KL divergence=0.019302183762192726, Entropy=2.212048292160034, training epoch=4, learning_rate=3e-05"
2019-10-02 10:49:21.002,"Policy training> Surrogate loss=-0.07373176515102386, KL divergence=0.021608036011457443, Entropy=2.198392391204834, training epoch=3, learning_rate=3e-05"
2019-10-02 10:49:15.265,"Policy training> Surrogate loss=-0.052680280059576035, KL divergence=0.01814121939241886, Entropy=2.233433246612549, training epoch=2, learning_rate=3e-05"
2019-10-02 10:49:08.910,"Policy training> Surrogate loss=-0.040388960391283035, KL divergence=0.020093657076358795, Entropy=2.1821682453155518, training epoch=1, learning_rate=3e-05"
2019-10-02 10:49:02.765,"Policy training> Surrogate loss=0.006490009371191263, KL divergence=0.016748661175370216, Entropy=2.205021619796753, training epoch=0, learning_rate=3e-05"
2019-10-02 10:48:55.093,"Training> Name=main_level/agent, Worker=0, Episode=380, Total reward=20.98, Steps=13434, Training iteration=18"
2019-10-02 10:48:52.087,"Training> Name=main_level/agent, Worker=0, Episode=379, Total reward=11.92, Steps=13396, Training iteration=18"
2019-10-02 10:48:49.086,"Training> Name=main_level/agent, Worker=0, Episode=378, Total reward=15.86, Steps=13367, Training iteration=18"
2019-10-02 10:48:47.086,"Training> Name=main_level/agent, Worker=0, Episode=377, Total reward=19.89, Steps=13329, Training iteration=18"
2019-10-02 10:48:43.084,"Training> Name=main_level/agent, Worker=0, Episode=376, Total reward=71.58, Steps=13290, Training iteration=18"
2019-10-02 10:48:33.082,"Training> Name=main_level/agent, Worker=0, Episode=375, Total reward=67.25, Steps=13139, Training iteration=18"
2019-10-02 10:48:23.079,"Training> Name=main_level/agent, Worker=0, Episode=374, Total reward=54.54, Steps=12994, Training iteration=18"
2019-10-02 10:48:14.076,"Training> Name=main_level/agent, Worker=0, Episode=373, Total reward=21.07, Steps=12875, Training iteration=18"
2019-10-02 10:48:10.075,"Training> Name=main_level/agent, Worker=0, Episode=372, Total reward=15.56, Steps=12817, Training iteration=18"
2019-10-02 10:48:06.073,"Training> Name=main_level/agent, Worker=0, Episode=371, Total reward=22.55, Steps=12774, Training iteration=18"
2019-10-02 10:48:02.072,"Training> Name=main_level/agent, Worker=0, Episode=370, Total reward=23.42, Steps=12721, Training iteration=18"
2019-10-02 10:47:59.071,"Training> Name=main_level/agent, Worker=0, Episode=369, Total reward=35.95, Steps=12672, Training iteration=18"
2019-10-02 10:47:52.069,"Training> Name=main_level/agent, Worker=0, Episode=368, Total reward=8.18, Steps=12579, Training iteration=18"
2019-10-02 10:47:50.069,"Training> Name=main_level/agent, Worker=0, Episode=367, Total reward=9.95, Steps=12556, Training iteration=18"
2019-10-02 10:47:47.068,"Training> Name=main_level/agent, Worker=0, Episode=366, Total reward=23.56, Steps=12524, Training iteration=18"
2019-10-02 10:47:44.067,"Training> Name=main_level/agent, Worker=0, Episode=365, Total reward=12.95, Steps=12476, Training iteration=18"
2019-10-02 10:47:41.066,"Training> Name=main_level/agent, Worker=0, Episode=364, Total reward=35.91, Steps=12445, Training iteration=18"
2019-10-02 10:47:35.064,"Training> Name=main_level/agent, Worker=0, Episode=363, Total reward=9.86, Steps=12360, Training iteration=18"
2019-10-02 10:47:33.063,"Training> Name=main_level/agent, Worker=0, Episode=362, Total reward=36.42, Steps=12342, Training iteration=18"
2019-10-02 10:47:27.061,"Training> Name=main_level/agent, Worker=0, Episode=361, Total reward=37.31, Steps=12259, Training iteration=18"
2019-10-02 10:47:21.060,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_18.pb
2019-10-02 10:47:20.059,Uploaded 3 files for checkpoint 18
2019-10-02 10:47:20.059,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:47:20.059,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:47:18.058,Checkpoint> Saving in path=['./checkpoint/18_Step-12180.ckpt']
2019-10-02 10:47:17.058,"Policy training> Surrogate loss=-0.11039173603057861, KL divergence=0.029352795332670212, Entropy=2.146019697189331, training epoch=9, learning_rate=3e-05"
2019-10-02 10:47:12.334,"Policy training> Surrogate loss=-0.1137668788433075, KL divergence=0.024732286110520363, Entropy=2.165799617767334, training epoch=8, learning_rate=3e-05"
2019-10-02 10:47:06.072,"Policy training> Surrogate loss=-0.10040489584207535, KL divergence=0.02500982955098152, Entropy=2.1686298847198486, training epoch=7, learning_rate=3e-05"
2019-10-02 10:47:00.928,"Policy training> Surrogate loss=-0.08827263861894608, KL divergence=0.024747909978032112, Entropy=2.1786508560180664, training epoch=6, learning_rate=3e-05"
2019-10-02 10:46:56.466,"Policy training> Surrogate loss=-0.086916483938694, KL divergence=0.020562417805194855, Entropy=2.171516180038452, training epoch=5, learning_rate=3e-05"
2019-10-02 10:46:50.674,"Policy training> Surrogate loss=-0.0718279778957367, KL divergence=0.021858898922801018, Entropy=2.1999826431274414, training epoch=4, learning_rate=3e-05"
2019-10-02 10:46:45.566,"Policy training> Surrogate loss=-0.06227341666817665, KL divergence=0.023323027417063713, Entropy=2.1657142639160156, training epoch=3, learning_rate=3e-05"
2019-10-02 10:46:40.533,"Policy training> Surrogate loss=-0.04555375128984451, KL divergence=0.02002509869635105, Entropy=2.217456817626953, training epoch=2, learning_rate=3e-05"
2019-10-02 10:46:34.788,"Policy training> Surrogate loss=-0.025052452459931374, KL divergence=0.017622675746679306, Entropy=2.1859569549560547, training epoch=1, learning_rate=3e-05"
2019-10-02 10:46:29.670,"Policy training> Surrogate loss=0.013463186100125313, KL divergence=0.012351274490356445, Entropy=2.181502342224121, training epoch=0, learning_rate=3e-05"
2019-10-02 10:46:23.909,"Training> Name=main_level/agent, Worker=0, Episode=360, Total reward=30.32, Steps=12180, Training iteration=17"
2019-10-02 10:46:18.907,"Training> Name=main_level/agent, Worker=0, Episode=359, Total reward=48.49, Steps=12112, Training iteration=17"
2019-10-02 10:46:10.905,"Training> Name=main_level/agent, Worker=0, Episode=358, Total reward=16.36, Steps=12005, Training iteration=17"
2019-10-02 10:46:08.904,"Training> Name=main_level/agent, Worker=0, Episode=357, Total reward=28.9, Steps=11977, Training iteration=17"
2019-10-02 10:46:03.903,"Training> Name=main_level/agent, Worker=0, Episode=356, Total reward=16.46, Steps=11919, Training iteration=17"
2019-10-02 10:46:01.902,"Training> Name=main_level/agent, Worker=0, Episode=355, Total reward=20.96, Steps=11890, Training iteration=17"
2019-10-02 10:45:58.901,"Training> Name=main_level/agent, Worker=0, Episode=354, Total reward=21.58, Steps=11848, Training iteration=17"
2019-10-02 10:45:54.900,"Training> Name=main_level/agent, Worker=0, Episode=353, Total reward=40.47, Steps=11797, Training iteration=17"
2019-10-02 10:45:47.898,"Training> Name=main_level/agent, Worker=0, Episode=352, Total reward=13.52, Steps=11709, Training iteration=17"
2019-10-02 10:45:44.897,"Training> Name=main_level/agent, Worker=0, Episode=351, Total reward=36.98, Steps=11669, Training iteration=17"
2019-10-02 10:45:38.895,"Training> Name=main_level/agent, Worker=0, Episode=350, Total reward=18.59, Steps=11581, Training iteration=17"
2019-10-02 10:45:35.894,"Training> Name=main_level/agent, Worker=0, Episode=349, Total reward=12.47, Steps=11550, Training iteration=17"
2019-10-02 10:45:31.893,"Training> Name=main_level/agent, Worker=0, Episode=348, Total reward=18.91, Steps=11496, Training iteration=17"
2019-10-02 10:45:28.892,"Training> Name=main_level/agent, Worker=0, Episode=347, Total reward=5.48, Steps=11456, Training iteration=17"
2019-10-02 10:45:26.891,"Training> Name=main_level/agent, Worker=0, Episode=346, Total reward=19.25, Steps=11438, Training iteration=17"
2019-10-02 10:45:22.890,"Training> Name=main_level/agent, Worker=0, Episode=345, Total reward=15.43, Steps=11383, Training iteration=17"
2019-10-02 10:45:19.889,"Training> Name=main_level/agent, Worker=0, Episode=344, Total reward=15.93, Steps=11345, Training iteration=17"
2019-10-02 10:45:16.888,"Training> Name=main_level/agent, Worker=0, Episode=343, Total reward=51.43, Steps=11308, Training iteration=17"
2019-10-02 10:45:10.887,"Training> Name=main_level/agent, Worker=0, Episode=342, Total reward=24.35, Steps=11218, Training iteration=17"
2019-10-02 10:45:06.886,"Training> Name=main_level/agent, Worker=0, Episode=341, Total reward=17.18, Steps=11171, Training iteration=17"
2019-10-02 10:45:02.885,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_17.pb
2019-10-02 10:45:01.884,Uploaded 3 files for checkpoint 17
2019-10-02 10:45:01.884,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:45:01.884,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:44:59.883,"Policy training> Surrogate loss=-0.10275661945343018, KL divergence=0.026591872796416283, Entropy=2.2073562145233154, training epoch=9, learning_rate=3e-05"
2019-10-02 10:44:59.883,Checkpoint> Saving in path=['./checkpoint/17_Step-11136.ckpt']
2019-10-02 10:44:55.738,"Policy training> Surrogate loss=-0.09580161422491074, KL divergence=0.0253441222012043, Entropy=2.2322804927825928, training epoch=8, learning_rate=3e-05"
2019-10-02 10:44:50.841,"Policy training> Surrogate loss=-0.1060275062918663, KL divergence=0.0240051057189703, Entropy=2.224522829055786, training epoch=7, learning_rate=3e-05"
2019-10-02 10:44:45.786,"Policy training> Surrogate loss=-0.10240344703197479, KL divergence=0.02224580943584442, Entropy=2.2195568084716797, training epoch=6, learning_rate=3e-05"
2019-10-02 10:44:41.318,"Policy training> Surrogate loss=-0.08258406072854996, KL divergence=0.020599758252501488, Entropy=2.23887300491333, training epoch=5, learning_rate=3e-05"
2019-10-02 10:44:37.301,"Policy training> Surrogate loss=-0.08314695209264755, KL divergence=0.01889951340854168, Entropy=2.2246689796447754, training epoch=4, learning_rate=3e-05"
2019-10-02 10:44:32.036,"Policy training> Surrogate loss=-0.08575767278671265, KL divergence=0.02014453336596489, Entropy=2.210693359375, training epoch=3, learning_rate=3e-05"
2019-10-02 10:44:27.840,"Policy training> Surrogate loss=-0.05001553148031235, KL divergence=0.01822763681411743, Entropy=2.2380053997039795, training epoch=2, learning_rate=3e-05"
2019-10-02 10:44:22.695,"Policy training> Surrogate loss=-0.01430704165250063, KL divergence=0.017406173050403595, Entropy=2.2436208724975586, training epoch=1, learning_rate=3e-05"
2019-10-02 10:44:18.549,"Policy training> Surrogate loss=0.008069303818047047, KL divergence=0.01247803121805191, Entropy=2.2338032722473145, training epoch=0, learning_rate=3e-05"
2019-10-02 10:44:13.366,"Training> Name=main_level/agent, Worker=0, Episode=340, Total reward=44.7, Steps=11136, Training iteration=16"
2019-10-02 10:44:07.302,"Training> Name=main_level/agent, Worker=0, Episode=339, Total reward=16.33, Steps=11061, Training iteration=16"
2019-10-02 10:44:04.301,"Training> Name=main_level/agent, Worker=0, Episode=338, Total reward=65.46, Steps=11026, Training iteration=16"
2019-10-02 10:43:55.298,"Training> Name=main_level/agent, Worker=0, Episode=337, Total reward=16.8, Steps=10897, Training iteration=16"
2019-10-02 10:43:53.298,"Training> Name=main_level/agent, Worker=0, Episode=336, Total reward=10.79, Steps=10868, Training iteration=16"
2019-10-02 10:43:50.297,"Training> Name=main_level/agent, Worker=0, Episode=335, Total reward=29.65, Steps=10836, Training iteration=16"
2019-10-02 10:43:46.295,"Training> Name=main_level/agent, Worker=0, Episode=334, Total reward=23.13, Steps=10781, Training iteration=16"
2019-10-02 10:43:43.294,"Training> Name=main_level/agent, Worker=0, Episode=333, Total reward=8.4, Steps=10736, Training iteration=16"
2019-10-02 10:43:40.294,"Training> Name=main_level/agent, Worker=0, Episode=332, Total reward=27.9, Steps=10705, Training iteration=16"
2019-10-02 10:43:35.292,"Training> Name=main_level/agent, Worker=0, Episode=331, Total reward=10.48, Steps=10634, Training iteration=16"
2019-10-02 10:43:33.291,"Training> Name=main_level/agent, Worker=0, Episode=330, Total reward=11.6, Steps=10609, Training iteration=16"
2019-10-02 10:43:31.291,"Training> Name=main_level/agent, Worker=0, Episode=329, Total reward=17.28, Steps=10585, Training iteration=16"
2019-10-02 10:43:27.289,"Training> Name=main_level/agent, Worker=0, Episode=328, Total reward=3.81, Steps=10536, Training iteration=16"
2019-10-02 10:43:25.289,"Training> Name=main_level/agent, Worker=0, Episode=327, Total reward=27.57, Steps=10522, Training iteration=16"
2019-10-02 10:43:20.287,"Training> Name=main_level/agent, Worker=0, Episode=326, Total reward=26.7, Steps=10453, Training iteration=16"
2019-10-02 10:43:16.286,"Training> Name=main_level/agent, Worker=0, Episode=325, Total reward=17.2, Steps=10390, Training iteration=16"
2019-10-02 10:43:12.285,"Training> Name=main_level/agent, Worker=0, Episode=324, Total reward=19.92, Steps=10348, Training iteration=16"
2019-10-02 10:43:09.284,"Training> Name=main_level/agent, Worker=0, Episode=323, Total reward=24.2, Steps=10304, Training iteration=16"
2019-10-02 10:43:05.282,"Training> Name=main_level/agent, Worker=0, Episode=322, Total reward=18.83, Steps=10250, Training iteration=16"
2019-10-02 10:43:01.281,"Training> Name=main_level/agent, Worker=0, Episode=321, Total reward=15.85, Steps=10206, Training iteration=16"
2019-10-02 10:42:59.280,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_16.pb
2019-10-02 10:42:57.280,Uploaded 3 files for checkpoint 16
2019-10-02 10:42:57.280,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:42:57.280,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:42:55.279,"Policy training> Surrogate loss=-0.10796882212162018, KL divergence=0.03993535786867142, Entropy=2.1549837589263916, training epoch=9, learning_rate=3e-05"
2019-10-02 10:42:55.279,Checkpoint> Saving in path=['./checkpoint/16_Step-10182.ckpt']
2019-10-02 10:42:50.048,"Policy training> Surrogate loss=-0.10613574087619781, KL divergence=0.03202664479613304, Entropy=2.1945087909698486, training epoch=8, learning_rate=3e-05"
2019-10-02 10:42:45.518,"Policy training> Surrogate loss=-0.09921172261238098, KL divergence=0.03505590185523033, Entropy=2.157628059387207, training epoch=7, learning_rate=3e-05"
2019-10-02 10:42:40.791,"Policy training> Surrogate loss=-0.10112932324409485, KL divergence=0.02902902476489544, Entropy=2.199039936065674, training epoch=6, learning_rate=3e-05"
2019-10-02 10:42:35.704,"Policy training> Surrogate loss=-0.08335745334625244, KL divergence=0.03488195687532425, Entropy=2.1871871948242188, training epoch=5, learning_rate=3e-05"
2019-10-02 10:42:30.649,"Policy training> Surrogate loss=-0.07628391683101654, KL divergence=0.02895895019173622, Entropy=2.164621591567993, training epoch=4, learning_rate=3e-05"
2019-10-02 10:42:26.255,"Policy training> Surrogate loss=-0.0734642967581749, KL divergence=0.02516350895166397, Entropy=2.190058469772339, training epoch=3, learning_rate=3e-05"
2019-10-02 10:42:21.068,"Policy training> Surrogate loss=-0.06526198983192444, KL divergence=0.02330094203352928, Entropy=2.1607136726379395, training epoch=2, learning_rate=3e-05"
2019-10-02 10:42:16.002,"Policy training> Surrogate loss=-0.04247872903943062, KL divergence=0.022201109677553177, Entropy=2.177605628967285, training epoch=1, learning_rate=3e-05"
2019-10-02 10:42:11.939,"Policy training> Surrogate loss=0.006239371839910746, KL divergence=0.012737452983856201, Entropy=2.181896209716797, training epoch=0, learning_rate=3e-05"
2019-10-02 10:42:05.818,"Training> Name=main_level/agent, Worker=0, Episode=320, Total reward=44.83, Steps=10182, Training iteration=15"
2019-10-02 10:41:58.815,"Training> Name=main_level/agent, Worker=0, Episode=319, Total reward=25.04, Steps=10088, Training iteration=15"
2019-10-02 10:41:54.814,"Training> Name=main_level/agent, Worker=0, Episode=318, Total reward=13.11, Steps=10031, Training iteration=15"
2019-10-02 10:41:52.813,"Training> Name=main_level/agent, Worker=0, Episode=317, Total reward=30.27, Steps=10008, Training iteration=15"
2019-10-02 10:41:47.812,"Training> Name=main_level/agent, Worker=0, Episode=316, Total reward=32.04, Steps=9945, Training iteration=15"
2019-10-02 10:41:42.810,"Training> Name=main_level/agent, Worker=0, Episode=315, Total reward=12.42, Steps=9881, Training iteration=15"
2019-10-02 10:41:39.809,"Training> Name=main_level/agent, Worker=0, Episode=314, Total reward=27.84, Steps=9849, Training iteration=15"
2019-10-02 10:41:36.808,"Training> Name=main_level/agent, Worker=0, Episode=313, Total reward=49.54, Steps=9800, Training iteration=15"
2019-10-02 10:41:27.806,"Training> Name=main_level/agent, Worker=0, Episode=312, Total reward=32.72, Steps=9678, Training iteration=15"
2019-10-02 10:41:21.804,"Training> Name=main_level/agent, Worker=0, Episode=311, Total reward=16.2, Steps=9597, Training iteration=15"
2019-10-02 10:41:19.803,"Training> Name=main_level/agent, Worker=0, Episode=310, Total reward=18.74, Steps=9566, Training iteration=15"
2019-10-02 10:41:15.802,"Training> Name=main_level/agent, Worker=0, Episode=309, Total reward=15.7, Steps=9522, Training iteration=15"
2019-10-02 10:41:12.801,"Training> Name=main_level/agent, Worker=0, Episode=308, Total reward=12.03, Steps=9485, Training iteration=15"
2019-10-02 10:41:10.800,"Training> Name=main_level/agent, Worker=0, Episode=307, Total reward=5.65, Steps=9455, Training iteration=15"
2019-10-02 10:41:08.799,"Training> Name=main_level/agent, Worker=0, Episode=306, Total reward=14.24, Steps=9436, Training iteration=15"
2019-10-02 10:41:05.798,"Training> Name=main_level/agent, Worker=0, Episode=305, Total reward=10.26, Steps=9392, Training iteration=15"
2019-10-02 10:41:02.798,"Training> Name=main_level/agent, Worker=0, Episode=304, Total reward=16.29, Steps=9367, Training iteration=15"
2019-10-02 10:41:00.797,"Training> Name=main_level/agent, Worker=0, Episode=303, Total reward=22.86, Steps=9335, Training iteration=15"
2019-10-02 10:40:56.796,"Training> Name=main_level/agent, Worker=0, Episode=302, Total reward=18.59, Steps=9285, Training iteration=15"
2019-10-02 10:40:53.795,"Training> Name=main_level/agent, Worker=0, Episode=301, Total reward=38.35, Steps=9249, Training iteration=15"
2019-10-02 10:40:47.793,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_15.pb
2019-10-02 10:40:46.792,Uploaded 3 files for checkpoint 15
2019-10-02 10:40:46.792,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:40:46.792,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:40:44.792,Checkpoint> Saving in path=['./checkpoint/15_Step-9180.ckpt']
2019-10-02 10:40:43.791,"Policy training> Surrogate loss=-0.1121649295091629, KL divergence=0.02591843344271183, Entropy=2.2218518257141113, training epoch=9, learning_rate=3e-05"
2019-10-02 10:40:39.751,"Policy training> Surrogate loss=-0.0999845415353775, KL divergence=0.022967366501688957, Entropy=2.22617769241333, training epoch=8, learning_rate=3e-05"
2019-10-02 10:40:35.677,"Policy training> Surrogate loss=-0.10211969912052155, KL divergence=0.022490616887807846, Entropy=2.2470898628234863, training epoch=7, learning_rate=3e-05"
2019-10-02 10:40:31.579,"Policy training> Surrogate loss=-0.09631656855344772, KL divergence=0.023216279223561287, Entropy=2.2189481258392334, training epoch=6, learning_rate=3e-05"
2019-10-02 10:40:27.149,"Policy training> Surrogate loss=-0.08466620743274689, KL divergence=0.019651290029287338, Entropy=2.2257373332977295, training epoch=5, learning_rate=3e-05"
2019-10-02 10:40:22.526,"Policy training> Surrogate loss=-0.07532207667827606, KL divergence=0.0176080409437418, Entropy=2.242335796356201, training epoch=4, learning_rate=3e-05"
2019-10-02 10:40:18.043,"Policy training> Surrogate loss=-0.06357628852128983, KL divergence=0.019361376762390137, Entropy=2.2346675395965576, training epoch=3, learning_rate=3e-05"
2019-10-02 10:40:13.913,"Policy training> Surrogate loss=-0.04403242841362953, KL divergence=0.015040376223623753, Entropy=2.2277321815490723, training epoch=2, learning_rate=3e-05"
2019-10-02 10:40:09.802,"Policy training> Surrogate loss=-0.03675263375043869, KL divergence=0.01476569939404726, Entropy=2.215803384780884, training epoch=1, learning_rate=3e-05"
2019-10-02 10:40:05.762,"Policy training> Surrogate loss=0.002982620382681489, KL divergence=0.011652990244328976, Entropy=2.214345932006836, training epoch=0, learning_rate=3e-05"
2019-10-02 10:40:00.724,"Training> Name=main_level/agent, Worker=0, Episode=300, Total reward=40.24, Steps=9180, Training iteration=14"
2019-10-02 10:39:54.722,"Training> Name=main_level/agent, Worker=0, Episode=299, Total reward=17.76, Steps=9101, Training iteration=14"
2019-10-02 10:39:51.721,"Training> Name=main_level/agent, Worker=0, Episode=298, Total reward=22.65, Steps=9054, Training iteration=14"
2019-10-02 10:39:47.720,"Training> Name=main_level/agent, Worker=0, Episode=297, Total reward=45.27, Steps=9011, Training iteration=14"
2019-10-02 10:39:40.718,"Training> Name=main_level/agent, Worker=0, Episode=296, Total reward=10.84, Steps=8912, Training iteration=14"
2019-10-02 10:39:38.717,"Training> Name=main_level/agent, Worker=0, Episode=295, Total reward=12.76, Steps=8884, Training iteration=14"
2019-10-02 10:39:35.716,"Training> Name=main_level/agent, Worker=0, Episode=294, Total reward=14.65, Steps=8854, Training iteration=14"
2019-10-02 10:39:32.715,"Training> Name=main_level/agent, Worker=0, Episode=293, Total reward=11.96, Steps=8815, Training iteration=14"
2019-10-02 10:39:29.714,"Training> Name=main_level/agent, Worker=0, Episode=292, Total reward=12.99, Steps=8771, Training iteration=14"
2019-10-02 10:39:26.713,"Training> Name=main_level/agent, Worker=0, Episode=291, Total reward=31.36, Steps=8744, Training iteration=14"
2019-10-02 10:39:21.711,"Training> Name=main_level/agent, Worker=0, Episode=290, Total reward=32.4, Steps=8675, Training iteration=14"
2019-10-02 10:39:16.710,"Training> Name=main_level/agent, Worker=0, Episode=289, Total reward=9.24, Steps=8606, Training iteration=14"
2019-10-02 10:39:14.709,"Training> Name=main_level/agent, Worker=0, Episode=288, Total reward=8.63, Steps=8585, Training iteration=14"
2019-10-02 10:39:12.708,"Training> Name=main_level/agent, Worker=0, Episode=287, Total reward=13.78, Steps=8558, Training iteration=14"
2019-10-02 10:39:09.707,"Training> Name=main_level/agent, Worker=0, Episode=286, Total reward=14.31, Steps=8524, Training iteration=14"
2019-10-02 10:39:07.707,"Training> Name=main_level/agent, Worker=0, Episode=285, Total reward=6.77, Steps=8490, Training iteration=14"
2019-10-02 10:39:05.706,"Training> Name=main_level/agent, Worker=0, Episode=284, Total reward=18.54, Steps=8470, Training iteration=14"
2019-10-02 10:39:02.705,"Training> Name=main_level/agent, Worker=0, Episode=283, Total reward=8.2, Steps=8438, Training iteration=14"
2019-10-02 10:39:00.705,"Training> Name=main_level/agent, Worker=0, Episode=282, Total reward=15.99, Steps=8421, Training iteration=14"
2019-10-02 10:38:58.704,"Training> Name=main_level/agent, Worker=0, Episode=281, Total reward=32.78, Steps=8393, Training iteration=14"
2019-10-02 10:38:52.702,Uploaded 3 files for checkpoint 14
2019-10-02 10:38:52.702,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:38:52.702,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:38:52.702,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_14.pb
2019-10-02 10:38:49.701,"Policy training> Surrogate loss=-0.1015276089310646, KL divergence=0.025941340252757072, Entropy=2.229377269744873, training epoch=9, learning_rate=3e-05"
2019-10-02 10:38:49.701,Checkpoint> Saving in path=['./checkpoint/14_Step-8338.ckpt']
2019-10-02 10:38:45.610,"Policy training> Surrogate loss=-0.09772026538848877, KL divergence=0.02424604631960392, Entropy=2.228396415710449, training epoch=8, learning_rate=3e-05"
2019-10-02 10:38:40.907,"Policy training> Surrogate loss=-0.092511385679245, KL divergence=0.025684842839837074, Entropy=2.221065044403076, training epoch=7, learning_rate=3e-05"
2019-10-02 10:38:36.589,"Policy training> Surrogate loss=-0.09050658345222473, KL divergence=0.021803824231028557, Entropy=2.2319390773773193, training epoch=6, learning_rate=3e-05"
2019-10-02 10:38:32.161,"Policy training> Surrogate loss=-0.08278074860572815, KL divergence=0.02366058900952339, Entropy=2.2209646701812744, training epoch=5, learning_rate=3e-05"
2019-10-02 10:38:27.454,"Policy training> Surrogate loss=-0.08230053633451462, KL divergence=0.019244346767663956, Entropy=2.2383880615234375, training epoch=4, learning_rate=3e-05"
2019-10-02 10:38:22.986,"Policy training> Surrogate loss=-0.06658156216144562, KL divergence=0.019324064254760742, Entropy=2.2381372451782227, training epoch=3, learning_rate=3e-05"
2019-10-02 10:38:18.982,"Policy training> Surrogate loss=-0.04294411092996597, KL divergence=0.014788765460252762, Entropy=2.2444233894348145, training epoch=2, learning_rate=3e-05"
2019-10-02 10:38:14.770,"Policy training> Surrogate loss=-0.032471220940351486, KL divergence=0.014412043616175652, Entropy=2.23160982131958, training epoch=1, learning_rate=3e-05"
2019-10-02 10:38:10.332,"Policy training> Surrogate loss=-0.00306288106366992, KL divergence=0.009364789351820946, Entropy=2.231797218322754, training epoch=0, learning_rate=3e-05"
2019-10-02 10:38:05.136,"Training> Name=main_level/agent, Worker=0, Episode=280, Total reward=19.83, Steps=8338, Training iteration=13"
2019-10-02 10:38:02.135,"Training> Name=main_level/agent, Worker=0, Episode=279, Total reward=3.0, Steps=8294, Training iteration=13"
2019-10-02 10:38:00.135,"Training> Name=main_level/agent, Worker=0, Episode=278, Total reward=17.29, Steps=8281, Training iteration=13"
2019-10-02 10:37:58.134,"Training> Name=main_level/agent, Worker=0, Episode=277, Total reward=48.36, Steps=8254, Training iteration=13"
2019-10-02 10:37:52.132,"Training> Name=main_level/agent, Worker=0, Episode=276, Total reward=49.29, Steps=8163, Training iteration=13"
2019-10-02 10:37:44.130,"Training> Name=main_level/agent, Worker=0, Episode=275, Total reward=13.86, Steps=8056, Training iteration=13"
2019-10-02 10:37:41.129,"Training> Name=main_level/agent, Worker=0, Episode=274, Total reward=12.73, Steps=8020, Training iteration=13"
2019-10-02 10:37:39.128,"Training> Name=main_level/agent, Worker=0, Episode=273, Total reward=29.88, Steps=7991, Training iteration=13"
2019-10-02 10:37:33.126,"Training> Name=main_level/agent, Worker=0, Episode=272, Total reward=7.97, Steps=7912, Training iteration=13"
2019-10-02 10:37:31.126,"Training> Name=main_level/agent, Worker=0, Episode=271, Total reward=12.35, Steps=7888, Training iteration=13"
2019-10-02 10:37:29.125,"Training> Name=main_level/agent, Worker=0, Episode=270, Total reward=12.34, Steps=7864, Training iteration=13"
2019-10-02 10:37:27.125,"Training> Name=main_level/agent, Worker=0, Episode=269, Total reward=14.96, Steps=7839, Training iteration=13"
2019-10-02 10:37:24.124,"Training> Name=main_level/agent, Worker=0, Episode=268, Total reward=20.73, Steps=7803, Training iteration=13"
2019-10-02 10:37:20.122,"Training> Name=main_level/agent, Worker=0, Episode=267, Total reward=12.37, Steps=7758, Training iteration=13"
2019-10-02 10:37:17.121,"Training> Name=main_level/agent, Worker=0, Episode=266, Total reward=14.04, Steps=7717, Training iteration=13"
2019-10-02 10:37:14.121,"Training> Name=main_level/agent, Worker=0, Episode=265, Total reward=16.71, Steps=7679, Training iteration=13"
2019-10-02 10:37:10.119,"Training> Name=main_level/agent, Worker=0, Episode=264, Total reward=15.17, Steps=7637, Training iteration=13"
2019-10-02 10:37:08.119,"Training> Name=main_level/agent, Worker=0, Episode=263, Total reward=13.82, Steps=7603, Training iteration=13"
2019-10-02 10:37:05.118,"Training> Name=main_level/agent, Worker=0, Episode=262, Total reward=20.14, Steps=7577, Training iteration=13"
2019-10-02 10:37:02.117,"Training> Name=main_level/agent, Worker=0, Episode=261, Total reward=21.92, Steps=7540, Training iteration=13"
2019-10-02 10:37:00.116,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_13.pb
2019-10-02 10:36:57.115,Uploaded 3 files for checkpoint 13
2019-10-02 10:36:57.115,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:36:57.115,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:36:38.110,Checkpoint> Saving in path=['./checkpoint/13_Step-7498.ckpt']
2019-10-02 10:36:37.109,"Policy training> Surrogate loss=-0.10352487117052078, KL divergence=0.02326018735766411, Entropy=2.286271572113037, training epoch=9, learning_rate=3e-05"
2019-10-02 10:36:33.846,"Policy training> Surrogate loss=-0.08992298692464828, KL divergence=0.026812704280018806, Entropy=2.2728066444396973, training epoch=8, learning_rate=3e-05"
2019-10-02 10:36:29.642,"Policy training> Surrogate loss=-0.09888904541730881, KL divergence=0.022472601383924484, Entropy=2.278749942779541, training epoch=7, learning_rate=3e-05"
2019-10-02 10:36:25.162,"Policy training> Surrogate loss=-0.08436526358127594, KL divergence=0.02279765158891678, Entropy=2.2793259620666504, training epoch=6, learning_rate=3e-05"
2019-10-02 10:36:21.137,"Policy training> Surrogate loss=-0.08344867825508118, KL divergence=0.018104998394846916, Entropy=2.278646945953369, training epoch=5, learning_rate=3e-05"
2019-10-02 10:36:17.128,"Policy training> Surrogate loss=-0.07037963718175888, KL divergence=0.02046293206512928, Entropy=2.285351514816284, training epoch=4, learning_rate=3e-05"
2019-10-02 10:36:12.534,"Policy training> Surrogate loss=-0.05585644766688347, KL divergence=0.019554035738110542, Entropy=2.287677049636841, training epoch=3, learning_rate=3e-05"
2019-10-02 10:36:07.506,"Policy training> Surrogate loss=-0.05662006884813309, KL divergence=0.013953830115497112, Entropy=2.2758612632751465, training epoch=2, learning_rate=3e-05"
2019-10-02 10:36:03.150,"Policy training> Surrogate loss=-0.017309416085481644, KL divergence=0.016228090971708298, Entropy=2.3012197017669678, training epoch=1, learning_rate=3e-05"
2019-10-02 10:35:59.244,"Policy training> Surrogate loss=0.0014726135414093733, KL divergence=0.011739335022866726, Entropy=2.3217318058013916, training epoch=0, learning_rate=3e-05"
2019-10-02 10:35:54.213,"Training> Name=main_level/agent, Worker=0, Episode=260, Total reward=30.16, Steps=7498, Training iteration=12"
2019-10-02 10:35:50.212,"Training> Name=main_level/agent, Worker=0, Episode=259, Total reward=2.78, Steps=7447, Training iteration=12"
2019-10-02 10:35:49.212,"Training> Name=main_level/agent, Worker=0, Episode=258, Total reward=10.39, Steps=7435, Training iteration=12"
2019-10-02 10:35:47.211,"Training> Name=main_level/agent, Worker=0, Episode=257, Total reward=23.83, Steps=7410, Training iteration=12"
2019-10-02 10:35:44.210,"Training> Name=main_level/agent, Worker=0, Episode=256, Total reward=17.11, Steps=7364, Training iteration=12"
2019-10-02 10:35:41.209,"Training> Name=main_level/agent, Worker=0, Episode=255, Total reward=50.7, Steps=7332, Training iteration=12"
2019-10-02 10:35:33.207,"Training> Name=main_level/agent, Worker=0, Episode=254, Total reward=17.79, Steps=7221, Training iteration=12"
2019-10-02 10:35:30.206,"Training> Name=main_level/agent, Worker=0, Episode=253, Total reward=15.61, Steps=7183, Training iteration=12"
2019-10-02 10:35:26.205,"Training> Name=main_level/agent, Worker=0, Episode=252, Total reward=16.15, Steps=7138, Training iteration=12"
2019-10-02 10:35:23.204,"Training> Name=main_level/agent, Worker=0, Episode=251, Total reward=29.57, Steps=7102, Training iteration=12"
2019-10-02 10:35:19.203,"Training> Name=main_level/agent, Worker=0, Episode=250, Total reward=21.89, Steps=7036, Training iteration=12"
2019-10-02 10:35:15.201,"Training> Name=main_level/agent, Worker=0, Episode=249, Total reward=11.39, Steps=6983, Training iteration=12"
2019-10-02 10:35:12.200,"Training> Name=main_level/agent, Worker=0, Episode=248, Total reward=8.32, Steps=6950, Training iteration=12"
2019-10-02 10:35:10.200,"Training> Name=main_level/agent, Worker=0, Episode=247, Total reward=32.71, Steps=6926, Training iteration=12"
2019-10-02 10:35:05.198,"Training> Name=main_level/agent, Worker=0, Episode=246, Total reward=8.92, Steps=6865, Training iteration=12"
2019-10-02 10:35:03.198,"Training> Name=main_level/agent, Worker=0, Episode=245, Total reward=16.03, Steps=6845, Training iteration=12"
2019-10-02 10:35:00.197,"Training> Name=main_level/agent, Worker=0, Episode=244, Total reward=13.13, Steps=6801, Training iteration=12"
2019-10-02 10:34:58.196,"Training> Name=main_level/agent, Worker=0, Episode=243, Total reward=17.66, Steps=6770, Training iteration=12"
2019-10-02 10:34:55.195,"Training> Name=main_level/agent, Worker=0, Episode=242, Total reward=26.16, Steps=6733, Training iteration=12"
2019-10-02 10:34:50.194,"Training> Name=main_level/agent, Worker=0, Episode=241, Total reward=22.62, Steps=6676, Training iteration=12"
2019-10-02 10:34:45.192,Uploaded 3 files for checkpoint 12
2019-10-02 10:34:45.192,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:34:45.192,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:34:45.192,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_12.pb
2019-10-02 10:34:41.191,"Policy training> Surrogate loss=-0.08138607442378998, KL divergence=0.024668840691447258, Entropy=2.3297877311706543, training epoch=9, learning_rate=3e-05"
2019-10-02 10:34:41.191,Checkpoint> Saving in path=['./checkpoint/12_Step-6637.ckpt']
2019-10-02 10:34:38.018,"Policy training> Surrogate loss=-0.0842917412519455, KL divergence=0.023389605805277824, Entropy=2.355607509613037, training epoch=8, learning_rate=3e-05"
2019-10-02 10:34:34.571,"Policy training> Surrogate loss=-0.08837578445672989, KL divergence=0.024194449186325073, Entropy=2.3381781578063965, training epoch=7, learning_rate=3e-05"
2019-10-02 10:34:31.450,"Policy training> Surrogate loss=-0.08599818497896194, KL divergence=0.022139426320791245, Entropy=2.336859703063965, training epoch=6, learning_rate=3e-05"
2019-10-02 10:34:29.426,"Policy training> Surrogate loss=-0.09243787080049515, KL divergence=0.015745973214507103, Entropy=2.339425802230835, training epoch=5, learning_rate=3e-05"
2019-10-02 10:34:24.791,"Policy training> Surrogate loss=-0.059513069689273834, KL divergence=0.018335392698645592, Entropy=2.355652332305908, training epoch=4, learning_rate=3e-05"
2019-10-02 10:34:21.422,"Policy training> Surrogate loss=-0.06183777377009392, KL divergence=0.01552932895720005, Entropy=2.3604509830474854, training epoch=3, learning_rate=3e-05"
2019-10-02 10:34:18.255,"Policy training> Surrogate loss=-0.041893426328897476, KL divergence=0.013929192908108234, Entropy=2.351466655731201, training epoch=2, learning_rate=3e-05"
2019-10-02 10:34:14.711,"Policy training> Surrogate loss=-0.02188369631767273, KL divergence=0.025151100009679794, Entropy=2.383899211883545, training epoch=1, learning_rate=3e-05"
2019-10-02 10:34:11.564,"Policy training> Surrogate loss=-0.003007379127666354, KL divergence=0.004575464874505997, Entropy=2.3806509971618652, training epoch=0, learning_rate=3e-05"
2019-10-02 10:34:08.373,"Training> Name=main_level/agent, Worker=0, Episode=240, Total reward=21.15, Steps=6637, Training iteration=11"
2019-10-02 10:34:04.369,"Training> Name=main_level/agent, Worker=0, Episode=239, Total reward=2.54, Steps=6589, Training iteration=11"
2019-10-02 10:34:03.368,"Training> Name=main_level/agent, Worker=0, Episode=238, Total reward=28.08, Steps=6577, Training iteration=11"
2019-10-02 10:33:59.367,"Training> Name=main_level/agent, Worker=0, Episode=237, Total reward=20.45, Steps=6533, Training iteration=11"
2019-10-02 10:33:57.367,"Training> Name=main_level/agent, Worker=0, Episode=236, Total reward=8.76, Steps=6500, Training iteration=11"
2019-10-02 10:33:55.366,"Training> Name=main_level/agent, Worker=0, Episode=235, Total reward=11.9, Steps=6479, Training iteration=11"
2019-10-02 10:33:52.365,"Training> Name=main_level/agent, Worker=0, Episode=234, Total reward=35.45, Steps=6450, Training iteration=11"
2019-10-02 10:33:47.363,"Training> Name=main_level/agent, Worker=0, Episode=233, Total reward=10.34, Steps=6381, Training iteration=11"
2019-10-02 10:33:45.363,"Training> Name=main_level/agent, Worker=0, Episode=232, Total reward=17.24, Steps=6349, Training iteration=11"
2019-10-02 10:33:42.362,"Training> Name=main_level/agent, Worker=0, Episode=231, Total reward=19.0, Steps=6309, Training iteration=11"
2019-10-02 10:33:39.361,"Training> Name=main_level/agent, Worker=0, Episode=230, Total reward=5.19, Steps=6273, Training iteration=11"
2019-10-02 10:33:37.360,"Training> Name=main_level/agent, Worker=0, Episode=229, Total reward=16.01, Steps=6256, Training iteration=11"
2019-10-02 10:33:34.359,"Training> Name=main_level/agent, Worker=0, Episode=228, Total reward=11.17, Steps=6221, Training iteration=11"
2019-10-02 10:33:32.358,"Training> Name=main_level/agent, Worker=0, Episode=227, Total reward=8.12, Steps=6198, Training iteration=11"
2019-10-02 10:33:30.358,"Training> Name=main_level/agent, Worker=0, Episode=226, Total reward=14.96, Steps=6179, Training iteration=11"
2019-10-02 10:33:28.357,"Training> Name=main_level/agent, Worker=0, Episode=225, Total reward=7.26, Steps=6147, Training iteration=11"
2019-10-02 10:33:26.356,"Training> Name=main_level/agent, Worker=0, Episode=224, Total reward=16.83, Steps=6127, Training iteration=11"
2019-10-02 10:33:23.355,"Training> Name=main_level/agent, Worker=0, Episode=223, Total reward=16.33, Steps=6092, Training iteration=11"
2019-10-02 10:33:21.355,"Training> Name=main_level/agent, Worker=0, Episode=222, Total reward=29.16, Steps=6059, Training iteration=11"
2019-10-02 10:33:17.353,"Training> Name=main_level/agent, Worker=0, Episode=221, Total reward=21.08, Steps=6010, Training iteration=11"
2019-10-02 10:33:12.352,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_11.pb
2019-10-02 10:33:11.351,Uploaded 3 files for checkpoint 11
2019-10-02 10:33:11.351,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:33:11.351,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:33:09.351,"Policy training> Surrogate loss=-0.08177433907985687, KL divergence=0.020608220249414444, Entropy=2.3428046703338623, training epoch=9, learning_rate=3e-05"
2019-10-02 10:33:09.351,Checkpoint> Saving in path=['./checkpoint/11_Step-5969.ckpt']
2019-10-02 10:33:05.336,"Policy training> Surrogate loss=-0.07973194122314453, KL divergence=0.022029560059309006, Entropy=2.350236415863037, training epoch=8, learning_rate=3e-05"
2019-10-02 10:33:02.900,"Policy training> Surrogate loss=-0.07206103950738907, KL divergence=0.019664745777845383, Entropy=2.354151487350464, training epoch=7, learning_rate=3e-05"
2019-10-02 10:32:58.843,"Policy training> Surrogate loss=-0.04494889825582504, KL divergence=0.023175453767180443, Entropy=2.3677005767822266, training epoch=6, learning_rate=3e-05"
2019-10-02 10:32:55.776,"Policy training> Surrogate loss=-0.04374494403600693, KL divergence=0.02655031345784664, Entropy=2.366994857788086, training epoch=5, learning_rate=3e-05"
2019-10-02 10:32:53.638,"Policy training> Surrogate loss=-0.0741683840751648, KL divergence=0.02011820301413536, Entropy=2.3464839458465576, training epoch=4, learning_rate=3e-05"
2019-10-02 10:32:50.214,"Policy training> Surrogate loss=-0.05192955210804939, KL divergence=0.021088074892759323, Entropy=2.340120792388916, training epoch=3, learning_rate=3e-05"
2019-10-02 10:32:46.576,"Policy training> Surrogate loss=-0.02809607982635498, KL divergence=0.012196168303489685, Entropy=2.3931407928466797, training epoch=2, learning_rate=3e-05"
2019-10-02 10:32:42.810,"Policy training> Surrogate loss=-0.011468467302620411, KL divergence=0.021975554525852203, Entropy=2.3481154441833496, training epoch=1, learning_rate=3e-05"
2019-10-02 10:32:39.790,"Policy training> Surrogate loss=0.01909402571618557, KL divergence=0.009054646827280521, Entropy=2.373863697052002, training epoch=0, learning_rate=3e-05"
2019-10-02 10:32:35.305,"Training> Name=main_level/agent, Worker=0, Episode=220, Total reward=12.27, Steps=5969, Training iteration=10"
2019-10-02 10:32:33.304,"Training> Name=main_level/agent, Worker=0, Episode=219, Total reward=5.37, Steps=5946, Training iteration=10"
2019-10-02 10:32:32.304,"Training> Name=main_level/agent, Worker=0, Episode=218, Total reward=18.82, Steps=5928, Training iteration=10"
2019-10-02 10:32:29.303,"Training> Name=main_level/agent, Worker=0, Episode=217, Total reward=24.2, Steps=5890, Training iteration=10"
2019-10-02 10:32:25.302,"Training> Name=main_level/agent, Worker=0, Episode=216, Total reward=15.99, Steps=5845, Training iteration=10"
2019-10-02 10:32:22.301,"Training> Name=main_level/agent, Worker=0, Episode=215, Total reward=15.09, Steps=5810, Training iteration=10"
2019-10-02 10:32:20.300,"Training> Name=main_level/agent, Worker=0, Episode=214, Total reward=18.29, Steps=5780, Training iteration=10"
2019-10-02 10:32:17.299,"Training> Name=main_level/agent, Worker=0, Episode=213, Total reward=11.17, Steps=5736, Training iteration=10"
2019-10-02 10:32:14.298,"Training> Name=main_level/agent, Worker=0, Episode=212, Total reward=17.5, Steps=5711, Training iteration=10"
2019-10-02 10:32:11.297,"Training> Name=main_level/agent, Worker=0, Episode=211, Total reward=12.9, Steps=5668, Training iteration=10"
2019-10-02 10:32:09.297,"Training> Name=main_level/agent, Worker=0, Episode=210, Total reward=26.98, Steps=5642, Training iteration=10"
2019-10-02 10:32:04.295,"Training> Name=main_level/agent, Worker=0, Episode=209, Total reward=25.66, Steps=5577, Training iteration=10"
2019-10-02 10:31:59.294,"Training> Name=main_level/agent, Worker=0, Episode=208, Total reward=12.05, Steps=5514, Training iteration=10"
2019-10-02 10:31:57.293,"Training> Name=main_level/agent, Worker=0, Episode=207, Total reward=3.18, Steps=5490, Training iteration=10"
2019-10-02 10:31:56.293,"Training> Name=main_level/agent, Worker=0, Episode=206, Total reward=15.06, Steps=5479, Training iteration=10"
2019-10-02 10:31:52.291,"Training> Name=main_level/agent, Worker=0, Episode=205, Total reward=5.25, Steps=5429, Training iteration=10"
2019-10-02 10:31:51.291,"Training> Name=main_level/agent, Worker=0, Episode=204, Total reward=18.49, Steps=5414, Training iteration=10"
2019-10-02 10:31:47.290,"Training> Name=main_level/agent, Worker=0, Episode=203, Total reward=10.67, Steps=5368, Training iteration=10"
2019-10-02 10:31:45.289,"Training> Name=main_level/agent, Worker=0, Episode=202, Total reward=25.68, Steps=5348, Training iteration=10"
2019-10-02 10:31:41.288,"Training> Name=main_level/agent, Worker=0, Episode=201, Total reward=7.92, Steps=5297, Training iteration=10"
2019-10-02 10:31:38.287,Uploaded 3 files for checkpoint 10
2019-10-02 10:31:38.287,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:31:38.287,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:31:38.287,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_10.pb
2019-10-02 10:31:33.285,Checkpoint> Saving in path=['./checkpoint/10_Step-5281.ckpt']
2019-10-02 10:31:32.285,"Policy training> Surrogate loss=-0.08919263631105423, KL divergence=0.02589365653693676, Entropy=2.4041805267333984, training epoch=9, learning_rate=3e-05"
2019-10-02 10:31:30.266,"Policy training> Surrogate loss=-0.09353888779878616, KL divergence=0.02093786746263504, Entropy=2.4049623012542725, training epoch=8, learning_rate=3e-05"
2019-10-02 10:31:25.226,"Policy training> Surrogate loss=-0.09972069412469864, KL divergence=0.020380808040499687, Entropy=2.4091880321502686, training epoch=7, learning_rate=3e-05"
2019-10-02 10:31:22.060,"Policy training> Surrogate loss=-0.09838832169771194, KL divergence=0.019356362521648407, Entropy=2.3989782333374023, training epoch=6, learning_rate=3e-05"
2019-10-02 10:31:18.559,"Policy training> Surrogate loss=-0.07353872060775757, KL divergence=0.016090862452983856, Entropy=2.416158437728882, training epoch=5, learning_rate=3e-05"
2019-10-02 10:31:14.507,"Policy training> Surrogate loss=-0.0736144632101059, KL divergence=0.019538981840014458, Entropy=2.395521640777588, training epoch=4, learning_rate=3e-05"
2019-10-02 10:31:11.245,"Policy training> Surrogate loss=-0.06879454106092453, KL divergence=0.01642746850848198, Entropy=2.4022109508514404, training epoch=3, learning_rate=3e-05"
2019-10-02 10:31:08.001,"Policy training> Surrogate loss=-0.05928235873579979, KL divergence=0.020915478467941284, Entropy=2.381702423095703, training epoch=2, learning_rate=3e-05"
2019-10-02 10:31:03.999,"Policy training> Surrogate loss=-0.025401322171092033, KL divergence=0.021121742203831673, Entropy=2.377500295639038, training epoch=1, learning_rate=3e-05"
2019-10-02 10:31:00.984,"Policy training> Surrogate loss=0.010396427474915981, KL divergence=0.0068755908869206905, Entropy=2.4336087703704834, training epoch=0, learning_rate=3e-05"
2019-10-02 10:30:56.634,"Training> Name=main_level/agent, Worker=0, Episode=200, Total reward=14.61, Steps=5281, Training iteration=9"
2019-10-02 10:30:53.630,"Training> Name=main_level/agent, Worker=0, Episode=199, Total reward=2.07, Steps=5254, Training iteration=9"
2019-10-02 10:30:52.629,"Training> Name=main_level/agent, Worker=0, Episode=198, Total reward=9.83, Steps=5243, Training iteration=9"
2019-10-02 10:30:49.629,"Training> Name=main_level/agent, Worker=0, Episode=197, Total reward=17.54, Steps=5221, Training iteration=9"
2019-10-02 10:30:47.628,"Training> Name=main_level/agent, Worker=0, Episode=196, Total reward=14.67, Steps=5192, Training iteration=9"
2019-10-02 10:30:45.627,"Training> Name=main_level/agent, Worker=0, Episode=195, Total reward=9.49, Steps=5165, Training iteration=9"
2019-10-02 10:30:42.626,"Training> Name=main_level/agent, Worker=0, Episode=194, Total reward=18.64, Steps=5133, Training iteration=9"
2019-10-02 10:30:39.625,"Training> Name=main_level/agent, Worker=0, Episode=193, Total reward=29.8, Steps=5092, Training iteration=9"
2019-10-02 10:30:33.624,"Training> Name=main_level/agent, Worker=0, Episode=192, Total reward=8.93, Steps=5013, Training iteration=9"
2019-10-02 10:30:30.623,"Training> Name=main_level/agent, Worker=0, Episode=191, Total reward=42.84, Steps=4977, Training iteration=9"
2019-10-02 10:30:24.621,"Training> Name=main_level/agent, Worker=0, Episode=190, Total reward=5.66, Steps=4892, Training iteration=9"
2019-10-02 10:30:23.620,"Training> Name=main_level/agent, Worker=0, Episode=189, Total reward=5.64, Steps=4877, Training iteration=9"
2019-10-02 10:30:21.620,"Training> Name=main_level/agent, Worker=0, Episode=188, Total reward=6.1, Steps=4859, Training iteration=9"
2019-10-02 10:30:18.619,"Training> Name=main_level/agent, Worker=0, Episode=187, Total reward=33.84, Steps=4826, Training iteration=9"
2019-10-02 10:30:12.617,"Training> Name=main_level/agent, Worker=0, Episode=186, Total reward=7.27, Steps=4747, Training iteration=9"
2019-10-02 10:30:10.616,"Training> Name=main_level/agent, Worker=0, Episode=185, Total reward=13.08, Steps=4726, Training iteration=9"
2019-10-02 10:30:08.616,"Training> Name=main_level/agent, Worker=0, Episode=184, Total reward=14.19, Steps=4696, Training iteration=9"
2019-10-02 10:30:05.615,"Training> Name=main_level/agent, Worker=0, Episode=183, Total reward=19.57, Steps=4662, Training iteration=9"
2019-10-02 10:30:02.614,"Training> Name=main_level/agent, Worker=0, Episode=182, Total reward=12.8, Steps=4623, Training iteration=9"
2019-10-02 10:30:00.613,"Training> Name=main_level/agent, Worker=0, Episode=181, Total reward=21.82, Steps=4601, Training iteration=9"
2019-10-02 10:29:56.612,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_9.pb
2019-10-02 10:29:55.611,Uploaded 3 files for checkpoint 9
2019-10-02 10:29:55.611,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:29:55.611,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:29:53.611,"Policy training> Surrogate loss=-0.10569453984498978, KL divergence=0.025125866755843163, Entropy=2.4617087841033936, training epoch=9, learning_rate=3e-05"
2019-10-02 10:29:53.611,Checkpoint> Saving in path=['./checkpoint/9_Step-4560.ckpt']
2019-10-02 10:29:51.454,"Policy training> Surrogate loss=-0.09202393889427185, KL divergence=0.0227133110165596, Entropy=2.4677979946136475, training epoch=8, learning_rate=3e-05"
2019-10-02 10:29:47.775,"Policy training> Surrogate loss=-0.08588788658380508, KL divergence=0.019057219848036766, Entropy=2.457979202270508, training epoch=7, learning_rate=3e-05"
2019-10-02 10:29:44.473,"Policy training> Surrogate loss=-0.08240413665771484, KL divergence=0.02107423171401024, Entropy=2.4684133529663086, training epoch=6, learning_rate=3e-05"
2019-10-02 10:29:41.393,"Policy training> Surrogate loss=-0.07936186343431473, KL divergence=0.022889675572514534, Entropy=2.4992284774780273, training epoch=5, learning_rate=3e-05"
2019-10-02 10:29:39.326,"Policy training> Surrogate loss=-0.05854295939207077, KL divergence=0.01582200638949871, Entropy=2.47052264213562, training epoch=4, learning_rate=3e-05"
2019-10-02 10:29:35.752,"Policy training> Surrogate loss=-0.047651417553424835, KL divergence=0.017113694921135902, Entropy=2.4811129570007324, training epoch=3, learning_rate=3e-05"
2019-10-02 10:29:32.553,"Policy training> Surrogate loss=-0.034148190170526505, KL divergence=0.015522420406341553, Entropy=2.4973461627960205, training epoch=2, learning_rate=3e-05"
2019-10-02 10:29:30.116,"Policy training> Surrogate loss=-0.028495702892541885, KL divergence=0.025236930698156357, Entropy=2.5034596920013428, training epoch=1, learning_rate=3e-05"
2019-10-02 10:29:26.110,"Policy training> Surrogate loss=-0.003941236063838005, KL divergence=0.011981132440268993, Entropy=2.5111541748046875, training epoch=0, learning_rate=3e-05"
2019-10-02 10:29:23.073,"Training> Name=main_level/agent, Worker=0, Episode=180, Total reward=6.47, Steps=4560, Training iteration=8"
2019-10-02 10:29:21.046,"Training> Name=main_level/agent, Worker=0, Episode=179, Total reward=5.15, Steps=4544, Training iteration=8"
2019-10-02 10:29:20.046,"Training> Name=main_level/agent, Worker=0, Episode=178, Total reward=31.84, Steps=4528, Training iteration=8"
2019-10-02 10:29:16.045,"Training> Name=main_level/agent, Worker=0, Episode=177, Total reward=24.41, Steps=4473, Training iteration=8"
2019-10-02 10:29:12.044,"Training> Name=main_level/agent, Worker=0, Episode=176, Total reward=10.86, Steps=4426, Training iteration=8"
2019-10-02 10:29:10.043,"Training> Name=main_level/agent, Worker=0, Episode=175, Total reward=18.39, Steps=4405, Training iteration=8"
2019-10-02 10:29:07.042,"Training> Name=main_level/agent, Worker=0, Episode=174, Total reward=12.56, Steps=4369, Training iteration=8"
2019-10-02 10:29:05.041,"Training> Name=main_level/agent, Worker=0, Episode=173, Total reward=20.23, Steps=4341, Training iteration=8"
2019-10-02 10:29:01.040,"Training> Name=main_level/agent, Worker=0, Episode=172, Total reward=15.17, Steps=4289, Training iteration=8"
2019-10-02 10:28:58.040,"Training> Name=main_level/agent, Worker=0, Episode=171, Total reward=8.38, Steps=4253, Training iteration=8"
2019-10-02 10:28:57.039,"Training> Name=main_level/agent, Worker=0, Episode=170, Total reward=8.07, Steps=4237, Training iteration=8"
2019-10-02 10:28:55.038,"Training> Name=main_level/agent, Worker=0, Episode=169, Total reward=23.1, Steps=4220, Training iteration=8"
2019-10-02 10:28:50.037,"Training> Name=main_level/agent, Worker=0, Episode=168, Total reward=6.23, Steps=4153, Training iteration=8"
2019-10-02 10:28:48.036,"Training> Name=main_level/agent, Worker=0, Episode=167, Total reward=7.02, Steps=4127, Training iteration=8"
2019-10-02 10:28:46.036,"Training> Name=main_level/agent, Worker=0, Episode=166, Total reward=3.72, Steps=4110, Training iteration=8"
2019-10-02 10:28:45.035,"Training> Name=main_level/agent, Worker=0, Episode=165, Total reward=8.86, Steps=4094, Training iteration=8"
2019-10-02 10:28:43.035,"Training> Name=main_level/agent, Worker=0, Episode=164, Total reward=9.29, Steps=4073, Training iteration=8"
2019-10-02 10:28:41.034,"Training> Name=main_level/agent, Worker=0, Episode=163, Total reward=20.26, Steps=4054, Training iteration=8"
2019-10-02 10:28:38.033,"Training> Name=main_level/agent, Worker=0, Episode=162, Total reward=9.05, Steps=4009, Training iteration=8"
2019-10-02 10:28:36.033,"Training> Name=main_level/agent, Worker=0, Episode=161, Total reward=11.73, Steps=3995, Training iteration=8"
2019-10-02 10:28:32.031,Uploaded 3 files for checkpoint 8
2019-10-02 10:28:32.031,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:28:32.031,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:28:32.031,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_8.pb
2019-10-02 10:28:30.031,Checkpoint> Saving in path=['./checkpoint/8_Step-3972.ckpt']
2019-10-02 10:28:29.030,"Policy training> Surrogate loss=-0.09645223617553711, KL divergence=0.019045783206820488, Entropy=2.49902606010437, training epoch=9, learning_rate=3e-05"
2019-10-02 10:28:25.998,"Policy training> Surrogate loss=-0.08636488020420074, KL divergence=0.018414871767163277, Entropy=2.4871013164520264, training epoch=8, learning_rate=3e-05"
2019-10-02 10:28:23.180,"Policy training> Surrogate loss=-0.0839853435754776, KL divergence=0.017510391771793365, Entropy=2.502912998199463, training epoch=7, learning_rate=3e-05"
2019-10-02 10:28:19.136,"Policy training> Surrogate loss=-0.07486657798290253, KL divergence=0.019847948104143143, Entropy=2.4930877685546875, training epoch=6, learning_rate=3e-05"
2019-10-02 10:28:17.038,"Policy training> Surrogate loss=-0.06171317771077156, KL divergence=0.013895124197006226, Entropy=2.5212206840515137, training epoch=5, learning_rate=3e-05"
2019-10-02 10:28:12.954,"Policy training> Surrogate loss=-0.06687261164188385, KL divergence=0.017380034551024437, Entropy=2.5024027824401855, training epoch=4, learning_rate=3e-05"
2019-10-02 10:28:10.930,"Policy training> Surrogate loss=-0.04811089485883713, KL divergence=0.013638781383633614, Entropy=2.5158562660217285, training epoch=3, learning_rate=3e-05"
2019-10-02 10:28:06.222,"Policy training> Surrogate loss=-0.041313380002975464, KL divergence=0.013866914436221123, Entropy=2.5255887508392334, training epoch=2, learning_rate=3e-05"
2019-10-02 10:28:03.826,"Policy training> Surrogate loss=-0.02720894291996956, KL divergence=0.015156972222030163, Entropy=2.521742105484009, training epoch=1, learning_rate=3e-05"
2019-10-02 10:28:00.187,"Policy training> Surrogate loss=0.017823006957769394, KL divergence=0.006229264196008444, Entropy=2.4987282752990723, training epoch=0, learning_rate=3e-05"
2019-10-02 10:27:56.087,"Training> Name=main_level/agent, Worker=0, Episode=160, Total reward=22.05, Steps=3972, Training iteration=7"
2019-10-02 10:27:53.080,"Training> Name=main_level/agent, Worker=0, Episode=159, Total reward=16.25, Steps=3924, Training iteration=7"
2019-10-02 10:27:50.079,"Training> Name=main_level/agent, Worker=0, Episode=158, Total reward=9.73, Steps=3888, Training iteration=7"
2019-10-02 10:27:48.078,"Training> Name=main_level/agent, Worker=0, Episode=157, Total reward=24.45, Steps=3865, Training iteration=7"
2019-10-02 10:27:44.077,"Training> Name=main_level/agent, Worker=0, Episode=156, Total reward=17.02, Steps=3820, Training iteration=7"
2019-10-02 10:27:41.076,"Training> Name=main_level/agent, Worker=0, Episode=155, Total reward=5.6, Steps=3780, Training iteration=7"
2019-10-02 10:27:39.075,"Training> Name=main_level/agent, Worker=0, Episode=154, Total reward=10.34, Steps=3762, Training iteration=7"
2019-10-02 10:27:37.075,"Training> Name=main_level/agent, Worker=0, Episode=153, Total reward=9.54, Steps=3743, Training iteration=7"
2019-10-02 10:27:35.074,"Training> Name=main_level/agent, Worker=0, Episode=152, Total reward=22.55, Steps=3715, Training iteration=7"
2019-10-02 10:27:31.073,"Training> Name=main_level/agent, Worker=0, Episode=151, Total reward=20.3, Steps=3653, Training iteration=7"
2019-10-02 10:27:26.071,"Training> Name=main_level/agent, Worker=0, Episode=150, Total reward=12.89, Steps=3603, Training iteration=7"
2019-10-02 10:27:24.071,"Training> Name=main_level/agent, Worker=0, Episode=149, Total reward=6.55, Steps=3575, Training iteration=7"
2019-10-02 10:27:22.070,"Training> Name=main_level/agent, Worker=0, Episode=148, Total reward=10.13, Steps=3555, Training iteration=7"
2019-10-02 10:27:20.069,"Training> Name=main_level/agent, Worker=0, Episode=147, Total reward=14.68, Steps=3525, Training iteration=7"
2019-10-02 10:27:17.068,"Training> Name=main_level/agent, Worker=0, Episode=146, Total reward=8.82, Steps=3489, Training iteration=7"
2019-10-02 10:27:15.068,"Training> Name=main_level/agent, Worker=0, Episode=145, Total reward=4.99, Steps=3469, Training iteration=7"
2019-10-02 10:27:13.067,"Training> Name=main_level/agent, Worker=0, Episode=144, Total reward=9.37, Steps=3451, Training iteration=7"
2019-10-02 10:27:12.067,"Training> Name=main_level/agent, Worker=0, Episode=143, Total reward=24.84, Steps=3430, Training iteration=7"
2019-10-02 10:27:08.065,"Training> Name=main_level/agent, Worker=0, Episode=142, Total reward=23.96, Steps=3385, Training iteration=7"
2019-10-02 10:27:05.065,"Training> Name=main_level/agent, Worker=0, Episode=141, Total reward=15.45, Steps=3348, Training iteration=7"
2019-10-02 10:27:01.064,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_7.pb
2019-10-02 10:27:00.063,Uploaded 3 files for checkpoint 7
2019-10-02 10:27:00.063,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:27:00.063,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:26:58.063,Checkpoint> Saving in path=['./checkpoint/7_Step-3309.ckpt']
2019-10-02 10:26:58.062,"Policy training> Surrogate loss=-0.10323888808488846, KL divergence=0.02122727781534195, Entropy=2.533529281616211, training epoch=9, learning_rate=3e-05"
2019-10-02 10:26:55.050,"Policy training> Surrogate loss=-0.08262944221496582, KL divergence=0.023411087691783905, Entropy=2.540191173553467, training epoch=8, learning_rate=3e-05"
2019-10-02 10:26:53.026,"Policy training> Surrogate loss=-0.0873485878109932, KL divergence=0.021601712331175804, Entropy=2.5443623065948486, training epoch=7, learning_rate=3e-05"
2019-10-02 10:26:49.766,"Policy training> Surrogate loss=-0.08267883956432343, KL divergence=0.01856384426355362, Entropy=2.5332882404327393, training epoch=6, learning_rate=3e-05"
2019-10-02 10:26:46.721,"Policy training> Surrogate loss=-0.052130356431007385, KL divergence=0.026602227240800858, Entropy=2.542693853378296, training epoch=5, learning_rate=3e-05"
2019-10-02 10:26:44.892,"Policy training> Surrogate loss=-0.03724314272403717, KL divergence=0.027460400015115738, Entropy=2.545008420944214, training epoch=4, learning_rate=3e-05"
2019-10-02 10:26:41.580,"Policy training> Surrogate loss=-0.024609871208667755, KL divergence=0.0270182304084301, Entropy=2.5423433780670166, training epoch=3, learning_rate=3e-05"
2019-10-02 10:26:39.456,"Policy training> Surrogate loss=-0.05575862154364586, KL divergence=0.021612850949168205, Entropy=2.5555055141448975, training epoch=2, learning_rate=3e-05"
2019-10-02 10:26:36.424,"Policy training> Surrogate loss=-0.035604968667030334, KL divergence=0.04121646657586098, Entropy=2.5218305587768555, training epoch=1, learning_rate=3e-05"
2019-10-02 10:26:34.300,"Policy training> Surrogate loss=-0.001586033497005701, KL divergence=0.011162816546857357, Entropy=2.55694317817688, training epoch=0, learning_rate=3e-05"
2019-10-02 10:26:30.024,"Training> Name=main_level/agent, Worker=0, Episode=140, Total reward=16.32, Steps=3309, Training iteration=6"
2019-10-02 10:26:28.023,"Training> Name=main_level/agent, Worker=0, Episode=139, Total reward=2.51, Steps=3277, Training iteration=6"
2019-10-02 10:26:26.022,"Training> Name=main_level/agent, Worker=0, Episode=138, Total reward=10.71, Steps=3264, Training iteration=6"
2019-10-02 10:26:24.022,"Training> Name=main_level/agent, Worker=0, Episode=137, Total reward=19.37, Steps=3240, Training iteration=6"
2019-10-02 10:26:21.021,"Training> Name=main_level/agent, Worker=0, Episode=136, Total reward=41.48, Steps=3201, Training iteration=6"
2019-10-02 10:26:15.019,"Training> Name=main_level/agent, Worker=0, Episode=135, Total reward=4.16, Steps=3117, Training iteration=6"
2019-10-02 10:26:14.019,"Training> Name=main_level/agent, Worker=0, Episode=134, Total reward=13.11, Steps=3106, Training iteration=6"
2019-10-02 10:26:11.018,"Training> Name=main_level/agent, Worker=0, Episode=133, Total reward=8.45, Steps=3071, Training iteration=6"
2019-10-02 10:26:09.017,"Training> Name=main_level/agent, Worker=0, Episode=132, Total reward=6.94, Steps=3046, Training iteration=6"
2019-10-02 10:26:07.016,"Training> Name=main_level/agent, Worker=0, Episode=131, Total reward=12.74, Steps=3030, Training iteration=6"
2019-10-02 10:26:05.016,"Training> Name=main_level/agent, Worker=0, Episode=130, Total reward=5.13, Steps=3005, Training iteration=6"
2019-10-02 10:26:04.015,"Training> Name=main_level/agent, Worker=0, Episode=129, Total reward=14.25, Steps=2989, Training iteration=6"
2019-10-02 10:26:01.014,"Training> Name=main_level/agent, Worker=0, Episode=128, Total reward=12.36, Steps=2952, Training iteration=6"
2019-10-02 10:25:58.013,"Training> Name=main_level/agent, Worker=0, Episode=127, Total reward=7.01, Steps=2918, Training iteration=6"
2019-10-02 10:25:57.013,"Training> Name=main_level/agent, Worker=0, Episode=126, Total reward=4.14, Steps=2902, Training iteration=6"
2019-10-02 10:25:55.012,"Training> Name=main_level/agent, Worker=0, Episode=125, Total reward=6.03, Steps=2886, Training iteration=6"
2019-10-02 10:25:53.012,"Training> Name=main_level/agent, Worker=0, Episode=124, Total reward=17.87, Steps=2869, Training iteration=6"
2019-10-02 10:25:50.011,"Training> Name=main_level/agent, Worker=0, Episode=123, Total reward=6.29, Steps=2831, Training iteration=6"
2019-10-02 10:25:49.010,"Training> Name=main_level/agent, Worker=0, Episode=122, Total reward=13.27, Steps=2814, Training iteration=6"
2019-10-02 10:25:46.010,"Training> Name=main_level/agent, Worker=0, Episode=121, Total reward=21.82, Steps=2784, Training iteration=6"
2019-10-02 10:25:42.008,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_6.pb
2019-10-02 10:25:41.008,Uploaded 3 files for checkpoint 6
2019-10-02 10:25:41.008,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:25:41.008,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:25:39.007,"Policy training> Surrogate loss=-0.10442910343408585, KL divergence=0.02244076505303383, Entropy=2.5633907318115234, training epoch=9, learning_rate=3e-05"
2019-10-02 10:25:39.007,Checkpoint> Saving in path=['./checkpoint/6_Step-2747.ckpt']
2019-10-02 10:25:37.006,"Policy training> Surrogate loss=-0.08751853555440903, KL divergence=0.023570464923977852, Entropy=2.56113600730896, training epoch=8, learning_rate=3e-05"
2019-10-02 10:25:34.962,"Policy training> Surrogate loss=-0.10308472067117691, KL divergence=0.024699201807379723, Entropy=2.5511929988861084, training epoch=7, learning_rate=3e-05"
2019-10-02 10:25:32.747,"Policy training> Surrogate loss=-0.09161107242107391, KL divergence=0.021824803203344345, Entropy=2.554433584213257, training epoch=6, learning_rate=3e-05"
2019-10-02 10:25:30.345,"Policy training> Surrogate loss=-0.08188750594854355, KL divergence=0.01887914165854454, Entropy=2.563688039779663, training epoch=5, learning_rate=3e-05"
2019-10-02 10:25:27.599,"Policy training> Surrogate loss=-0.05088238790631294, KL divergence=0.0219919104129076, Entropy=2.573596954345703, training epoch=4, learning_rate=3e-05"
2019-10-02 10:25:25.572,"Policy training> Surrogate loss=-0.05154084041714668, KL divergence=0.02641230821609497, Entropy=2.5791027545928955, training epoch=3, learning_rate=3e-05"
2019-10-02 10:25:22.527,"Policy training> Surrogate loss=-0.014671345241367817, KL divergence=0.02897304855287075, Entropy=2.5870769023895264, training epoch=2, learning_rate=3e-05"
2019-10-02 10:25:19.944,"Policy training> Surrogate loss=-0.006496974267065525, KL divergence=0.023521944880485535, Entropy=2.5916333198547363, training epoch=1, learning_rate=3e-05"
2019-10-02 10:25:17.896,"Policy training> Surrogate loss=-0.0054925731383264065, KL divergence=0.003939592279493809, Entropy=2.5999560356140137, training epoch=0, learning_rate=3e-05"
2019-10-02 10:25:15.463,"Training> Name=main_level/agent, Worker=0, Episode=120, Total reward=7.72, Steps=2747, Training iteration=5"
2019-10-02 10:25:13.462,"Training> Name=main_level/agent, Worker=0, Episode=119, Total reward=2.27, Steps=2727, Training iteration=5"
2019-10-02 10:25:12.462,"Training> Name=main_level/agent, Worker=0, Episode=118, Total reward=9.76, Steps=2715, Training iteration=5"
2019-10-02 10:25:10.461,"Training> Name=main_level/agent, Worker=0, Episode=117, Total reward=10.95, Steps=2697, Training iteration=5"
2019-10-02 10:25:08.460,"Training> Name=main_level/agent, Worker=0, Episode=116, Total reward=5.4, Steps=2675, Training iteration=5"
2019-10-02 10:25:06.460,"Training> Name=main_level/agent, Worker=0, Episode=115, Total reward=5.05, Steps=2659, Training iteration=5"
2019-10-02 10:25:05.459,"Training> Name=main_level/agent, Worker=0, Episode=114, Total reward=16.83, Steps=2641, Training iteration=5"
2019-10-02 10:25:02.458,"Training> Name=main_level/agent, Worker=0, Episode=113, Total reward=6.0, Steps=2603, Training iteration=5"
2019-10-02 10:25:00.458,"Training> Name=main_level/agent, Worker=0, Episode=112, Total reward=8.32, Steps=2577, Training iteration=5"
2019-10-02 10:24:58.457,"Training> Name=main_level/agent, Worker=0, Episode=111, Total reward=12.83, Steps=2558, Training iteration=5"
2019-10-02 10:24:55.456,"Training> Name=main_level/agent, Worker=0, Episode=110, Total reward=6.06, Steps=2523, Training iteration=5"
2019-10-02 10:24:53.455,"Training> Name=main_level/agent, Worker=0, Episode=109, Total reward=13.53, Steps=2507, Training iteration=5"
2019-10-02 10:24:51.455,"Training> Name=main_level/agent, Worker=0, Episode=108, Total reward=8.14, Steps=2476, Training iteration=5"
2019-10-02 10:24:49.454,"Training> Name=main_level/agent, Worker=0, Episode=107, Total reward=5.78, Steps=2455, Training iteration=5"
2019-10-02 10:24:48.454,"Training> Name=main_level/agent, Worker=0, Episode=106, Total reward=5.71, Steps=2441, Training iteration=5"
2019-10-02 10:24:46.453,"Training> Name=main_level/agent, Worker=0, Episode=105, Total reward=3.95, Steps=2425, Training iteration=5"
2019-10-02 10:24:45.453,"Training> Name=main_level/agent, Worker=0, Episode=104, Total reward=13.16, Steps=2410, Training iteration=5"
2019-10-02 10:24:42.452,"Training> Name=main_level/agent, Worker=0, Episode=103, Total reward=15.42, Steps=2381, Training iteration=5"
2019-10-02 10:24:40.451,"Training> Name=main_level/agent, Worker=0, Episode=102, Total reward=25.19, Steps=2358, Training iteration=5"
2019-10-02 10:24:37.450,"Training> Name=main_level/agent, Worker=0, Episode=101, Total reward=21.67, Steps=2313, Training iteration=5"
2019-10-02 10:24:33.449,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_5.pb
2019-10-02 10:24:32.448,Uploaded 3 files for checkpoint 5
2019-10-02 10:24:32.448,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:24:32.448,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:24:30.447,"Policy training> Surrogate loss=-0.0874607190489769, KL divergence=0.019624046981334686, Entropy=2.600583076477051, training epoch=9, learning_rate=3e-05"
2019-10-02 10:24:30.447,Checkpoint> Saving in path=['./checkpoint/5_Step-2275.ckpt']
2019-10-02 10:24:27.382,"Policy training> Surrogate loss=-0.07707502692937851, KL divergence=0.017130743712186813, Entropy=2.5974302291870117, training epoch=8, learning_rate=3e-05"
2019-10-02 10:24:24.920,"Policy training> Surrogate loss=-0.08460864424705505, KL divergence=0.014421201311051846, Entropy=2.604881763458252, training epoch=7, learning_rate=3e-05"
2019-10-02 10:24:21.862,"Policy training> Surrogate loss=-0.07644233107566833, KL divergence=0.015219520777463913, Entropy=2.6056952476501465, training epoch=6, learning_rate=3e-05"
2019-10-02 10:24:19.563,"Policy training> Surrogate loss=-0.05338438227772713, KL divergence=0.014250703155994415, Entropy=2.6040658950805664, training epoch=5, learning_rate=3e-05"
2019-10-02 10:24:16.549,"Policy training> Surrogate loss=-0.06371098756790161, KL divergence=0.014445492997765541, Entropy=2.6251587867736816, training epoch=4, learning_rate=3e-05"
2019-10-02 10:24:14.339,"Policy training> Surrogate loss=-0.04210686311125755, KL divergence=0.013633539900183678, Entropy=2.621126413345337, training epoch=3, learning_rate=3e-05"
2019-10-02 10:24:12.069,"Policy training> Surrogate loss=-0.02169853076338768, KL divergence=0.0211794450879097, Entropy=2.5998377799987793, training epoch=2, learning_rate=3e-05"
2019-10-02 10:24:08.941,"Policy training> Surrogate loss=-0.025432169437408447, KL divergence=0.0245228074491024, Entropy=2.5896918773651123, training epoch=1, learning_rate=3e-05"
2019-10-02 10:24:06.851,"Policy training> Surrogate loss=0.014186350628733635, KL divergence=0.004612687509506941, Entropy=2.625349998474121, training epoch=0, learning_rate=3e-05"
2019-10-02 10:24:03.450,"Training> Name=main_level/agent, Worker=0, Episode=100, Total reward=9.66, Steps=2275, Training iteration=4"
2019-10-02 10:24:01.444,"Training> Name=main_level/agent, Worker=0, Episode=99, Total reward=2.18, Steps=2254, Training iteration=4"
2019-10-02 10:24:00.444,"Training> Name=main_level/agent, Worker=0, Episode=98, Total reward=17.44, Steps=2241, Training iteration=4"
2019-10-02 10:23:57.443,"Training> Name=main_level/agent, Worker=0, Episode=97, Total reward=8.92, Steps=2206, Training iteration=4"
2019-10-02 10:23:55.442,"Training> Name=main_level/agent, Worker=0, Episode=96, Total reward=28.43, Steps=2188, Training iteration=4"
2019-10-02 10:23:51.441,"Training> Name=main_level/agent, Worker=0, Episode=95, Total reward=5.96, Steps=2129, Training iteration=4"
2019-10-02 10:23:49.440,"Training> Name=main_level/agent, Worker=0, Episode=94, Total reward=9.5, Steps=2110, Training iteration=4"
2019-10-02 10:23:47.439,"Training> Name=main_level/agent, Worker=0, Episode=93, Total reward=6.23, Steps=2085, Training iteration=4"
2019-10-02 10:23:45.439,"Training> Name=main_level/agent, Worker=0, Episode=92, Total reward=25.64, Steps=2067, Training iteration=4"
2019-10-02 10:23:41.437,"Training> Name=main_level/agent, Worker=0, Episode=91, Total reward=11.68, Steps=2003, Training iteration=4"
2019-10-02 10:23:38.436,"Training> Name=main_level/agent, Worker=0, Episode=90, Total reward=6.09, Steps=1965, Training iteration=4"
2019-10-02 10:23:36.436,"Training> Name=main_level/agent, Worker=0, Episode=89, Total reward=5.49, Steps=1950, Training iteration=4"
2019-10-02 10:23:35.435,"Training> Name=main_level/agent, Worker=0, Episode=88, Total reward=9.59, Steps=1934, Training iteration=4"
2019-10-02 10:23:33.435,"Training> Name=main_level/agent, Worker=0, Episode=87, Total reward=9.34, Steps=1914, Training iteration=4"
2019-10-02 10:23:31.434,"Training> Name=main_level/agent, Worker=0, Episode=86, Total reward=5.02, Steps=1895, Training iteration=4"
2019-10-02 10:23:30.433,"Training> Name=main_level/agent, Worker=0, Episode=85, Total reward=4.15, Steps=1881, Training iteration=4"
2019-10-02 10:23:28.433,"Training> Name=main_level/agent, Worker=0, Episode=84, Total reward=9.8, Steps=1866, Training iteration=4"
2019-10-02 10:23:27.432,"Training> Name=main_level/agent, Worker=0, Episode=83, Total reward=17.13, Steps=1847, Training iteration=4"
2019-10-02 10:23:24.431,"Training> Name=main_level/agent, Worker=0, Episode=82, Total reward=21.82, Steps=1810, Training iteration=4"
2019-10-02 10:23:20.430,"Training> Name=main_level/agent, Worker=0, Episode=81, Total reward=21.94, Steps=1764, Training iteration=4"
2019-10-02 10:23:16.429,Uploaded 3 files for checkpoint 4
2019-10-02 10:23:16.429,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:23:16.429,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:23:16.429,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_4.pb
2019-10-02 10:23:11.427,Checkpoint> Saving in path=['./checkpoint/4_Step-1728.ckpt']
2019-10-02 10:23:10.427,"Policy training> Surrogate loss=-0.08506821095943451, KL divergence=0.021870318800210953, Entropy=2.635873317718506, training epoch=9, learning_rate=3e-05"
2019-10-02 10:23:08.426,"Policy training> Surrogate loss=-0.0695546418428421, KL divergence=0.019169781357049942, Entropy=2.643638849258423, training epoch=8, learning_rate=3e-05"
2019-10-02 10:23:06.101,"Policy training> Surrogate loss=-0.04270011559128761, KL divergence=0.0186330396682024, Entropy=2.650397777557373, training epoch=7, learning_rate=3e-05"
2019-10-02 10:23:04.098,"Policy training> Surrogate loss=-0.0662403479218483, KL divergence=0.016232604160904884, Entropy=2.6469976902008057, training epoch=6, learning_rate=3e-05"
2019-10-02 10:23:01.950,"Policy training> Surrogate loss=-0.050839491188526154, KL divergence=0.012640099041163921, Entropy=2.6495602130889893, training epoch=5, learning_rate=3e-05"
2019-10-02 10:22:59.638,"Policy training> Surrogate loss=-0.06194856017827988, KL divergence=0.01051252894103527, Entropy=2.656137466430664, training epoch=4, learning_rate=3e-05"
2019-10-02 10:22:57.418,"Policy training> Surrogate loss=-0.030053487047553062, KL divergence=0.008229230530560017, Entropy=2.6554505825042725, training epoch=3, learning_rate=3e-05"
2019-10-02 10:22:55.406,"Policy training> Surrogate loss=-0.008727025240659714, KL divergence=0.013128739781677723, Entropy=2.6518726348876953, training epoch=2, learning_rate=3e-05"
2019-10-02 10:22:53.199,"Policy training> Surrogate loss=-0.016263965517282486, KL divergence=0.012099762447178364, Entropy=2.6557705402374268, training epoch=1, learning_rate=3e-05"
2019-10-02 10:22:50.164,"Policy training> Surrogate loss=-0.006494974251836538, KL divergence=0.0017491549951955676, Entropy=2.66172456741333, training epoch=0, learning_rate=3e-05"
2019-10-02 10:22:47.857,"Training> Name=main_level/agent, Worker=0, Episode=80, Total reward=20.71, Steps=1728, Training iteration=3"
2019-10-02 10:22:44.846,"Training> Name=main_level/agent, Worker=0, Episode=79, Total reward=3.47, Steps=1692, Training iteration=3"
2019-10-02 10:22:43.845,"Training> Name=main_level/agent, Worker=0, Episode=78, Total reward=7.17, Steps=1677, Training iteration=3"
2019-10-02 10:22:41.845,"Training> Name=main_level/agent, Worker=0, Episode=77, Total reward=9.16, Steps=1660, Training iteration=3"
2019-10-02 10:22:39.844,"Training> Name=main_level/agent, Worker=0, Episode=76, Total reward=5.64, Steps=1639, Training iteration=3"
2019-10-02 10:22:38.844,"Training> Name=main_level/agent, Worker=0, Episode=75, Total reward=3.39, Steps=1624, Training iteration=3"
2019-10-02 10:22:36.843,"Training> Name=main_level/agent, Worker=0, Episode=74, Total reward=11.08, Steps=1611, Training iteration=3"
2019-10-02 10:22:34.842,"Training> Name=main_level/agent, Worker=0, Episode=73, Total reward=8.24, Steps=1589, Training iteration=3"
2019-10-02 10:22:32.842,"Training> Name=main_level/agent, Worker=0, Episode=72, Total reward=8.22, Steps=1560, Training iteration=3"
2019-10-02 10:22:30.841,"Training> Name=main_level/agent, Worker=0, Episode=71, Total reward=13.33, Steps=1540, Training iteration=3"
2019-10-02 10:22:27.840,"Training> Name=main_level/agent, Worker=0, Episode=70, Total reward=14.71, Steps=1509, Training iteration=3"
2019-10-02 10:22:25.840,"Training> Name=main_level/agent, Worker=0, Episode=69, Total reward=3.92, Steps=1475, Training iteration=3"
2019-10-02 10:22:23.839,"Training> Name=main_level/agent, Worker=0, Episode=68, Total reward=3.43, Steps=1452, Training iteration=3"
2019-10-02 10:22:21.838,"Training> Name=main_level/agent, Worker=0, Episode=67, Total reward=6.64, Steps=1437, Training iteration=3"
2019-10-02 10:22:20.838,"Training> Name=main_level/agent, Worker=0, Episode=66, Total reward=5.76, Steps=1422, Training iteration=3"
2019-10-02 10:22:18.837,"Training> Name=main_level/agent, Worker=0, Episode=65, Total reward=6.35, Steps=1403, Training iteration=3"
2019-10-02 10:22:16.837,"Training> Name=main_level/agent, Worker=0, Episode=64, Total reward=10.47, Steps=1385, Training iteration=3"
2019-10-02 10:22:15.836,"Training> Name=main_level/agent, Worker=0, Episode=63, Total reward=8.25, Steps=1368, Training iteration=3"
2019-10-02 10:22:13.836,"Training> Name=main_level/agent, Worker=0, Episode=62, Total reward=21.29, Steps=1351, Training iteration=3"
2019-10-02 10:22:10.835,"Training> Name=main_level/agent, Worker=0, Episode=61, Total reward=26.25, Steps=1308, Training iteration=3"
2019-10-02 10:22:05.833,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_3.pb
2019-10-02 10:22:04.833,Uploaded 3 files for checkpoint 3
2019-10-02 10:22:04.833,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:22:04.833,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:22:02.832,Checkpoint> Saving in path=['./checkpoint/3_Step-1259.ckpt']
2019-10-02 10:22:01.832,"Policy training> Surrogate loss=-0.08429429680109024, KL divergence=0.01813126727938652, Entropy=2.669849395751953, training epoch=9, learning_rate=3e-05"
2019-10-02 10:22:00.043,"Policy training> Surrogate loss=-0.1003795936703682, KL divergence=0.015870699658989906, Entropy=2.6733720302581787, training epoch=8, learning_rate=3e-05"
2019-10-02 10:21:58.042,"Policy training> Surrogate loss=-0.07179033756256104, KL divergence=0.013010009191930294, Entropy=2.676272392272949, training epoch=7, learning_rate=3e-05"
2019-10-02 10:21:56.012,"Policy training> Surrogate loss=-0.0585394911468029, KL divergence=0.012351938523352146, Entropy=2.6752865314483643, training epoch=6, learning_rate=3e-05"
2019-10-02 10:21:55.006,"Policy training> Surrogate loss=-0.06119777634739876, KL divergence=0.012507651932537556, Entropy=2.673189401626587, training epoch=5, learning_rate=3e-05"
2019-10-02 10:21:52.615,"Policy training> Surrogate loss=-0.0434129536151886, KL divergence=0.012943197973072529, Entropy=2.676802396774292, training epoch=4, learning_rate=3e-05"
2019-10-02 10:21:50.605,"Policy training> Surrogate loss=-0.025076771154999733, KL divergence=0.013555734418332577, Entropy=2.677252769470215, training epoch=3, learning_rate=3e-05"
2019-10-02 10:21:48.604,"Policy training> Surrogate loss=-0.030071603134274483, KL divergence=0.014217540621757507, Entropy=2.6718695163726807, training epoch=2, learning_rate=3e-05"
2019-10-02 10:21:46.563,"Policy training> Surrogate loss=-0.013976369984447956, KL divergence=0.0050148069858551025, Entropy=2.676196336746216, training epoch=1, learning_rate=3e-05"
2019-10-02 10:21:44.541,"Policy training> Surrogate loss=0.006381312385201454, KL divergence=0.0006032162928022444, Entropy=2.679731607437134, training epoch=0, learning_rate=3e-05"
2019-10-02 10:21:42.478,"Training> Name=main_level/agent, Worker=0, Episode=60, Total reward=21.16, Steps=1259, Training iteration=2"
2019-10-02 10:21:39.424,"Training> Name=main_level/agent, Worker=0, Episode=59, Total reward=2.35, Steps=1215, Training iteration=2"
2019-10-02 10:21:37.423,"Training> Name=main_level/agent, Worker=0, Episode=58, Total reward=12.2, Steps=1203, Training iteration=2"
2019-10-02 10:21:35.423,"Training> Name=main_level/agent, Worker=0, Episode=57, Total reward=13.71, Steps=1179, Training iteration=2"
2019-10-02 10:21:33.422,"Training> Name=main_level/agent, Worker=0, Episode=56, Total reward=6.11, Steps=1155, Training iteration=2"
2019-10-02 10:21:32.422,"Training> Name=main_level/agent, Worker=0, Episode=55, Total reward=4.12, Steps=1139, Training iteration=2"
2019-10-02 10:21:30.421,"Training> Name=main_level/agent, Worker=0, Episode=54, Total reward=9.8, Steps=1125, Training iteration=2"
2019-10-02 10:21:29.421,"Training> Name=main_level/agent, Worker=0, Episode=53, Total reward=11.3, Steps=1104, Training iteration=2"
2019-10-02 10:21:26.420,"Training> Name=main_level/agent, Worker=0, Episode=52, Total reward=7.66, Steps=1070, Training iteration=2"
2019-10-02 10:21:24.419,"Training> Name=main_level/agent, Worker=0, Episode=51, Total reward=9.83, Steps=1053, Training iteration=2"
2019-10-02 10:21:23.418,"Training> Name=main_level/agent, Worker=0, Episode=50, Total reward=4.3, Steps=1037, Training iteration=2"
2019-10-02 10:21:21.418,"Training> Name=main_level/agent, Worker=0, Episode=49, Total reward=7.89, Steps=1022, Training iteration=2"
2019-10-02 10:21:20.417,"Training> Name=main_level/agent, Worker=0, Episode=48, Total reward=3.67, Steps=1003, Training iteration=2"
2019-10-02 10:21:18.417,"Training> Name=main_level/agent, Worker=0, Episode=47, Total reward=7.19, Steps=985, Training iteration=2"
2019-10-02 10:21:16.416,"Training> Name=main_level/agent, Worker=0, Episode=46, Total reward=3.91, Steps=969, Training iteration=2"
2019-10-02 10:21:15.416,"Training> Name=main_level/agent, Worker=0, Episode=45, Total reward=3.42, Steps=955, Training iteration=2"
2019-10-02 10:21:14.415,"Training> Name=main_level/agent, Worker=0, Episode=44, Total reward=11.42, Steps=943, Training iteration=2"
2019-10-02 10:21:12.415,"Training> Name=main_level/agent, Worker=0, Episode=43, Total reward=13.2, Steps=922, Training iteration=2"
2019-10-02 10:21:10.414,"Training> Name=main_level/agent, Worker=0, Episode=42, Total reward=17.54, Steps=896, Training iteration=2"
2019-10-02 10:21:07.413,"Training> Name=main_level/agent, Worker=0, Episode=41, Total reward=8.48, Steps=871, Training iteration=2"
2019-10-02 10:21:05.412,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_2.pb
2019-10-02 10:21:04.412,Uploaded 3 files for checkpoint 2
2019-10-02 10:21:04.412,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:21:04.412,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:21:00.410,"Policy training> Surrogate loss=-0.09612148255109787, KL divergence=0.011465462855994701, Entropy=2.682922124862671, training epoch=9, learning_rate=3e-05"
2019-10-02 10:21:00.410,Checkpoint> Saving in path=['./checkpoint/2_Step-855.ckpt']
2019-10-02 10:20:58.400,"Policy training> Surrogate loss=-0.06414616852998734, KL divergence=0.009217037819325924, Entropy=2.683567762374878, training epoch=8, learning_rate=3e-05"
2019-10-02 10:20:56.193,"Policy training> Surrogate loss=-0.0650630071759224, KL divergence=0.008770983666181564, Entropy=2.6840245723724365, training epoch=7, learning_rate=3e-05"
2019-10-02 10:20:54.081,"Policy training> Surrogate loss=-0.04936100170016289, KL divergence=0.009608611464500427, Entropy=2.6875362396240234, training epoch=6, learning_rate=3e-05"
2019-10-02 10:20:53.069,"Policy training> Surrogate loss=-0.06550490856170654, KL divergence=0.011901934631168842, Entropy=2.690199851989746, training epoch=5, learning_rate=3e-05"
2019-10-02 10:20:50.337,"Policy training> Surrogate loss=-0.058644723147153854, KL divergence=0.011163376271724701, Entropy=2.6899526119232178, training epoch=4, learning_rate=3e-05"
2019-10-02 10:20:48.310,"Policy training> Surrogate loss=-0.016824955120682716, KL divergence=0.010607059113681316, Entropy=2.6860954761505127, training epoch=3, learning_rate=3e-05"
2019-10-02 10:20:46.071,"Policy training> Surrogate loss=-0.04047325626015663, KL divergence=0.008549186401069164, Entropy=2.6847476959228516, training epoch=2, learning_rate=3e-05"
2019-10-02 10:20:44.963,"Policy training> Surrogate loss=-0.015623819082975388, KL divergence=0.00327284075319767, Entropy=2.6870386600494385, training epoch=1, learning_rate=3e-05"
2019-10-02 10:20:42.921,"Policy training> Surrogate loss=-0.007315758615732193, KL divergence=0.0003680738154798746, Entropy=2.689359664916992, training epoch=0, learning_rate=3e-05"
2019-10-02 10:20:39.904,"Training> Name=main_level/agent, Worker=0, Episode=40, Total reward=27.05, Steps=855, Training iteration=1"
2019-10-02 10:20:35.902,"Training> Name=main_level/agent, Worker=0, Episode=39, Total reward=2.78, Steps=800, Training iteration=1"
2019-10-02 10:20:34.902,"Training> Name=main_level/agent, Worker=0, Episode=38, Total reward=9.13, Steps=788, Training iteration=1"
2019-10-02 10:20:32.901,"Training> Name=main_level/agent, Worker=0, Episode=37, Total reward=8.87, Steps=769, Training iteration=1"
2019-10-02 10:20:31.901,"Training> Name=main_level/agent, Worker=0, Episode=36, Total reward=6.15, Steps=752, Training iteration=1"
2019-10-02 10:20:29.900,"Training> Name=main_level/agent, Worker=0, Episode=35, Total reward=4.89, Steps=734, Training iteration=1"
2019-10-02 10:20:28.900,"Training> Name=main_level/agent, Worker=0, Episode=34, Total reward=7.67, Steps=721, Training iteration=1"
2019-10-02 10:20:26.899,"Training> Name=main_level/agent, Worker=0, Episode=33, Total reward=3.64, Steps=703, Training iteration=1"
2019-10-02 10:20:24.899,"Training> Name=main_level/agent, Worker=0, Episode=32, Total reward=6.48, Steps=682, Training iteration=1"
2019-10-02 10:20:23.898,"Training> Name=main_level/agent, Worker=0, Episode=31, Total reward=11.26, Steps=665, Training iteration=1"
2019-10-02 10:20:21.897,"Training> Name=main_level/agent, Worker=0, Episode=30, Total reward=6.0, Steps=643, Training iteration=1"
2019-10-02 10:20:19.897,"Training> Name=main_level/agent, Worker=0, Episode=29, Total reward=6.47, Steps=624, Training iteration=1"
2019-10-02 10:20:18.896,"Training> Name=main_level/agent, Worker=0, Episode=28, Total reward=8.16, Steps=610, Training iteration=1"
2019-10-02 10:20:15.895,"Training> Name=main_level/agent, Worker=0, Episode=27, Total reward=6.31, Steps=576, Training iteration=1"
2019-10-02 10:20:13.894,"Training> Name=main_level/agent, Worker=0, Episode=26, Total reward=4.16, Steps=561, Training iteration=1"
2019-10-02 10:20:12.894,"Training> Name=main_level/agent, Worker=0, Episode=25, Total reward=5.11, Steps=547, Training iteration=1"
2019-10-02 10:20:10.893,"Training> Name=main_level/agent, Worker=0, Episode=24, Total reward=5.98, Steps=530, Training iteration=1"
2019-10-02 10:20:09.893,"Training> Name=main_level/agent, Worker=0, Episode=23, Total reward=10.54, Steps=515, Training iteration=1"
2019-10-02 10:20:07.892,"Training> Name=main_level/agent, Worker=0, Episode=22, Total reward=14.39, Steps=492, Training iteration=1"
2019-10-02 10:20:05.892,"Training> Name=main_level/agent, Worker=0, Episode=21, Total reward=13.87, Steps=466, Training iteration=1"
2019-10-02 10:20:01.890,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_1.pb
2019-10-02 10:20:00.890,Uploaded 3 files for checkpoint 1
2019-10-02 10:20:00.890,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:20:00.890,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:19:58.889,Checkpoint> Saving in path=['./checkpoint/1_Step-442.ckpt']
2019-10-02 10:19:57.889,"Policy training> Surrogate loss=-0.031982894986867905, KL divergence=0.017624979838728905, Entropy=2.690673828125, training epoch=9, learning_rate=3e-05"
2019-10-02 10:19:55.887,"Policy training> Surrogate loss=-0.0920623242855072, KL divergence=0.01631203480064869, Entropy=2.6927127838134766, training epoch=8, learning_rate=3e-05"
2019-10-02 10:19:54.540,"Policy training> Surrogate loss=-0.03860609605908394, KL divergence=0.014336545020341873, Entropy=2.6946470737457275, training epoch=7, learning_rate=3e-05"
2019-10-02 10:19:52.540,"Policy training> Surrogate loss=-0.023045377805829048, KL divergence=0.011743317358195782, Entropy=2.696857452392578, training epoch=6, learning_rate=3e-05"
2019-10-02 10:19:51.331,"Policy training> Surrogate loss=-0.06588802486658096, KL divergence=0.00966320838779211, Entropy=2.6985886096954346, training epoch=5, learning_rate=3e-05"
2019-10-02 10:19:49.001,"Policy training> Surrogate loss=-0.006140038371086121, KL divergence=0.00768307177349925, Entropy=2.7003233432769775, training epoch=4, learning_rate=3e-05"
2019-10-02 10:19:46.959,"Policy training> Surrogate loss=-0.06292925029993057, KL divergence=0.004996495321393013, Entropy=2.7027747631073, training epoch=3, learning_rate=3e-05"
2019-10-02 10:19:44.796,"Policy training> Surrogate loss=-0.022906621918082237, KL divergence=0.00259190215729177, Entropy=2.7050063610076904, training epoch=2, learning_rate=3e-05"
2019-10-02 10:19:42.745,"Policy training> Surrogate loss=-0.025252602994441986, KL divergence=0.0010130865266546607, Entropy=2.7064192295074463, training epoch=1, learning_rate=3e-05"
2019-10-02 10:19:40.645,"Policy training> Surrogate loss=-0.021903464570641518, KL divergence=0.00015867526235524565, Entropy=2.707183837890625, training epoch=0, learning_rate=3e-05"
2019-10-02 10:19:36.489,"Training> Name=main_level/agent, Worker=0, Episode=20, Total reward=15.52, Steps=442, Training iteration=0"
2019-10-02 10:19:33.488,"Training> Name=main_level/agent, Worker=0, Episode=19, Total reward=3.17, Steps=405, Training iteration=0"
2019-10-02 10:19:32.487,"Training> Name=main_level/agent, Worker=0, Episode=18, Total reward=5.55, Steps=394, Training iteration=0"
2019-10-02 10:19:31.487,"Training> Name=main_level/agent, Worker=0, Episode=17, Total reward=12.27, Steps=381, Training iteration=0"
2019-10-02 10:19:29.486,"Training> Name=main_level/agent, Worker=0, Episode=16, Total reward=15.88, Steps=357, Training iteration=0"
2019-10-02 10:19:26.485,"Training> Name=main_level/agent, Worker=0, Episode=15, Total reward=3.05, Steps=321, Training iteration=0"
2019-10-02 10:19:25.485,"Training> Name=main_level/agent, Worker=0, Episode=14, Total reward=8.81, Steps=310, Training iteration=0"
2019-10-02 10:19:23.484,"Training> Name=main_level/agent, Worker=0, Episode=13, Total reward=7.67, Steps=290, Training iteration=0"
2019-10-02 10:19:20.483,"Training> Name=main_level/agent, Worker=0, Episode=12, Total reward=7.1, Steps=259, Training iteration=0"
2019-10-02 10:19:18.483,"Training> Name=main_level/agent, Worker=0, Episode=11, Total reward=13.71, Steps=239, Training iteration=0"
2019-10-02 10:19:16.482,"Training> Name=main_level/agent, Worker=0, Episode=10, Total reward=5.28, Steps=209, Training iteration=0"
2019-10-02 10:19:14.482,"Training> Name=main_level/agent, Worker=0, Episode=9, Total reward=5.76, Steps=190, Training iteration=0"
2019-10-02 10:19:12.481,"Training> Name=main_level/agent, Worker=0, Episode=8, Total reward=9.11, Steps=174, Training iteration=0"
2019-10-02 10:19:10.480,"Training> Name=main_level/agent, Worker=0, Episode=7, Total reward=4.96, Steps=146, Training iteration=0"
2019-10-02 10:19:08.479,"Training> Name=main_level/agent, Worker=0, Episode=6, Total reward=8.34, Steps=131, Training iteration=0"
2019-10-02 10:19:07.479,"Training> Name=main_level/agent, Worker=0, Episode=5, Total reward=4.6, Steps=111, Training iteration=0"
2019-10-02 10:19:05.478,"Training> Name=main_level/agent, Worker=0, Episode=4, Total reward=12.49, Steps=95, Training iteration=0"
2019-10-02 10:19:03.478,"Training> Name=main_level/agent, Worker=0, Episode=3, Total reward=12.92, Steps=74, Training iteration=0"
2019-10-02 10:19:01.477,"Training> Name=main_level/agent, Worker=0, Episode=2, Total reward=12.61, Steps=50, Training iteration=0"
2019-10-02 10:18:58.476,"Training> Name=main_level/agent, Worker=0, Episode=1, Total reward=13.21, Steps=22, Training iteration=0"
2019-10-02 10:18:44.472,saved intermediate frozen graph: DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2/model/model_0.pb
2019-10-02 10:18:44.472,"DoorMan: installing SIGINT, SIGTERM"
2019-10-02 10:18:43.472,Uploaded 3 files for checkpoint 0
2019-10-02 10:18:43.472,INFO:tensorflow:Froze 11 variables.
2019-10-02 10:18:43.472,INFO:tensorflow:Converted 11 variables to const ops.
2019-10-02 10:18:42.471,Checkpoint> Saving in path=['./checkpoint/0_Step-0.ckpt']
2019-10-02 10:18:39.470,2019-10-02 10:18:39.102420: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-02 10:18:33.469,"Loaded action space from file: [{'index': 0, 'steering_angle': -30, 'speed': 2.6666666666666665}, {'index': 1, 'steering_angle': -30, 'speed': 5.333333333333333}, {'index': 2, 'steering_angle': -30, 'speed': 8}, {'index': 3, 'steering_angle': -15, 'speed': 2.6666666666666665}, {'index': 4, 'steering_angle': -15, 'speed': 5.333333333333333}, {'index': 5, 'steering_angle': -15, 'speed': 8}, {'index': 6, 'steering_angle': 0, 'speed': 2.6666666666666665}, {'index': 7, 'steering_angle': 0, 'speed': 5.333333333333333}, {'index': 8, 'steering_angle': 0, 'speed': 8}, {'index': 9, 'steering_angle': 15, 'speed': 2.6666666666666665}, {'index': 10, 'steering_angle': 15, 'speed': 5.333333333333333}, {'index': 11, 'steering_angle': 15, 'speed': 8}, {'index': 12, 'steering_angle': 30, 'speed': 2.6666666666666665}, {'index': 13, 'steering_angle': 30, 'speed': 5.333333333333333}, {'index': 14, 'steering_angle': 30, 'speed': 8}]"
2019-10-02 10:18:33.469,## Creating agent - name: agent
2019-10-02 10:18:33.468,Using the following hyper-parameters
2019-10-02 10:18:33.468,"{
  ""batch_size"": 64,
  ""beta_entropy"": 0.01,
  ""discount_factor"": 0.999,
  ""e_greedy_value"": 1.0,
  ""epsilon_steps"": 10000,
  ""exploration_type"": ""categorical"",
  ""loss_type"": ""huber"",
  ""lr"": 3e-05,
  ""num_episodes_between_training"": 20,
  ""num_epochs"": 10,
  ""stack_size"": 1,
  ""term_cond_avg_score"": 100000.0,
  ""term_cond_max_episodes"": 100000"
2019-10-02 10:18:33.468,}
2019-10-02 10:18:33.468,Uploaded hyperparameters.json to S3
2019-10-02 10:18:33.468,Uploaded IP address information to S3: 10.0.1.34
2019-10-02 10:18:33.468,## Creating graph - name: BasicRLGraphManager
2019-10-02 10:18:32.468,Initializing SageS3Client...
2019-10-02 10:18:32.468,Successfully downloaded model metadata from model-metadata/LR-REDUCER-8ms/model_metadata.json.
2019-10-02 10:18:30.467,"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])"
2019-10-02 10:18:30.467,"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])"
2019-10-02 10:18:30.467,"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])"
2019-10-02 10:18:30.467,"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])"
2019-10-02 10:18:30.467,"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])"
2019-10-02 10:18:30.467,"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])"
2019-10-02 10:18:26.465,"2019-10-02 10:18:24,047 sagemaker-containers INFO     Imported framework sagemaker_bootstrap"
2019-10-02 10:18:26.465,"2019-10-02 10:18:24,048 sagemaker_bootstrap INFO     SM_USER_ARGS=[""--aws_region"",""us-east-1"",""--batch_size"",""64"",""--beta_entropy"",""0.01"",""--discount_factor"",""0.999"",""--e_greedy_value"",""1"",""--epsilon_steps"",""10000"",""--exploration_type"",""Categorical"",""--loss_type"",""Huber"",""--lr"",""3e-05"",""--model_metadata_s3_key"",""s3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/model-metadata/LR-REDUCER-8ms/model_metadata.json"",""--num_episodes_between_training"",""20"",""--num_epochs"",""10"",""--reward_function_s3_source"",""s3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/reward-functions/LR-REDUCER-8ms/reward_function.py"",""--s3_bucket"",""aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07"",""--s3_prefix"",""DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2"",""--stack_size"",""1""]"
2019-10-02 10:18:26.465,"2019-10-02 10:18:24,048 sagemaker_bootstrap INFO     All eniron vars=environ({'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/0c099b98-30f1-4670-8107-bc2cd295dbcf', 'SM_CURRENT_HOST': 'algo-1', 'SM_HP_EXPLORATION_TYPE': 'Categorical', 'SM_HP_LR': '3e-05', 'TRAINING_JOB_NAME': 'dr-sm-rltj--20191002101606-325b7ca3-edd8-4654-b15d-97d47a2b8a21', 'HOSTNAME': 'ip-10-0-1-34.ec2.internal', 'PATH': '/opt/ml/code/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'SM_HP_NUM_EPOCHS': '10', 'DMLC_INTERFACE': 'eth0', 'SM_HP_BATCH_SIZE': '64', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_HOSTS': '[""algo-1""]', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'SAGEMAKER_JOB_NAME': '', 'SM_MODULE_DIR': '', 'SM_MODULE_NAME': '', 'SM_HP_AWS_REGION': 'us-east-1', 'SM_HP_MODEL_METADATA_S3_KEY': 's3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/model-metadata/LR-REDUCER-8ms/model_metadata.json', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'PYTHONPATH': '/opt/amazon/:', 'SAGEMAKER_REGION': '', 'SM_HP_BETA_ENTROPY': '0.01', 'SAGEMAKER_TRAINING_COMMAND': '/opt/ml/code/sage-train.sh', 'SM_HP_LOSS_TYPE': 'Huber', 'HOME': '/root', 'SM_HP_S3_PREFIX': 'DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2', 'SM_HPS': '{""aws_region"":""us-east-1"",""batch_size"":64,""beta_entropy"":0.01,""discount_factor"":0.999,""e_greedy_value"":1,""epsilon_steps"":10000,""exploration_type"":""Categorical"",""loss_type"":""Huber"",""lr"":3e-05,""model_metadata_s3_key"":""s3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/model-metadata/LR-REDUCER-8ms/model_metadata.json"",""num_episodes_between_training"":20,""num_epochs"":10,""reward_function_s3_source"":""s3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/reward-functions/LR-REDUCER-8ms/reward_function.py"",""s3_bucket"":""aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07"",""s3_prefix"":""DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2"",""stack_size"":1}', 'AWS_REGION': 'us-east-1', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_bootstrap:train', 'CURRENT_HOST': 'algo-1', 'SM_CHANNELS': '[]', 'SM_NUM_CPUS': '8', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HP_NUM_EPISODES_BETWEEN_TRAINING': '20', 'SM_TRAINING_ENV': '{""channel_input_dirs"":{},""current_host"":""algo-1"",""framework_module"":""sagemaker_bootstrap:train"",""hosts"":[""algo-1""],""hyperparameters"":{""aws_region"":""us-east-1"",""batch_size"":64,""beta_entropy"":0.01,""discount_factor"":0.999,""e_greedy_value"":1,""epsilon_steps"":10000,""exploration_type"":""Categorical"",""loss_type"":""Huber"",""lr"":3e-05,""model_metadata_s3_key"":""s3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/model-metadata/LR-REDUCER-8ms/model_metadata.json"",""num_episodes_between_training"":20,""num_epochs"":10,""reward_function_s3_source"":""s3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/reward-functions/LR-REDUCER-8ms/reward_function.py"",""s3_bucket"":""aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07"",""s3_prefix"":""DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2"",""stack_size"":1},""input_config_dir"":""/opt/ml/input/config"",""input_data_config"":{},""input_dir"":""/opt/ml/input"",""job_name"":""dr-sm-rltj--20191002101606-325b7ca3-edd8-4654-b15d-97d47a2b8a21"",""log_level"":20,""model_dir"":""/opt/ml/model"",""module_dir"":"""",""module_name"":"""",""network_interface_name"":""eth0"",""num_cpus"":8,""num_gpus"":0,""output_data_dir"":""/opt/ml/output/data"",""output_dir"":""/opt/ml/output"",""resource_config"":{""current_host"":""algo-1"",""hosts"":[""algo-1""],""network_interface_name"":""eth0""}}', 'NVIDIA_VISIBLE_DEVICES': 'void', 'SM_HP_DISCOUNT_FACTOR': '0.999', 'SM_NUM_GPUS': '0', 'SM_INPUT_DIR': '/opt/ml/input', 'PYTHONUNBUFFERED': '1', 'SM_HP_EPSILON_STEPS': '10000', 'SM_HP_E_GREEDY_VALUE': '1', 'SM_HP_S3_BUCKET': 'aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07', 'NODE_TYPE': 'SAGEMAKER_TRAINING_WORKER', 'SM_FRAMEWORK_MODULE': 'sagemaker_bootstrap:train', 'SM_USER_ARGS': '[""--aws_region"",""us-east-1"",""--batch_size"",""64"",""--beta_entropy"",""0.01"",""--discount_factor"",""0.999"",""--e_greedy_value"",""1"",""--epsilon_steps"",""10000"",""--exploration_type"",""Categorical"",""--loss_type"",""Huber"",""--lr"",""3e-05"",""--model_metadata_s3_key"",""s3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/model-metadata/LR-REDUCER-8ms/model_metadata.json"",""--num_episodes_between_training"",""20"",""--num_epochs"",""10"",""--reward_function_s3_source"",""s3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/reward-functions/LR-REDUCER-8ms/reward_function.py"",""--s3_bucket"",""aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07"",""--s3_prefix"",""DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2"",""--stack_size"",""1""]', 'SM_HP_REWARD_FUNCTION_S3_SOURCE': 's3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/reward-functions/LR-REDUCER-8ms/reward_function.py', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/1862fa0b-e02c-4ec4-8789-5b3186157354', 'SM_LOG_LEVEL': '20', 'SM_HP_STACK_SIZE': '1', 'SM_MODEL_DIR': '/opt/ml/model', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:940645781828:training-job/dr-sm-rltj--20191002101606-325b7ca3-edd8-4654-b15d-97d47a2b8a21', 'SM_RESOURCE_CONFIG': '{""current_host"":""algo-1"",""hosts"":[""algo-1""],""network_interface_name"":""eth0""}'})"
2019-10-02 10:18:26.465,"2019-10-02 10:18:24,048 sagemaker_bootstrap INFO     Launching training command: /opt/ml/code/sage-train.sh --aws_region us-east-1 --batch_size 64 --beta_entropy 0.01 --discount_factor 0.999 --e_greedy_value 1 --epsilon_steps 10000 --exploration_type Categorical --loss_type Huber --lr 3e-05 --model_metadata_s3_key s3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/model-metadata/LR-REDUCER-8ms/model_metadata.json --num_episodes_between_training 20 --num_epochs 10 --reward_function_s3_source s3://aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07/reward-functions/LR-REDUCER-8ms/reward_function.py --s3_bucket aws-deepracer-ff0cc1db-3f91-4d02-9c3f-3ceaef9a7f07 --s3_prefix DeepRacer-SageMaker-RoboMaker-comm-940645781828-20191002101606-898fc6a6-c07b-430d-a1e9-9f86ac4172c2 --stack_size 1"
2019-10-02 10:18:26.465,Starting sage-train.sh
2019-10-02 10:18:26.465,20:C 02 Oct 2019 10:18:24.117 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
2019-10-02 10:18:26.465,"20:C 02 Oct 2019 10:18:24.118 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=20, just started"
2019-10-02 10:18:26.465,"20:C 02 Oct 2019 10:18:24.118 # Configuration loaded
                _._                                                  
           _.-``__ ''-._                                             
      _.-``    `.  `_.  ''-._           Redis 5.0.5 (00000000/0) 64 bit
  .-`` .-```.  ```\/    _.,_ ''-._                                   
 (    '      ,       .-`  | `,    )     Running in standalone mode
 |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379
 |    `-._   `._    /     _.-'    |     PID: 20
  `-._    `-._  `-./  _.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |           http://redis.io        
  `-._    `-._`-.__.-'_.-'    _.-'                                   
 |`-._`-._    `-.__.-'    _.-'_.-'|                                  
 |    `-._`-._        _.-'_.-'    |                                  
  `-._    `-._`-.__.-'_.-'    _.-'                                   
      `-._    `-.__.-'    _.-'                                       
          `-._        _.-'                                           
              `-.__.-'                                               
"
2019-10-02 10:18:26.465,20:M 02 Oct 2019 10:18:24.122 # WARNING: The TCP backlog setting of 512 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
2019-10-02 10:18:26.465,20:M 02 Oct 2019 10:18:24.122 # Server initialized
2019-10-02 10:18:26.465,20:M 02 Oct 2019 10:18:24.122 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect.
2019-10-02 10:18:26.465,"20:M 02 Oct 2019 10:18:24.122 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled."
2019-10-02 10:18:26.465,20:M 02 Oct 2019 10:18:24.122 * Ready to accept connections